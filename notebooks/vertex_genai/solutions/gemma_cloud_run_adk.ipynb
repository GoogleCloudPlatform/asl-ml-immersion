{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fdb77ea71896541",
   "metadata": {},
   "source": [
    "# Run LLM inference on Cloud Run GPUs with Gemma 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5d860-0945-4799-b2ba-74298c87ee3b",
   "metadata": {},
   "source": [
    "## Deploy a Gemma model with a prebuilt container\n",
    "This guide shows how to run LLM inference on Cloud Run GPUs with Gemma 3 and Ollama, and has the following objectives:\n",
    " - Deploy Ollama with the Gemma 3 model on a GPU-enabled Cloud Run service using a prebuilt container.\n",
    " - Using the deployed Cloud Run service with the Google Gen AI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1ed47-11ec-4f24-9967-65cd62c43141",
   "metadata": {},
   "source": [
    "***Gemma is a family of generative artificial intelligence (AI) models*** and you can use them in a wide variety of generation tasks, including question answering, summarization, and reasoning. Gemma models are provided with open weights and permit responsible commercial use, allowing you to tune and deploy them in your own projects and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d07082-c407-46c5-9fbe-9a420ba2f989",
   "metadata": {},
   "source": [
    "#### You can deploy the container image and make Ollama with Gemma 3 available as a Cloud Run service.\n",
    "Use the following gcloud run deploy command to deploy your Cloud Run service:\n",
    "gcloud run deploy {SERVICE_NAME} \\\n",
    " --image {IMAGE} \\\n",
    " --concurrency 4 \\\n",
    " --cpu 8 \\\n",
    " --set-env-vars OLLAMA_NUM_PARALLEL=4 \\\n",
    " --set-env-vars=API_KEY={YOUR_API_KEY} \\\n",
    " --gpu 1 \\\n",
    " --gpu-type nvidia-l4 \\\n",
    " --max-instances 1 \\\n",
    " --memory 32Gi \\\n",
    " --allow-unauthenticated \\\n",
    " --no-cpu-throttling \\\n",
    " --timeout=600 \\\n",
    " --region {REGION}\n",
    " \n",
    "The Cloud Run service is configured with:\n",
    "Explanation of Variables:\n",
    "\n",
    " - SERVICE_NAME: The unique name for your Cloud Run service.\n",
    " - IMAGE: The Docker image to deploy. This can be one of our pre-built images or an image you built yourself from this repository\n",
    " - YOUR_API_KEY: Crucial for authentication. Set this to a strong, unique API key string of your choice. This key will be required to access your service. See the Authentication section below for more details. If you're deploying from AI Studio, this is generated on your behalf. Note that this should not be an API key re-used from another service.\n",
    " - REGION: The Google Cloud region where your Cloud Run service will be deployed (e.g., us-central1). Ensure this region supports the specified GPU type. See GPU support for Cloud Run services for more details. If you're deploying from AI Studio, this defaults to europe-west1.\n",
    "\n",
    "After successful deployment, the gcloud command will output the Cloud Run service URL. Save this URL as <cloud_run_url> for interacting with your service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c47ddc-622b-438b-aa26-9e0b7db828c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Supported Models and Pre-Built Docker Images:\n",
    "\n",
    " - gemma-3-1b-it\n",
    " - gemma-3-4b-it\n",
    " - gemma-3-12b-it\n",
    " - gemma-3-27b-it\n",
    " - gemma-3n-e2b-it\n",
    " - gemma-3n-e4b-it\n",
    " \n",
    "These images have the respective Gemma models bundled:\n",
    "\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3-1b\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3-4b\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3-12b\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3-27b\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3n-e2b\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3n-e4b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7672bbe-364c-421a-88a6-dd9e2ad94dca",
   "metadata": {},
   "source": [
    "### Copy the next commant to console:"
   ]
  },
  {
   "cell_type": "code",
   "id": "802b2d53-594d-4330-99ce-75cdd2a264e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(\"gcloud run deploy llm-gemma3-4b \\\n",
    " --image us-docker.pkg.dev/cloudrun/container/gemma/gemma3-4b \\\n",
    " --concurrency 4 \\\n",
    " --cpu 8 \\\n",
    " --set-env-vars OLLAMA_NUM_PARALLEL=4 \\\n",
    " --set-env-vars=API_KEY=TESTKEY12345 \\\n",
    " --gpu 1 \\\n",
    " --gpu-type nvidia-l4 \\\n",
    " --max-instances 1 \\\n",
    " --memory 32Gi \\\n",
    " --allow-unauthenticated \\\n",
    " --no-cpu-throttling \\\n",
    " --timeout=600 \\\n",
    " --region us-central1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d82b0083-5f5e-4e4c-9665-f32009616c23",
   "metadata": {},
   "source": [
    "## Send prompts to the LLM CloudRun service by using Google Gen AI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e7f3c3-93d6-410d-9c83-005204d7115d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Initializing Google Gen AI SDK Client"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ade28bc-9ef3-40e3-a52b-7bbe9e95fd53",
   "metadata": {
    "tags": []
   },
   "source": [
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "GEMMA_CLOUD_RUN_ENDPOINT=\"https://llm-gemma3-4b-25570882233.us-central1.run.app\" #TODO: Add your cloud run enpoint here\n",
    "GEMMA_CLOUD_RUN_API_KEY=\"TESTKEY12345\"\n",
    "# Configure the client to use your Cloud Run endpoint and API key\n",
    "client = genai.Client(api_key=GEMMA_CLOUD_RUN_API_KEY, http_options=HttpOptions(base_url=GEMMA_CLOUD_RUN_ENDPOINT))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b6a87fc-4fa0-4aac-bc39-2fd09c70da58",
   "metadata": {},
   "source": [
    "#### Generate content (non-streaming): "
   ]
  },
  {
   "cell_type": "code",
   "id": "e217b4e4-cbf3-4825-9a25-65ddd367e5e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "response = client.models.generate_content(\n",
    "   model=\"gemma-3-4b-it\", # Replace model with the Gemma 3 model you selected, such as \"gemma-3-4b-it\".\n",
    "   contents=[\"How does AI work?\"]\n",
    ")\n",
    "print(response.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce2078e7-044b-4465-8cc8-41bcf49df595",
   "metadata": {},
   "source": [
    "#### Stream generate content example"
   ]
  },
  {
   "cell_type": "code",
   "id": "2bd05495-f797-480e-8247-4abe5a6db33a",
   "metadata": {
    "tags": []
   },
   "source": [
    "response = client.models.generate_content_stream(\n",
    "   model=\"gemma-3-4b-it\", # Replace model with the Gemma 3 model you selected, such as \"gemma-3-4b-it\".\n",
    "   contents=[\"Write a story about a magic backpack. You are the narrator of an interactive text adventure game.\"]\n",
    ")\n",
    "for chunk in response:\n",
    "   print(chunk.text, end=\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40c6abcd-84dd-4348-bb0d-19decba9d70d",
   "metadata": {
    "tags": []
   },
   "source": [
    "import subprocess\n",
    "\n",
    "from IPython.display import Image, Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "prompt = \"Write a story about a magic backpack. You are the narrator of an interactive text adventure game.\"\n",
    "\n",
    "MODEL=\"gemma3:4b\"\n",
    "client = OpenAI(\n",
    "    api_key=GEMMA_CLOUD_RUN_API_KEY,\n",
    "    base_url=f\"{GEMMA_CLOUD_RUN_ENDPOINT}/v1\",\n",
    ")\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    #extra_headers=f\"Bearer {identity_token}\",\n",
    ")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(chat_response.choices[0].message.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d8592df-4225-422a-b680-2ba864ed6835",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic App: Weather Agent"
   ]
  },
  {
   "cell_type": "code",
   "id": "32e0d010-727d-4c40-b0cc-6b85e2899d6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "import asyncio\n",
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm  # For multi-model support\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from google.genai import types  # For creating message Content/Parts\n",
    "from IPython.display import HTML, Markdown, display\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c9645813-a5f6-4f54-b59b-6316422072a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "LOCATION = \"us-central1\"\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\"  # Use Vertex AI API"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b86b3211-27f5-4efe-bdcc-a18f70ce8d3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%bash\n",
    "echo > adk_agents/.env \"GOOGLE_CLOUD_LOCATION=$GOOGLE_CLOUD_LOCATION\n",
    "GOOGLE_GENAI_USE_VERTEXAI=$GOOGLE_GENAI_USE_VERTEXAI\n",
    "\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9243f145-3259-4429-867d-50721281ef1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "!mkdir ./adk_agents/agent1_weather_gemma/"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af359943-6fc5-46ae-aa5f-defa852008e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%writefile ./adk_agents/agent1_weather_gemma/__init__.py\n",
    "# pylint: skip-file\n",
    "from . import agent"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44d36912-69fc-4cf9-b66c-1f611d6cc15a",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%writefile ./adk_agents/agent1_weather_gemma/agent.py\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "\n",
    "root_agent = LlmAgent(\n",
    "    name=\"weather_agent_v1\",\n",
    "    model=LiteLlm(\n",
    "        model=\"openai/gemma3:4b\",\n",
    "        api_base=\"https://llm-gemma3-4b-25570882233.us-central1.run.app/v1\",\n",
    "        api_key=\"TESTKEY12345\",\n",
    "        # Pass authentication headers if needed\n",
    "        # extra_headers=auth_headers\n",
    "        # Alternatively, if endpoint uses an API key:\n",
    "        # api_key=\"YOUR_ENDPOINT_API_KEY\"\n",
    "    ),\n",
    "    description=\"Provides average weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use information about average weather. \",\n",
    "    tools=[], # Pass the function directly\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00cb6322-c408-4574-a0aa-f7d6deb72023",
   "metadata": {
    "tags": []
   },
   "source": [
    "from adk_agents.agent1_weather_gemma import agent\n",
    "\n",
    "importlib.reload(agent)  # Force reload\n",
    "\n",
    "APP_NAME = \"weather_tutorial_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\"  # Using a fixed ID for simplicity\n",
    "\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Create the specific session where the conversation will happen\n",
    "session = await session_service.create_session(\n",
    "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    agent=agent.root_agent,  # The agent we want to run\n",
    "    app_name=APP_NAME,  # Associates runs with our app\n",
    "    session_service=session_service,  # Uses our session manager\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "115d2672-0e09-4570-9ef6-92bd83a7698a",
   "metadata": {
    "tags": []
   },
   "source": [
    "async def call_agent_async(query: str, runner, user_id, session_id):\n",
    "    \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "    print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "\n",
    "    final_response_text = \"Agent did not produce a final response.\"  # Default\n",
    "\n",
    "    # Key Concept: run_async executes the agent logic and yields Events.\n",
    "    # We iterate through events to find the final answer.\n",
    "    async for event in runner.run_async(\n",
    "        user_id=user_id, session_id=session_id, new_message=content\n",
    "    ):\n",
    "        # You can uncomment the line below to see *all* events during execution\n",
    "        # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "        # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "        if event.is_final_response():\n",
    "            if event.content and event.content.parts:\n",
    "                # Assuming text response in the first part\n",
    "                final_response_text = event.content.parts[0].text\n",
    "            elif (\n",
    "                event.actions and event.actions.escalate\n",
    "            ):  # Handle potential errors/escalations\n",
    "                final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "            # Add more checks here if needed (e.g., specific error codes)\n",
    "            break  # Stop processing events once the final response is found\n",
    "\n",
    "    print(f\"<<< Agent Response: {final_response_text}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "188f3dc5-2c19-4a5e-80a9-915baf8490d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "await call_agent_async(\n",
    "    \"What is the weather like in London?\",\n",
    "    runner=runner,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d1ec2cb-ffb2-4185-86d2-076c9775a6cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clean up\n",
    "After you have finished, it is a good practice to clean up your cloud resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b6138-4d26-40ca-a306-3470b1136367",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-adk_kernel-adk_kernel",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "ADK kernel (Local)",
   "language": "python",
   "name": "conda-env-adk_kernel-adk_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

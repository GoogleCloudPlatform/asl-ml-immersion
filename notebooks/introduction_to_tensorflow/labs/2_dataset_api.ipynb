{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Data Load and Preprocessing with tf.data\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Learn how to use tf.data to read data from memory\n",
    "1. Learn how to use tf.data to read data from disk\n",
    "1. Learn how to write production input pipelines with feature engineering (batching, shuffling, etc.)\n",
    "1. Learn how to optimize pipeline with tf.data\n",
    "\n",
    "\n",
    "In this notebook, we will start by refactoring the linear regression we implemented in the previous lab so that it takes its data from a`tf.data.Dataset`, and we will learn how to implement **stochastic gradient descent** with it. In this case, the original dataset will be synthetic and read by the `tf.data` API directly from memory.\n",
    "\n",
    "We will use TensorFlow for framework, but **tf.data works with any frameworks like JAX or Pytorch**.\n",
    "\n",
    "In a second part, we will learn how to load a dataset with the `tf.data` API when the dataset resides on disk, and then learn how to optimize the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the synthetic dataset of the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POINTS = 10\n",
    "X = tf.constant(range(N_POINTS), dtype=tf.float32)\n",
    "Y = 2 * X + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by implementing a function that takes as input\n",
    "\n",
    "- our $X$ and $Y$ vectors of synthetic data generated by the linear function $y= 2x + 10$\n",
    "- the number of passes over the dataset we want to train on (`epochs`)\n",
    "- the size of the batches in the dataset (`batch_size`)\n",
    "and returns a `tf.data.Dataset`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Note that the last batch may not contain the exact number of elements you specified because the dataset was exhausted.\n",
    "\n",
    "If you want batches with the exact same number of elements per batch, we will have to discard the last batch by\n",
    "setting:\n",
    "\n",
    "```python\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "```\n",
    "\n",
    "We will do that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Implement the `create_dataset` function. <br>\n",
    "Your function should create a `tf.data.Dataset` from the input tensors `X` and `Y`, and then configure it to repeat for the specified number of epochs and create batches of the specified `batch_size`. Drop the last batch if it's smaller than the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, Y, epochs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    # TODO: Your code goes here\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function by iterating twice over our dataset in batches of 3 data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "EPOCH = 2\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    print(\"x:\", x.numpy(), \"y:\", y.numpy())\n",
    "    assert len(x) == BATCH_SIZE\n",
    "    assert len(y) == BATCH_SIZE\n",
    "assert EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function and the function that computes the gradients are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(X, Y, w0, w1):\n",
    "    Y_hat = w0 * X + w1\n",
    "    errors = (Y_hat - Y) ** 2\n",
    "    return tf.reduce_mean(errors)\n",
    "\n",
    "\n",
    "def compute_gradients(X, Y, w0, w1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "    return tape.gradient(loss, [w0, w1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference now is that now, in the training loop, we will iterate directly on the `tf.data.Dataset` generated by our `create_dataset` function. \n",
    "\n",
    "We will configure the dataset so that it iterates 250 times over our synthetic dataset in batches of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Implement the training loop. For each batch from the dataset, you should:\n",
    "1. Compute the gradients of the loss with respect to the weights `w0` and `w1`.\n",
    "2. Update the weights using the gradients and the learning rate.\n",
    "3. Every 100 steps, calculate the loss and print the step number, loss, and current values of `w0` and `w1`.\n",
    "\n",
    "Make sure to update the `loss` variable within the loop so the final assertions can check your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 250\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.02\n",
    "\n",
    "MSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\\n\"\n",
    "\n",
    "w0 = tf.Variable(0.0)\n",
    "w1 = tf.Variable(0.0)\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    # TODO: Your code goes here\n",
    "    pass\n",
    "\n",
    "assert loss < 0.0001\n",
    "assert abs(w0 - 2) < 0.001\n",
    "assert abs(w1 - 10) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating the CSV files\n",
    "\n",
    "We will start with the **taxifare dataset** CSV files that we wrote out in a previous lab. \n",
    "\n",
    "The taxifare dataset files have been saved into `../data`.\n",
    "\n",
    "Check that it is the case in the cell below, and, if not, regenerate the taxifare\n",
    "dataset by running the previous lab notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../data/taxi*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Low-level tf.data API to read the CSV files\n",
    "\n",
    "To get a more flexible pipeline, we can utilize low-level tf.data APIs to fully control the behavior of the pipeline.\n",
    "\n",
    "For text-based data including CSV, we can use `TextLineDataset` to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Dataset object (`ds`) is still just a definition, and it hasn't loaded the actual data yet.<br>\n",
    "Let's iterate over the first two elements of this dataset using `dataset.take(2)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in ds.take(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems it loads the header row as the first element. Since it's not part of the training data, lets' skip it with the `skip()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\").skip(1)\n",
    "\n",
    "for data in ds.take(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features with `.map()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've loaded the CSV file as a text file, and each row was simply represented as a single string value containing speparators (`,`).\n",
    "\n",
    "Let's write a parsing function that takes a row and splits it into multiple values.\n",
    "\n",
    "**Exercise 3**: Implement the `parse_csv` function to split a single CSV row string into a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(row):\n",
    "    # TODO: Your code goes here\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure it works by calling this function in the for loop with `.take()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\").skip(1)\n",
    "\n",
    "for data in ds.take(2):\n",
    "    values = parse_csv(data)\n",
    "    pprint(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling the function in a for loop, we can wrap it in a `.map()` method to include it in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\").skip(1).map(parse_csv)\n",
    "\n",
    "for data in ds.take(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extend the `parse_csv` function.<br>\n",
    "In machine learning training, we want to pass training data in tuples `(features, label)`.\n",
    "\n",
    "In this CSV file we have these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head -1 ../data/taxi-train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to predict the `fare_amount` value, using `pickuplon`, `pickuplat`, `dropofflon` and `dropofflat` as features. If so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Update the `parse_csv` function to return a tuple of `(features, label)`. <br>\n",
    "The label should be the first column (`fare_amount`), and the features should be the 3rd through 6th columns (`pickuplon`, `pickuplat`, `dropofflon`, `dropofflat`). \n",
    "Remember to convert the extracted values to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_csv(row):\n",
    "    columns = tf.strings.split(row, \",\")\n",
    "    # Label: fare_amount\n",
    "    label = ...  # TODO: Your code goes here\n",
    "    # Feature: pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "    features = ...  # TODO: Your code goes here\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\").skip(1)\n",
    "ds = ds.map(parse_csv)\n",
    "\n",
    "for features, label in ds.take(2):\n",
    "    print(f\"features: \\n  {features}, \\nlabel: \\n  {label} \\n++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, a machine learning training module requires batched data. Let's refactor our pipeline to batch the data by adding `.batch(BATCH_SIZE)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\").skip(1)\n",
    "ds = ds.map(parse_csv).batch(BATCH_SIZE)\n",
    "\n",
    "for features, label in ds.take(2):\n",
    "    print(f\"features: \\n  {features}, \\nlabel: \\n  {label} \\n++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset is an iterator of *batches*, instead of *rows*, which is suitable for mini-batch training for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling & Repeating\n",
    "\n",
    "When training a deep learning model in batches over multiple workers, it is helpful if we shuffle the data. That way, different workers will be working on different parts of the input file at the same time, and so averaging gradients across workers will help.<br>\n",
    "We can add shuffling with `.shuffle()`. But please note that the shuffle buffer specified in `buffer_size` will be stored on memory, and it is not suitable for full shuffling on very large scale datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap our data pipeline in a `create_dataset` function so that we can control its behaviour and shuffle data only when the dataset is used for training.\n",
    "\n",
    "We will introduce an additional argument `mode` to our function to allow the function body to distinguish the case when it needs to shuffle the data (`mode == \"train\"`) from when it shouldn't (`mode == \"eval\"`).\n",
    "\n",
    "Also, let's add `.repeat()` to read the data indefinitely during training. \n",
    "\n",
    "**Exercise 5**: Implement the `create_dataset` function that takes a file pattern. It should:\n",
    "1. Create a `TextLineDataset` from the file pattern and skip the header row.\n",
    "2. Map the `parse_csv` function to each row.\n",
    "3. Make the dataset repeat indefinitely.\n",
    "4. If the mode is 'train', shuffle the dataset.\n",
    "5. Batch the dataset, dropping the remainder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size, mode=\"eval\"):\n",
    "    # TODO: Your code goes here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our function works well in both modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell multiple times to see the results are different.\n",
    "tempds = create_dataset(\"../data/taxi-train.csv\", 2, \"train\")\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempds = create_dataset(\"../data/taxi-valid.csv\", 2, \"eval\")\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Performance with tf.Data\n",
    "\n",
    "Maximizing the performance of data loading and preprocessing phase is critical for many machine learning use cases.\n",
    "\n",
    "`tf.data` offers a number of ways to optimize the process, depending on the cause of performance bottlenecks.<br>\n",
    "Let's take a look at some scenarios.\n",
    "\n",
    "For comparison, we use this `benchmark` function that simulates a training application loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Performance bottleneck in heavy map operation \n",
    "\n",
    "While feature transformation `.map()` is flexible and convenient, this process can be a performance bottleneck when the preprocessing function contains heavy operations.\n",
    "\n",
    "Let's simulate that case by adding sleep time into our parse function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def heavy_parse_csv(row):\n",
    "    columns = tf.strings.split(row, \",\")\n",
    "    label = tf.strings.to_number(columns[0])\n",
    "    features = tf.strings.to_number(columns[2:6])\n",
    "\n",
    "    # Perform a heavy preprocessing...\n",
    "    tf.py_function(lambda: time.sleep(0.001), [], ())\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size=128):\n",
    "    ds = tf.data.TextLineDataset(pattern).skip(1)\n",
    "    ds = ds.map(heavy_parse_csv)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tempds = create_dataset(\"../data/taxi-train.csv\")\n",
    "benchmark(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow looks like this. The map operation between data read and training is the bottleneck in this case.\n",
    "\n",
    "![Map bottleneck](https://www.tensorflow.org/guide/images/data_performance/sequential_map.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can optimize this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 1: Parallelize map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because input elements are independent of one another, the pre-processing can be parallelized across multiple CPU cores. To make this possible, the map transformation provides the num_parallel_calls argument to specify the level of parallelism.\n",
    "\n",
    "In `.map()` you can specify the `num_parallel_calls` arg along with the function. The number of parallelism can be auto-tuned by specifying `tf.data.AUTOTUNE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size=128):\n",
    "    ds = tf.data.TextLineDataset(pattern).skip(1)\n",
    "    ds = ds.map(heavy_parse_csv, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tempds = create_dataset(\"../data/taxi-train.csv\")\n",
    "benchmark(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is much faster!\n",
    "\n",
    "The flow now looks like this.\n",
    "![parallelized](https://www.tensorflow.org/guide/images/data_performance/parallel_map.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Solution 2: Vectorize the map operation\n",
    "`.map()` processes each individual element returned by a `Dataset`. Our current function is structured to work on one CSV element at a time. However, processing data in batches is always more efficient when feasible.\n",
    "\n",
    "Let's vectorize our function (that is, have it operate over a batch of inputs at once) and apply the `batch` transformation before the `map` transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def heavy_parse_csv_batch(row):\n",
    "    columns = tf.strings.split(row, \",\").to_tensor()\n",
    "    label = tf.strings.to_number(columns[:, 0])\n",
    "    features = tf.strings.to_number(columns[:, 2:6])\n",
    "\n",
    "    # Perform a heavy preprocessing...\n",
    "    tf.py_function(lambda: time.sleep(0.001), [], ())\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size=128):\n",
    "    ds = tf.data.TextLineDataset(pattern).skip(1)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds = ds.map(heavy_parse_csv_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tempds = create_dataset(\"../data/taxi-train.csv\")\n",
    "benchmark(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Performance Bottleneck in I/O (Data Loading) \n",
    "Let's take a look at the next scenario.\n",
    "\n",
    "In a real-world setting, the input data may be stored remotely (for example on Google Cloud Storage in a different location). A dataset pipeline that works well when reading data locally might become bottlenecked on I/O when reading data remotely because of the following differences between local and remote storage:\n",
    "\n",
    "- **Time-to-first-byte**: Reading the first byte of a file from remote storage can take orders of magnitude longer than from local storage.\n",
    "- **Read throughput**: While remote storage typically offers large aggregate bandwidth, reading a single file might only be able to utilize a small fraction of this bandwidth.\n",
    "\n",
    "In addition, once the raw bytes are loaded into memory, it may also be necessary to deserialize and/or decrypt the data (e.g. protobuf), which requires additional computation. This overhead is present irrespective of whether the data is stored locally or remotely, but can be worse in the remote case if data is not prefetched effectively.\n",
    "\n",
    "Let's create a custom dataset to simulate this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IOBoundDataset(tf.data.Dataset):\n",
    "    def _generator(file_name, num_samples):\n",
    "        # Opening the file\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        for sample_idx in range(num_samples):\n",
    "            # Reading each line from the file\n",
    "            time.sleep(0.15)\n",
    "\n",
    "            yield (sample_idx,)\n",
    "\n",
    "    def __new__(cls, file_name, num_samples=5):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_signature=tf.TensorSpec(shape=(1,), dtype=tf.int64),\n",
    "            args=(\n",
    "                file_name,\n",
    "                num_samples,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = IOBoundDataset(\"dummy_file.csv\").repeat(20)\n",
    "benchmark(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Solution 1: Cache\n",
    "Since machine learning training often involves using the same dataset repeatedly, a good strategy is to cache the data during the first epoch and then retrieve it from the cache in subsequent epochs, rather than reloading it from a remote source each time.\n",
    "\n",
    "You can simply insert `.cache()` to use caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = IOBoundDataset(\"dummy_file.csv\").cache().repeat(20)\n",
    "benchmark(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks much faster! However, be careful about what you cache since the cached data will be stored on memory. For example, it's not realistic to cache a terabyte-scale dataset.\n",
    "\n",
    "For example, if you have a CSV file that contains paths of videos for training, instead of caching the actual video file, consider caching the small CSV file by inserting `cache()` before the video load function.\n",
    "\n",
    "```python \n",
    "dataset.map(parse_csv_fn).cache().map(load_video_fn)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Solution 2: Interleave\n",
    "\n",
    "To mitigate the impact of the various data extraction overheads, the tf.data.Dataset.interleave transformation can be used to parallelize the data loading step, interleaving the contents of other datasets (such as data file readers).\n",
    "\n",
    "Let's say we have multiple sharded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"dummy_file_shard001.csv\",\n",
    "    \"dummy_file_shard002.csv\",\n",
    "    \"dummy_file_shard003.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can insert `interleave()` to interleave multiple file load operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "ds = ds.interleave(IOBoundDataset)\n",
    "\n",
    "benchmark(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how they are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for d in ds:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset contains values from 0 to 4, and here we can see the data load from 3 files are interleaved.\n",
    "\n",
    "This is the flow image of interleaving (with 2 files in this case)\n",
    "![sequential interleave](https://www.tensorflow.org/guide/images/data_performance/sequential_interleave.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `.map()`, you can parallelize this interleave operation by adding `num_parallel_calls` for further performance gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "ds = ds.interleave(IOBoundDataset, num_parallel_calls=len(files))\n",
    "\n",
    "benchmark(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parallel interleave](https://www.tensorflow.org/guide/images/data_performance/parallel_interleave.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By conbining these techniques, you can design a highly optimized data load and transform pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f0e28c-799f-4176-90db-5b4a53ebc6ab",
   "metadata": {},
   "source": [
    "# Semantic Search with Vertex Vector Search and PaLM Embeddings\n",
    "\n",
    "**Learning Objectives**\n",
    "  1. Learn how to create text embeddings using the Vertex PaLM API\n",
    "  1. Learn how to load embeddings in Vertex Vector Search\n",
    "  2. Learn how to query Vertex Vector Search\n",
    "  1. Learn how to build an information retrieval system based on semantic match\n",
    "  \n",
    "  \n",
    "In this notebook, we implement a simple (albeit fast and scalable) [semantic search](https://en.wikipedia.org/wiki/Semantic_search#:~:text=Semantic%20search%20seeks%20to%20improve,to%20generate%20more%20relevant%20results.) retrieval system using [Vertex Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview) and [Vertex PaLM Embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings). In a semantic search system, a number of documents are returned to a user query, ranked by their semantic match. This means that the returned documents should match the intent or meaning of the query rather than its actual exact  keywords as opposed to a boolean or keyword-based retrieval system. Such a semantic search system has in general two components, namely:\n",
    "\n",
    "* A component that produces semantically meaningful vector representations of both the documents as well as the user queries; we will use the [Vertex PaLM Embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings) API to creates these embeddings, leveraging the power of the [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) large language model developed at Google. \n",
    "\n",
    "* A component that allows users to store the document vector embeddings and retrieve the most relevant documents by returning the documents whose embeddings are the closest to the user-query embedding in the embedding space. We will use [Vertex Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview) which can scale up to billions of embeddings thanks to an [efficient approximate nearest neighbor strategy](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) to compare and retrieve the closest document vectors to a query vector based on a [recent paper from Google research](https://arxiv.org/abs/1908.10396).\n",
    "\n",
    "\n",
    "\n",
    "**Dataset:** We will use a very small subset of the [COVID-19 Open Research Dataset Challenge (CORD-19)\n",
    "](https://www.kaggle.com/datasets/allen-institute-for-ai/CORD-19-research-challenge), which contains around 1 million of medical research papers focused on COVID 19. We will focus on only 4000 titles, abstracts, and urls from 2021 only for the sake of speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f75e37a-b298-45db-96e5-8ab75aa7efa0",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319c211-03fe-4265-aa61-ee7b9a0f71c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from IPython import display\n",
    "from vertexai.language_models import TextEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddaf916-96df-4da8-be80-4ba9dd6fa619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = f\"{PROJECT}-cord19-semantic-search\"\n",
    "\n",
    "# Do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c5c0a-aa15-47be-808f-e34c856b7659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil ls gs://{BUCKET} || gsutil mb -l {REGION} gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21881016-af83-4b9a-b9c4-a097440f269b",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2187f1-072a-44b1-83af-f3fe5f0f3710",
   "metadata": {},
   "source": [
    "The dataset we will use is the title, abstract, and url metadata of a 4000 samples from the ~1 million medical papers in the [COVID-19 Open Research Dataset Challenge (CORD-19)\n",
    "](https://www.kaggle.com/datasets/allen-institute-for-ai/CORD-19-research-challenge). In this lab, we use the abstract as the documents, on which to compute and store the embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e5f77-baea-403b-8387-5be599b87519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/cord19_metadata_sample.csv.gz\")\n",
    "metadata = metadata[~metadata.abstract.isna()]\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db134b0d-9baa-47d9-b8a7-ba74e904eb50",
   "metadata": {},
   "source": [
    "## Creating the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59422c52-0aae-420a-b1a2-0c54824afccc",
   "metadata": {},
   "source": [
    "The first thing to do is to create embedding vectors for our 4000 abstracts. For that, we need to first instantiate the `TextEmbeddingModel` client with the appropriate version of the PaLM model, which is `textembedding-gecko` for embeddings. It is a smaller version than for text (`text-bison`) and chat (`chat-bison`) generation, and it allows for a faster processing time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a4a16-6015-4a11-b034-c0807bace95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cae5f0-436f-4351-8ba3-3dc53fdf6488",
   "metadata": {},
   "source": [
    "The embedding model can take up to a list of 5 texts to process at a single time. Because of that, we will iterate over the `metadata.abstract`'s in batches of 5 and feed these batches to `model.get_embbedings` to create the embeddings of all the 4000 abstracts, which we will then store in the list `vectors`. Running the next cell will take a couple of minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59bb0fa-79f7-4817-8652-7f56f42786f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_BATCH_SIZE = 5\n",
    "vectors = []\n",
    "\n",
    "for i in range(0, len(metadata), MAX_BATCH_SIZE):\n",
    "    batch = metadata.abstract[i : i + MAX_BATCH_SIZE].to_list()\n",
    "    embeddings = model.get_embeddings(batch)\n",
    "    vectors.extend([embedding.values for embedding in embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6109de9-2a7f-43aa-b70f-4496ad50be5c",
   "metadata": {},
   "source": [
    "## Creating the Vector Search engine input file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d8533-d98c-41a6-b5b3-36634c3525a1",
   "metadata": {},
   "source": [
    "At this point, our 4000 abstract embeddings are stored in memory in the `vectors` list. To store these embeddings into [Vertex Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview), we need to serialize them into a JSON file with the [following format](https://cloud.google.com/vertex-ai/docs/vector-search/setup/format-structure):\n",
    "\n",
    "```python\n",
    "{\"id\": <DOCUMENT_ID1>, \"embedding\": [0.1, ..., -0.7]}\n",
    "{\"id\": <DOCUMENT_ID2>, \"embedding\": [-0.4, ..., 0.8]}\n",
    "etc.\n",
    "```\n",
    "where the value of the `id` field should be an identifier allowing us to retrieve the actual document from a separate source, and the value of `embedding` is the vector returned by the PaLM API. \n",
    "\n",
    "For the document `id` we simply use the row index in the `metadata` DataFrame, which will serve as our in-memory document store. This makes it particularly easy to retrieve the abstract, title and url from an `id` returned by the vector search:\n",
    "\n",
    "```python\n",
    "metadata.abstract[id]\n",
    "metadata.title[id]\n",
    "metadata.url[id]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ac408-ea19-4350-b708-467dd4c392f4",
   "metadata": {},
   "source": [
    "The next cell iterates over `vectors` appending for each entry a JSON line as above to `cord19_embeddings.json` containing the index of the abstract in `metadata` as well as the embedding vector returned by PaLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85de3f2-9a4b-4b26-85b5-7f07d4e0c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file_path = \"cord19_embeddings.json\"\n",
    "\n",
    "# Removing the embedding file if it already exists\n",
    "!test -f {embeddings_file_path} && rm {embeddings_file_path}\n",
    "\n",
    "with open(embeddings_file_path, \"a\") as embeddings_file:\n",
    "    for i, embedding in enumerate(vectors):\n",
    "        json_line = json.dumps({\"id\": i, \"embedding\": embedding}) + \"\\n\"\n",
    "        embeddings_file.writelines(json_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e752f01-f353-4887-b15b-0d025bef04d6",
   "metadata": {},
   "source": [
    "Let us verify that our embedding file has 4000 lines, one per abstract, and then let us save it to a GCS bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e23c1-b96f-4749-99ce-8c95df337650",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l {embeddings_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3d22b-da6b-4eab-9f8d-210e7394d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_URI = f\"gs://{BUCKET}\"\n",
    "\n",
    "!gsutil cp {embeddings_file_path} {EMBEDDINGS_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b10e1-ad07-40e0-bfa3-38ed4ed8ce8c",
   "metadata": {},
   "source": [
    "## Creating the Vector Search engine index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8c2ea-8ddb-4e79-ad00-9726db109ec1",
   "metadata": {},
   "source": [
    "We are now up to the task of setting up [Vertex Vector Search](https://cloud.google.com/vertex-ai/docs/matching-engine/overview). The procedure requires two steps:\n",
    "\n",
    "1. The [creation of an index](https://cloud.google.com/vertex-ai/docs/vector-search/overview)\n",
    "1. The [deployment of this index to an endpoint](https://cloud.google.com/vertex-ai/docs/vector-search/deploy-index-public)\n",
    "\n",
    "While creating the index, the embedding vectors are uploaded to the matching engine and a tree-like data structure (the index) is created allowing for fast but approximate retrieval of the `approximate_neighbors_count` nearest neighbors of a given vector. The index depends on a notion of distance between embedding vectors that we need to specify in the `distance_measure_type`. We choose here the `COSINE_DISTANCE` which essentially is a measure of the angle between the embedding vectors. Other possible choices are the square of the euclidean distance (`SQUARED_L2_DISTANCE`), the [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry) (`L1_DISTANCE`), or the dot product distance (`DOT_PRODUCT_DISTANCE`). (Note that if the embeddings you are using have been trained to minimize the one of these distances between matching pairs, then you may get better results by selecting this particular distance, otherwise the `COSINE_DISTANCE` will do just fine.) \n",
    "\n",
    "The next cell creates the matching engine index from the embedding file. Running it will take up about 1 hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4ac9c-d0b1-4673-bab1-6f60e86b48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"cord19_vertex_palm_embeddings\"\n",
    "\n",
    "matching_engine_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    contents_delta_uri=EMBEDDINGS_URI,\n",
    "    dimensions=len(vectors[0]),\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"COSINE_DISTANCE\",\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=7,\n",
    "    description=DISPLAY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b792a5-1bce-43df-b104-04680af74ab4",
   "metadata": {},
   "source": [
    "Once the index is created it is associated with the resource name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb86d7-680f-44a2-9472-105e159eb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_RESOURCE_NAME = matching_engine_index.resource_name\n",
    "print(INDEX_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8b62c-1124-4594-8cac-16815a46efdc",
   "metadata": {},
   "source": [
    "In turns, this index resource-name can be used to instantiate an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0a525-3d12-4848-ad7f-99772cdb2f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_engine_index = aiplatform.MatchingEngineIndex(\n",
    "    index_name=INDEX_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd056fc-5502-4982-b943-c03bd77fc9c6",
   "metadata": {},
   "source": [
    "Now that our index is up and running, we need to make it accessible to be able to query it. The first step is to create a public endpoint (for speedups, one can also create a [private endpoint in a VPC network](https://cloud.google.com/vertex-ai/docs/vector-search/deploy-index-vpc)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaaeff3-d227-4d2b-817a-07cc09ca9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_engine_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    description=DISPLAY_NAME,\n",
    "    public_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644c8b9-6229-482d-a917-9740ec9fc188",
   "metadata": {},
   "source": [
    "The second step is to deploy the index to the endpoint we created: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccef87-6ddc-434e-8292-4bb0a0848631",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_INDEX_ID = \"cord19_vertex_palm_embeddings_deployed_index\"\n",
    "\n",
    "matching_engine = matching_engine_endpoint.deploy_index(\n",
    "    index=matching_engine_index, deployed_index_id=DEPLOYED_INDEX_ID\n",
    ")\n",
    "\n",
    "matching_engine.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96414f-80ef-480a-a3a3-c69389da8fd9",
   "metadata": {},
   "source": [
    "## Querying Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394442cd-70c1-4a1a-8cf5-fa43e1973152",
   "metadata": {},
   "source": [
    "We are now ready to issue queries to Vector Search! \n",
    "\n",
    "To begin with, we need to create a PaLM embedding from a user query: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7a045-0721-4e2f-9601-3254b35d6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"prophylactic measures\"\n",
    "\n",
    "text_embeddings = [vector.values for vector in model.get_embeddings([QUERY])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb8901-b2ff-4cc0-82f9-fcbc927fa553",
   "metadata": {},
   "source": [
    "Then we can use the `find_neighbors` method from our deployed Vector Search index. This method takes as input the embedding vector from the user query and returns the abstract id's of the `NUM_NEIGHBORS` nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd67c63-9278-46e2-b343-d731c64ae9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of neighbors to return\n",
    "NUM_NEIGHBORS = 10\n",
    "\n",
    "response = matching_engine.find_neighbors(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    queries=text_embeddings,\n",
    "    num_neighbors=NUM_NEIGHBORS,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a03d66-2ed7-4df3-87a9-5a3c214d89f8",
   "metadata": {},
   "source": [
    "The next cell formats the `NUM_NEIGHBORS` most relevant abstracts into a dataframe containing also the corresponding paper titles and urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb86676-d091-48d2-9d1a-10a5f4b952d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_ids = [int(match.id) for match in response[0]]\n",
    "matched_distances = [match.distance for match in response[0]]\n",
    "matched_titles = [metadata.title[i] for i in matched_ids]\n",
    "matched_abstracts = [metadata.abstract[i] for i in matched_ids]\n",
    "matched_urls = [metadata.url[i] for i in matched_ids]\n",
    "\n",
    "matches = pd.DataFrame(\n",
    "    {\n",
    "        \"distance\": matched_distances,\n",
    "        \"title\": matched_titles,\n",
    "        \"abstract\": matched_abstracts,\n",
    "        \"url\": matched_urls,\n",
    "    }\n",
    ")\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f493d6-5e73-4dc1-9404-f093111e18e0",
   "metadata": {},
   "source": [
    "Here is the Vector Search response formatted as a simple list for convenience. You may see in the list of returned papers some in a different language than english even though the query was in english. This demonstrates the muli-language ability of the PaLM large language model and illustrates that the matches are done on the basis of meaning meaning rather than exact keywords match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51e621-4cee-4f77-b876-b82ac369cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"<html><body><ol>\"\n",
    "for i in range(len(matches)):\n",
    "    html += f\"\"\"            \n",
    "    <li> \n",
    "        <article>\n",
    "            <header>\n",
    "                <a href=\"{matches.url[i]}\"> <h2>{matches.title[i]}</h2></a>\n",
    "            </header>\n",
    "            <p>{matches.abstract[i]}</p>\n",
    "        </article>\n",
    "    </li>\n",
    "    \"\"\"\n",
    "html += \"</body></html>\"\n",
    "display.HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f0192e-e1eb-4a51-b046-3fa63b040b49",
   "metadata": {},
   "source": [
    "## Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8defa4-940e-4384-85ad-69de7f284b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_engine.delete(force=True)\n",
    "matching_engine_index.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9607d5-49d7-4810-aeb8-d5d01a40606d",
   "metadata": {},
   "source": [
    "Copyright 2023 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "asl_kernel",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "ASL Kernel (Local)",
   "language": "python",
   "name": "asl_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

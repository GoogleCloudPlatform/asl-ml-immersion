{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02aa307c-a2b7-4927-98f5-51b27b908ba0",
   "metadata": {},
   "source": [
    "# Prompt Engineering with LLMs using Gemini\n",
    "\n",
    "**Learning Objective**\n",
    "\n",
    "1. Learn how to use Google Gen AI SDK to call Gemini\n",
    "1. Learn how to setup the Gemini parameters \n",
    "1. Learn prompt engineering for text generation\n",
    "1. Learn prompt engineering for chat applications\n",
    "\n",
    "\n",
    "The Google Gen AI SDK lets you test, customize, and deploy instances of Google's Gemini large language models (LLM) so that you can leverage the capabilities of Gemini in your applications. The Gemini family of models supports text completion, multi-turn chat, and text embeddings generation.\n",
    "\n",
    "This notebook will provide examples of accessing pre-trained Gemini models with the SDK for use cases like text classification, summarization, extraction, and chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30622e-7ae5-4092-bebf-80ecd3b874f6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee081e-f7d1-44a8-8bee-57a3b6fbd750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import Content, Part\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da340d5-828a-48cf-a030-e0596a918050",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "The cell below implements the helper function `generate_content_stream` to generate stream responses from Gemini using the Gen AI SDK. <br>\n",
    "If you don't need streaming outputs, use `generate_content` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45189fc6-4f04-4d42-ac9e-a95b005510ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6954a-a9ca-41dd-aa61-e6538e514f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt,\n",
    "    model_name=\"gemini-2.0-flash-001\",\n",
    "):\n",
    "    responses = client.models.generate_content_stream(\n",
    "        model=model_name, contents=prompt\n",
    "    )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cff7e-8203-4ec8-8dbd-82dd14e3b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = generate(\n",
    "    \"What are five important things to understand about large language models?\"\n",
    ")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e32f10-3419-4f9b-9be2-334b4e1b0f01",
   "metadata": {},
   "source": [
    "### Text Classification \n",
    "\n",
    "Now that we've tested our wrapper function, let us explore prompting for classification. Text classification is a common machine learning use-case and is frequently used for tasks like spam detection, sentiment analysis, topic classification, and more. \n",
    "\n",
    "Both **zero-shot** and **few-shot** prompting are common with text classification use cases. Zero-shot prompting is where you do not provide examples with labels in the input, and few-shot prompting is where you provide (a few) examples in the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095984db-e3f1-431c-8464-6ed9c745b380",
   "metadata": {},
   "source": [
    "Let us start with zero-shot classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c24b55-ce59-4abc-b66e-c4d054bb6750",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Write a zero-shot prompt that allows you to categorize a text into the following categories: \"technology\", \"polictics\", and \"sport\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc027f3b-84ec-46ab-95b1-5d6afd6d1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "TODO - Fill in your prompt here\n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d73f1-11ce-4581-90da-d69e76b57247",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Write a few-shot prompting for classification. Along with increasing the accuracy of your model, few-shot prompting gives you a certain control over the output format. The prompt should be able to classify a text into the categories \"dogs\" and \"cats\", and return only these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9c371-dc6c-4ef7-98ad-dc8058b88bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What is the topic for a given text? \n",
    "- cats \n",
    "- dogs \n",
    "\n",
    "Text: #TODO\n",
    "The answer is: #TODO\n",
    "\n",
    "Text: #TODO\n",
    "The answer is: #TODO\n",
    "\n",
    "#TODO - add more examples\n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff60e3-2e6d-4d70-988b-4940adb67c13",
   "metadata": {},
   "source": [
    "### Text Summarization\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Gemini can also be used for text summarization use cases. Text summarization produces a concise and fluent summary of a longer text document. \n",
    "In the cell below, write a prompt that can summarize the following text:\n",
    "\n",
    "```\n",
    "A transformer is a deep learning model. It is distinguished by its adoption of self-attention, \n",
    "differentially weighting the significance of each part of the input (which includes the \n",
    "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to \n",
    "process sequential input data, such as natural language, with applications towards tasks such \n",
    "as translation and text summarization. However, unlike RNNs, transformers process the \n",
    "entire input all at once. The attention mechanism provides context for any position in the \n",
    "input sequence. For example, if the input data is a natural language sentence, the transformer \n",
    "does not have to process one word at a time. This allows for more parallelization than RNNs \n",
    "and therefore reduces training times.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04163aa1-b750-4033-93b3-c08e572872c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837c16a-67cc-4bdb-ae66-4882d61279de",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Modify the prompt in the cell above so that it outputs 4 bullet point summary of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d03ec-ae6c-4d47-8d2e-09ebec182d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "#TODO\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df5150-b6ff-499e-adad-6ae6d5cc7b0c",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Consider the following dialog between a customer and service representative:\n",
    "\n",
    "```\n",
    "Customer: Hi! I'm reaching out to customer service because I am having issues.\n",
    "\n",
    "Service Rep: What seems to be the problem? \n",
    "\n",
    "Customer: I am trying to use Gemini but I keep getting an error. \n",
    "\n",
    "Service Rep: Can you share the error with me? \n",
    "\n",
    "Customer: Sure. The error says: \"ResourceExhausted: 429 Quota exceeded for \n",
    "      aiplatform.googleapis.com/online_prediction_requests_per_base_model \n",
    "      with base model: gemini-2.0-flash-001\"\n",
    "      \n",
    "Service Rep: It looks like you have exceeded the quota for usage. Please refer to \n",
    "             https://cloud.google.com/vertex-ai/docs/quotas for information about quotas\n",
    "             and limits. \n",
    "             \n",
    "Customer: Can you increase my quota?\n",
    "\n",
    "Service Rep: I cannot, but let me follow up with somebody who will be able to help.\n",
    "\n",
    "```\n",
    "\n",
    "Write a prompt that can give a short summary of what was said along with todo items for \n",
    "the support representative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594633f-ee1b-4a18-a11c-429b80e512e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "#TODO\n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd094d3-3026-4ff3-9306-357c50bc921d",
   "metadata": {},
   "source": [
    "### Text Extraction \n",
    "Gemini can be used to extract and structure text. Text extraction can be used for a variety of purposes. One common purpose is to convert documents into a machine-readable format. This can be useful for storing documents in a database or for processing documents with software. Another common purpose is to extract information from documents. This can be useful for finding specific information in a document or for summarizing the content of a document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003d56b-67cc-4dc5-9a29-288b5043af71",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Consider the following recipe:\n",
    "\n",
    "```\n",
    "Ingredients:\n",
    "* 1 tablespoon olive oil\n",
    "* 1 onion, chopped\n",
    "* 2 carrots, chopped\n",
    "* 2 celery stalks, chopped\n",
    "* 1 teaspoon ground cumin\n",
    "* 1/2 teaspoon ground coriander\n",
    "* 1/4 teaspoon turmeric powder\n",
    "* 1/4 teaspoon cayenne pepper (optional)\n",
    "* Salt and pepper to taste\n",
    "* 1 (15 ounce) can black beans, rinsed and drained\n",
    "* 1 (15 ounce) can kidney beans, rinsed and drained\n",
    "* 1 (14.5 ounce) can diced tomatoes, undrained\n",
    "* 1 (10 ounce) can diced tomatoes with green chilies, undrained\n",
    "* 4 cups vegetable broth\n",
    "* 1 cup chopped fresh cilantro\n",
    "```\n",
    "\n",
    "Write a zero-shot prompt that can return the ingredients in JSON format with keys:\n",
    "\"ingredient\", \"quantity\", and \"type\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b19b1-79cb-400a-87c7-6b13fe756c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669e27e-3bfe-4c95-8c11-dd424cc8a735",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Consider a product description of the type\n",
    "\n",
    "```\n",
    " Google Nest WiFi, network speed up to 1200Mpbs, 2.4GHz and 5GHz frequencies, WP3 protocol\n",
    "```\n",
    "\n",
    "Write a few-shot prompt that can output a the product characteristics in JSON format, as for example:\n",
    "\n",
    "```python\n",
    "JSON: {\n",
    "  \"product\":\"Google Nest WiFi\",\n",
    "  \"speed\":\"1200Mpbs\",\n",
    "  \"frequencies\": [\"2.4GHz\", \"5GHz\"],\n",
    "  \"protocol\":\"WP3\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc370e-b071-4377-b9c0-10e53c76c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "#TODO\n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdb835-a706-4f00-8f19-972f3fd447e4",
   "metadata": {},
   "source": [
    "## Prompt engineering for chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba2c36-d280-433f-b124-0106c8ce2c9c",
   "metadata": {},
   "source": [
    "The Gen AI SDK for chat is optimized for multi-turn chat. Multi-turn chat is when a model tracks the history of a chat conversation and then uses that history as the context for responses.\n",
    "\n",
    "Gemini enables you to have freeform conversations across multiple turns. The `Chat` class simplifies the process by managing the state of the conversation, so unlike with `generate_content`, you do not have to store the conversation history as a list.\n",
    "\n",
    "Let's initialize the chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fb4f8-df82-43c6-acc6-d93854d18c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = client.chats.create(model=\"gemini-2.0-flash-001\")\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239950b-cb28-4cb5-aedf-0278ed356e06",
   "metadata": {},
   "source": [
    "The `Chat.send_message` method returns the same `GenerateContentResponse` type as `client.models.generate_content`. It also appends your message and the response to the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f03267-2469-4b9a-9f37-eef780a2feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    \"In one sentence, explain how a computer works to a young child.\"\n",
    ")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825730b6-168f-455b-9ac3-f98a04e5dee2",
   "metadata": {},
   "source": [
    "Recall that within a chat session, history is preserved. This enables the model to remember things within a given chat session for context. You can get this history with the `get_history()` method of the chat session object. Notice that the history is simply a list of previous input/output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526265fd-85ff-452e-94ed-906d690361b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c520abca-dd89-4a1f-8e57-964ab856a5a4",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Ask a follow up question to our previous question using the `send_message()` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d279ee-4ac7-4b7e-901e-261d4b88191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = #TODO\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062fa76-69b4-461c-b2ff-37ace5427fd8",
   "metadata": {},
   "source": [
    "### Adding chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0a97a-fe2c-498c-81bb-2ca4164a7d8e",
   "metadata": {},
   "source": [
    "You can add chat history to a chat by adding messages from role user and model alternately. System messages can be set in the first part for the first message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b9209-300b-4107-87bc-36a7c195ae32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat2 = client.chats.create(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    history=[\n",
    "        Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                Part.from_text(\n",
    "                    text=\"\"\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.Who do you work for?\"\"\"\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "        Content(role=\"model\", parts=[Part.from_text(text=\"I work for Ned.\")]),\n",
    "        Content(role=\"user\", parts=[Part.from_text(text=\"What do I like?\")]),\n",
    "        Content(\n",
    "            role=\"model\",\n",
    "            parts=[Part.from_text(text=\"Ned likes watching movies.\")],\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "response = chat2.send_message(\"Are my favorite movies based on a book series?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55819d-862e-41ef-be09-0fbc6df3f0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat2.send_message(\"When were these books published?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ff364-3db0-49e7-a87e-4a2681595aa5",
   "metadata": {},
   "source": [
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

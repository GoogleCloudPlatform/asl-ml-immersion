{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation: Building an RNN Encoder-Decoder with Keras 3\n",
    "\n",
    "**Learning Objectives**\n",
    "1.  Learn how to build an efficient `tf.data.Dataset` pipeline for a seq2seq task.\n",
    "2.  Learn how to preprocess text using the `keras.layers.TextVectorization` layer.\n",
    "3.  Learn how to train an encoder-decoder model in Keras using the Functional API.\n",
    "4.  Learn how to create separate encoder and decoder models for inference.\n",
    "5.  Learn how to implement a translation (decoding) function from scratch.\n",
    "6.  Learn how to use the BLEU score to evaluate a translation model.\n",
    "\n",
    "\n",
    "***Note: For faster execution, please ensure you are using a GPU runtime. An NVIDIA T4 GPU is recommended for this notebook.***\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll build a Spanish-to-English translation model using a modern RNN encoder-decoder architecture with Keras 3.\n",
    "\n",
    "We will start by building an efficient and scalable input pipeline with the `tf.data.Dataset` API. A key part of our workflow will be using the `TextVectorization` layer to handle all text preprocessing—from standardization and tokenization to integer-encoding—directly within our model.\n",
    "\n",
    "Next, we will use the Keras Functional API to build and train our RNN encoder-decoder model. After training, we will create two specialized models—a dedicated encoder and a decoder—from the layers of our trained model. These specialized models are essential for performing inference, allowing us to implement a function that generates translations word by word.\n",
    "\n",
    "Finally, we'll evaluate the quality of our model's translations using the industry-standard BLEU score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 02:22:52.430403: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760494972.451957   31154 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760494972.458525   31154 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import evaluate\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import GRU, Dense, Embedding, Input\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "MODEL_PATH = \"translate_models/baseline\"\n",
    "DATA_URL = (\n",
    "    \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    ")\n",
    "LOAD_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a language dataset provided by http://www.manythings.org/anki/. The dataset contains Spanish-English  translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "The dataset is a curated list of 120K translation pairs from http://tatoeba.org/, a platform for community contributed translations by native speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation data stored at: /home/jupyter/.keras/datasets/spa-eng_extracted/spa-eng/spa.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = keras.utils.get_file(\"spa-eng.zip\", origin=DATA_URL, extract=True)\n",
    "\n",
    "path_to_file = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"spa-eng_extracted/spa-eng/spa.txt\"\n",
    ")\n",
    "print(\"Translation data stored at:\", path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    path_to_file, sep=\"\\t\", header=None, names=[\"english\", \"spanish\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>How boring!</td>\n",
       "      <td>¡Qué aburrimiento!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>I love sports.</td>\n",
       "      <td>Adoro el deporte.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55586</th>\n",
       "      <td>Would you like to swap jobs?</td>\n",
       "      <td>¿Te gustaría que intercambiemos los trabajos?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            english  \\\n",
       "1025                    How boring!   \n",
       "4338                 I love sports.   \n",
       "55586  Would you like to swap jobs?   \n",
       "\n",
       "                                             spanish  \n",
       "1025                              ¡Qué aburrimiento!  \n",
       "4338                               Adoro el deporte.  \n",
       "55586  ¿Te gustaría que intercambiemos los trabajos?  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tf.data Pipeline\n",
    "To begin, we'll construct our training and evaluation datasets using the `tf.data` API, which is the standard for building efficient and scalable input pipelines in TensorFlow. This approach allows us to handle data in a memory-efficient way and seamlessly integrate it with Keras.\n",
    "\n",
    "Our process will be as follows:\n",
    "\n",
    "1. Load the data: We create a tf.data.Dataset directly from our pandas DataFrame using tf.data.Dataset.from_tensor_slices. This creates a dataset where each element is a pair of (Spanish, English) sentences.\n",
    "\n",
    "2. Split the data: We'll use the .take() and .skip() methods to create our training and validation sets. This is a clean and efficient way to split the data without having to load everything into memory at once.\n",
    "\n",
    "3. Define a standardization function: We create a custom_standardization function to preprocess our raw text. This function replicates the logic of the original preprocess_sentence function by lowercasing text, adding spaces around punctuation, and, most importantly, adding <start> and <end> tokens to each sentence. It is built using tf.strings operations, which allows it to be embedded directly into our TextVectorization layer and run efficiently on the GPU/TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760494975.213441   31154 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "TEST_PROP = 0.2\n",
    "NUM_EXAMPLES = 30000\n",
    "\n",
    "# Create a single dataset from your pandas DataFrame\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data[\"spanish\"][:NUM_EXAMPLES], data[\"english\"][:NUM_EXAMPLES])\n",
    ")\n",
    "\n",
    "# Create the training and validation splits using take() and skip()\n",
    "TRAIN_SIZE = int(NUM_EXAMPLES * (1 - TEST_PROP))\n",
    "train_raw = full_dataset.take(TRAIN_SIZE)\n",
    "val_raw = full_dataset.skip(TRAIN_SIZE)\n",
    "\n",
    "# Define the custom standardization function for TextVectorization\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"Replicates the original preprocess_sentence function using tf.strings.\"\"\"\n",
    "    # Lowercase and strip leading/trailing whitespace\n",
    "    s = tf.strings.lower(input_string)\n",
    "    s = tf.strings.strip(s)\n",
    "\n",
    "    # Add spaces around punctuation\n",
    "    s = tf.strings.regex_replace(s, r\"([?.!,¿])\", r\" \\1 \")\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    s = tf.strings.regex_replace(s, r'[\" \"]+', \" \")\n",
    "\n",
    "    # Filter out unwanted characters, replacing them with a space\n",
    "    s = tf.strings.regex_replace(s, r\"[^a-zA-Z?.!,¿]+\", \" \")\n",
    "\n",
    "    # Strip again to remove any leading/trailing spaces created by cleaning\n",
    "    s = tf.strings.strip(s)\n",
    "\n",
    "    # Add the <start> and <end> tokens\n",
    "    s = tf.strings.join([\"<start>\", s, \"<end>\"], separator=\" \")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Adapt the TextVectorization Layers\n",
    "With our raw text datasets ready, we need to convert the sentences from strings into integer sequences that our model can understand. In Keras 3, the modern and efficient way to do this is with the `keras.layers.TextVectorization` layer. This layer handles tokenization, vocabulary creation, and integer-encoding directly within our model's graph.\n",
    "\n",
    "We will create two separate TextVectorization layers: one for the source language (Spanish) and one for the target language (English). We pass our custom_standardization function to the standardize argument to ensure the text is preprocessed according to our specific rules (including adding `<start>` and `<end>` tokens) before tokenization.\n",
    "\n",
    "The most crucial step is calling the `.adapt()` method. The `adapt` method reads through the text from our training dataset and builds a vocabulary of all the unique words. It's during this step that the layer learns the mapping from each word to a unique integer index. We do this for both the source and target vectorization layers on their respective text data.\n",
    "\n",
    "***NOTE: This cell takes 10-15 to run while it's adapting the pre-processing layer to our dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and adapt the TextVectorization layers\n",
    "source_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "target_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "# Adapt the layers to the training data\n",
    "source_text = train_raw.map(lambda x, y: x)\n",
    "target_text = train_raw.map(lambda x, y: y)\n",
    "\n",
    "source_vectorization.adapt(source_text)\n",
    "target_vectorization.adapt(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Final Datasets\n",
    "Now that our TextVectorization layers have learned the vocabularies, we can build our final input pipelines.\n",
    "\n",
    "First, we store the vocabulary sizes for later use in our model's Embedding layers. Then, we define two helper functions:\n",
    "\n",
    "1. `vectorize_text`: This function takes the raw text sentences and applies the appropriate TextVectorization layer to convert them into integer sequences.\n",
    "\n",
    "2. `create_dataset`: This function prepares the integerized sequences for our encoder-decoder model. The decoder needs two versions of the target sequence during training: one for input (to predict the next word) and one for output (to compare against the prediction for calculating loss). This function creates:\n",
    "   - target_in: The target sequence with the last token removed.\n",
    "   - target_out: The target sequence with the first (<start>) token removed.  \n",
    "\n",
    "Finally, we chain together several `tf.data` methods to construct our final train_dataset and eval_dataset. We `.shuffle()` the training data, `.batch()` both datasets, apply our preprocessing functions using `.map()`, and call `.prefetch()` for better performance. We also inspect the shape of the first batch to confirm our pipeline is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: (64, 10)\n",
      "Target in shape: (64, 8)\n",
      "Target out shape: (64, 8)\n"
     ]
    }
   ],
   "source": [
    "# Set vocabulary sizes\n",
    "INPUT_VOCAB_SIZE = source_vectorization.vocabulary_size()\n",
    "TARGET_VOCAB_SIZE = target_vectorization.vocabulary_size()\n",
    "\n",
    "\n",
    "def vectorize_text(source, target):\n",
    "    source = source_vectorization(source)\n",
    "    target = target_vectorization(target)\n",
    "    return source, target\n",
    "\n",
    "\n",
    "def create_dataset(source, target):\n",
    "    target_in = target[:, :-1]\n",
    "    target_out = target[:, 1:]\n",
    "    return (source, target_in), target_out\n",
    "\n",
    "\n",
    "# Create the final training and validation datasets\n",
    "train_dataset = (\n",
    "    train_raw.shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(vectorize_text)\n",
    "    .map(create_dataset)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "eval_dataset = (\n",
    "    val_raw.batch(BATCH_SIZE)\n",
    "    .map(vectorize_text)\n",
    "    .map(create_dataset)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Set max lengths for the Embedding layers\n",
    "# We do this by inspecting the element_spec of the dataset\n",
    "for (source, target_in), target_out in train_dataset.take(1):\n",
    "    max_length_inp = source.shape[1]\n",
    "    max_length_targ = target_in.shape[1]\n",
    "    print(\"Source shape:\", source.shape)\n",
    "    print(\"Target in shape:\", target_in.shape)\n",
    "    print(\"Target out shape:\", target_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview a Batch of Data\n",
    "To verify that our data pipeline is working correctly, let's take one batch from our train_dataset and inspect the first example. We'll convert the integer tensors back to text to see what the model will receive during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example from a Training Batch ---\n",
      "\n",
      "Source (Spanish):\n",
      "  Tensor: [ 2 12 45  4  3  0  0  0  0  0]\n",
      "  Text:   <start> me gusta . <end>\n",
      "\n",
      "Target Input (English - for Decoder):\n",
      "  Tensor: [ 2  5 38 10  4  3  0  0]\n",
      "  Text:   <start> i like it . <end>\n",
      "\n",
      "Target Output (English - for Loss Calculation):\n",
      "  Tensor: [ 5 38 10  4  3  0  0  0]\n",
      "  Text:   i like it . <end>\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabularies from the vectorization layers\n",
    "source_vocab = source_vectorization.get_vocabulary()\n",
    "source_index_lookup = {i: word for i, word in enumerate(source_vocab)}\n",
    "\n",
    "target_vocab = target_vectorization.get_vocabulary()\n",
    "target_index_lookup = {i: word for i, word in enumerate(target_vocab)}\n",
    "\n",
    "# Helper function to convert a tensor of token IDs back to a string\n",
    "\n",
    "\n",
    "def to_text(tensor, lookup_dict):\n",
    "    # Join the words, filtering out the padding token (ID 0)\n",
    "    return \" \".join([lookup_dict[i] for i in tensor.numpy() if i != 0])\n",
    "\n",
    "\n",
    "# Take one batch from the training dataset and inspect the first example\n",
    "for (source_batch, target_in_batch), target_out_batch in train_dataset.take(1):\n",
    "    # Get the first example from the batch\n",
    "    source_example = source_batch[0]\n",
    "    target_in_example = target_in_batch[0]\n",
    "    target_out_example = target_out_batch[0]\n",
    "\n",
    "    print(\"--- Example from a Training Batch ---\")\n",
    "    print(\"\\nSource (Spanish):\")\n",
    "    print(f\"  Tensor: {source_example.numpy()}\")\n",
    "    print(f\"  Text:   {to_text(source_example, source_index_lookup)}\")\n",
    "\n",
    "    print(\"\\nTarget Input (English - for Decoder):\")\n",
    "    print(f\"  Tensor: {target_in_example.numpy()}\")\n",
    "    print(f\"  Text:   {to_text(target_in_example, target_index_lookup)}\")\n",
    "\n",
    "    print(\"\\nTarget Output (English - for Loss Calculation):\")\n",
    "    print(f\"  Tensor: {target_out_example.numpy()}\")\n",
    "    print(f\"  Text:   {to_text(target_out_example, target_index_lookup)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RNN encoder-decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an encoder-decoder architecture, however we embed our words into a latent space prior to feeding them into the RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_UNITS = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Encoder\n",
    "We will build our translator using a standard encoder-decoder architecture. The encoder's job is to process the entire input sentence and compress its meaning into a fixed-size vector, often called the \"thought vector\" or \"context vector.\"\n",
    "\n",
    "We'll use the Keras Functional API to construct the encoder, which gives us a clear way to define the flow of data. Our encoder will have two main layers:\n",
    "\n",
    "1. **Embedding Layer**: This layer takes the integer-encoded vocabulary and learns a dense vector representation (an embedding) for each word. These embeddings can capture semantic relationships between words.\n",
    "\n",
    "2. **GRU (Gated Recurrent Unit) Layer**: This is a type of Recurrent Neural Network (RNN) that processes the sequence of word embeddings one by one. We configure it with return_state=True to get the final hidden state of the GRU after it has processed the entire input sentence. This final hidden state is the \"thought vector\" that encapsulates the meaning of the input sentence and will be passed to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), name=\"encoder_input\")\n",
    "\n",
    "encoder_inputs_embedded = Embedding(\n",
    "    input_dim=INPUT_VOCAB_SIZE,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    name=\"encoder_embedding\",\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_rnn = GRU(\n",
    "    units=HIDDEN_UNITS,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer=\"glorot_uniform\",\n",
    "    name=\"encoder_gru\",\n",
    "    reset_after=False,\n",
    ")\n",
    "\n",
    "encoder_outputs, encoder_state = encoder_rnn(encoder_inputs_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Decoder\n",
    "The decoder takes the \"thought vector\" from the encoder and generates the translated sentence word by word.\n",
    "\n",
    "Its architecture mirrors the encoder, containing Embedding and GRU layers. The crucial difference is that we initialize the decoder's GRU layer with the final hidden state of the encoder `(initial_state=encoder_state)`. This is how the decoder receives the context from the source sentence, which it then uses to generate the correct translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,), name=\"decoder_input\")\n",
    "\n",
    "decoder_inputs_embedded = Embedding(\n",
    "    input_dim=TARGET_VOCAB_SIZE,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    name=\"decoder_embedding\",\n",
    ")(decoder_inputs)\n",
    "\n",
    "decoder_rnn = GRU(\n",
    "    units=HIDDEN_UNITS,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer=\"glorot_uniform\",\n",
    "    name=\"decoder_gru\",\n",
    "    reset_after=False,\n",
    ")\n",
    "\n",
    "decoder_outputs, decoder_state = decoder_rnn(\n",
    "    decoder_inputs_embedded, initial_state=encoder_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the encoder-decoder architecture is a softmax `Dense` layer that will create the next word probability vector or next word `predictions` from the `decoder_output`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder_dense = Dense(TARGET_VOCAB_SIZE, activation=\"softmax\", name=\"dense\")\n",
    "\n",
    "predictions = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Compile the Model\n",
    "To create the complete trainable model, we instantiate a Model object and specify the encoder and decoder inputs, and the final predictions as the output.\n",
    "\n",
    "We then compile the model, configuring it for training. We use the `adam` optimizer and the `sparse_categorical_crossentropy` loss function. This loss is ideal for our task because our targets are integers (the word indices) and the model's output is a probability distribution over the vocabulary.\n",
    "\n",
    "Finally, we call `.summary()` to print a useful overview of the model's architecture, including the layers, output shapes, and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,970,688</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,079,552</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)   │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,935,232</span> │ encoder_embeddin… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)]            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)   │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,935,232</span> │ decoder_embeddin… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │            │ encoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)]            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,322,425</span> │ decoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">4217</span>)             │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m1,970,688\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m1,079,552\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_gru (\u001b[38;5;33mGRU\u001b[0m)   │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m3,935,232\u001b[0m │ encoder_embeddin… │\n",
       "│                     │ \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │            │                   │\n",
       "│                     │ \u001b[38;5;34m1024\u001b[0m)]            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_gru (\u001b[38;5;33mGRU\u001b[0m)   │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m3,935,232\u001b[0m │ decoder_embeddin… │\n",
       "│                     │ \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │            │ encoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m1024\u001b[0m)]            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m4,322,425\u001b[0m │ decoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m4217\u001b[0m)             │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,243,129</span> (58.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,243,129\u001b[0m (58.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,243,129</span> (58.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,243,129\u001b[0m (58.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=predictions)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model!\n",
    "\n",
    "***NOTE: Update the number of `EPOCHS` to 10/15 to get a decent translation performance. However, this will increase the training time.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760495324.894985   31190 service.cc:148] XLA service 0x7f6fd400d6c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1760495324.895027   31190 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "W0000 00:00:1760495325.163860   31190 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1760495325.322773   31190 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1760495328.054608   31190 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "W0000 00:00:1760495328.328663   31191 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495331.116997   31190 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495333.152728   31192 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495335.240146   31190 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495337.547772   31193 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495340.405585   31192 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495342.441677   31192 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495344.553950   31191 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495347.387653   31190 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495350.033355   31190 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495352.793247   31191 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495354.898687   31192 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495357.598453   31192 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495361.338674   31192 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495365.747850   31190 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495369.119518   31190 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495376.438220   31191 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495379.024837   31190 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1760495379.576393   31193 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 - 67s - 178ms/step - loss: 2.7221 - val_loss: 2.7872\n",
      "Epoch 2/5\n",
      "375/375 - 20s - 54ms/step - loss: 1.7850 - val_loss: 2.3783\n",
      "Epoch 3/5\n",
      "375/375 - 18s - 49ms/step - loss: 1.2807 - val_loss: 2.1386\n",
      "Epoch 4/5\n",
      "375/375 - 17s - 44ms/step - loss: 0.8743 - val_loss: 2.0280\n",
      "Epoch 5/5\n",
      "375/375 - 16s - 43ms/step - loss: 0.5724 - val_loss: 2.0372\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, validation_data=eval_dataset, epochs=EPOCHS, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing the Translation (Inference) Function\n",
    "\n",
    "Now that our model is trained, we need a way to use it for translation. We can't simply call `model.predict()` on a Spanish sentence because the model expects both a source (Spanish) and a target (English) sentence as input. During training, we used the ground-truth target sentence in a technique called \"teacher forcing.\" For inference, we don't have the target sentence—that's what we need to generate!\n",
    "\n",
    "The solution is to generate the translation word by word in a loop. The process works like this:\n",
    "\n",
    "1.  Take the input sentence (e.g., \"No estamos comiendo.\") and pass it through the encoder to get its final hidden state (the \"thought vector\").\n",
    "2.  Start the decoder with the `<start>` token.\n",
    "3.  Feed the `<start>` token and the encoder's state into the decoder to predict the first word of the translation.\n",
    "4.  Take the predicted word, feed it back into the decoder as input for the next step, and use the new decoder state.\n",
    "5.  Repeat this process until the decoder predicts the `<end>` token, signaling that the translation is complete.\n",
    "\n",
    "To implement this, we will create two new, specialized models using the layers from the model we just trained:\n",
    "\n",
    "*   An **`encoder_model`** that takes a raw string sentence, vectorizes it, and returns the encoder's final hidden state.\n",
    "*   A **`decoder_model`** that takes the current predicted sequence and a hidden state, and returns the prediction for the next word, along with the updated hidden state.\n",
    "\n",
    "The following cells will define these two models and the `decode_sequences` function that uses them to perform the translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Create Inference Encoder Model ---\n",
    "# This model will convert raw Spanish sentences into the encoder's final state.\n",
    "\n",
    "# Input layer for raw strings\n",
    "encoder_string_input = keras.Input(\n",
    "    shape=(1,), dtype=\"string\", name=\"encoder_string_input\"\n",
    ")\n",
    "\n",
    "# Vectorize the strings using the adapted layer\n",
    "encoder_vectorized = source_vectorization(encoder_string_input)\n",
    "\n",
    "# Reuse the trained Embedding layer\n",
    "encoder_embedding_layer = model.get_layer(\"encoder_embedding\")\n",
    "encoder_embedded = encoder_embedding_layer(encoder_vectorized)\n",
    "\n",
    "# Reuse the trained GRU layer\n",
    "encoder_gru_layer = model.get_layer(\"encoder_gru\")\n",
    "_, encoder_state_output = encoder_gru_layer(encoder_embedded)\n",
    "\n",
    "# Create the final encoder model for inference\n",
    "encoder_model = keras.Model(\n",
    "    inputs=encoder_string_input,\n",
    "    outputs=encoder_state_output,\n",
    "    name=\"inference_encoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Create Inference Decoder Model ---\n",
    "# This model's structure is similar to the original, as it works with token IDs.\n",
    "\n",
    "# Input layers for the decoder\n",
    "decoder_input = keras.Input(shape=(None,), name=\"decoder_input\")\n",
    "decoder_state_input = keras.Input(\n",
    "    shape=(HIDDEN_UNITS,), name=\"decoder_state_input\"\n",
    ")\n",
    "\n",
    "# Reuse trained layers from the main model\n",
    "decoder_embedding_layer = model.get_layer(\"decoder_embedding\")\n",
    "decoder_gru_layer = model.get_layer(\"decoder_gru\")\n",
    "decoder_dense_layer = model.get_layer(\"dense\")\n",
    "\n",
    "# Build the decoder graph\n",
    "decoder_embedded = decoder_embedding_layer(decoder_input)\n",
    "decoder_outputs, decoder_state_output = decoder_gru_layer(\n",
    "    decoder_embedded, initial_state=decoder_state_input\n",
    ")\n",
    "decoder_predictions = decoder_dense_layer(decoder_outputs)\n",
    "\n",
    "# Create the final decoder model for inference\n",
    "decoder_model = keras.Model(\n",
    "    inputs=[decoder_input, decoder_state_input],\n",
    "    outputs=[decoder_predictions, decoder_state_output],\n",
    "    name=\"inference_decoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the Translation\n",
    "\n",
    "With our specialized encoder and decoder models ready, we can now define the main `decode_sequences` function that ties everything together to perform the translation.\n",
    "\n",
    "This function implements the step-by-step decoding process:\n",
    "\n",
    "1.  **Encode the input**: The raw input sentences are passed to the `encoder_model` to get the initial \"thought vector\" or hidden state.\n",
    "\n",
    "2.  **Initialize the sequence**: A target sequence is created, containing only the `<start>` token for each sentence in the batch.\n",
    "\n",
    "3.  **Iteratively decode**: In a loop, the `decoder_model` is called with the current target sequence and the hidden state to predict the next token. For simplicity, we use `argmax` to select the most probable token as our prediction.\n",
    "\n",
    "4.  **Append and update**: The predicted token is appended to our result, and the process is repeated with the new token and the updated hidden state from the decoder.\n",
    "\n",
    "5.  **Stop condition**: The loop continues until the special `<end>` token is generated or the maximum sequence length is reached.\n",
    "\n",
    "Finally, we'll test our function with a few example sentences to see the translation results in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the vocabulary and start/end token IDs from the adapted TextVectorization layer\n",
    "targ_vocab = target_vectorization.get_vocabulary()\n",
    "targ_index_lookup = dict(zip(range(len(targ_vocab)), targ_vocab))\n",
    "start_token_id = target_vectorization.get_vocabulary().index(\"<start>\")\n",
    "end_token_id = target_vectorization.get_vocabulary().index(\"<end>\")\n",
    "\n",
    "\n",
    "def decode_sequences(input_sentences, max_decode_length=50):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        input_sentences: A list of raw strings in the source language.\n",
    "    Returns:\n",
    "        A list of translated sentences.\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    # Encode the input strings to get the initial state.\n",
    "    states_value = encoder_model(input_sentences)\n",
    "\n",
    "    # Initialize the target sequence with the <start> token ID.\n",
    "    target_seq = tf.fill([batch_size, 1], start_token_id)\n",
    "    decoded_sentences = [\"\" for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(max_decode_length):\n",
    "        output_tokens, decoder_state = decoder_model([target_seq, states_value])\n",
    "\n",
    "        # Sample a token (we use argmax for simplicity)\n",
    "        sampled_token_index = tf.argmax(output_tokens[:, -1, :], axis=-1)\n",
    "\n",
    "        for j in range(batch_size.numpy()):\n",
    "            token = targ_index_lookup[sampled_token_index[j].numpy()]\n",
    "            if token == \"<end>\":\n",
    "                continue  # Don't add the end token to the output\n",
    "            decoded_sentences[j] += token + \" \"\n",
    "\n",
    "        # Update the target sequence for the next iteration\n",
    "        target_seq = tf.expand_dims(sampled_token_index, axis=-1)\n",
    "\n",
    "        # Update the decoder state\n",
    "        states_value = decoder_state\n",
    "\n",
    "    return [s.strip() for s in decoded_sentences]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sentences = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "]\n",
    "\n",
    "machine_translations = decode_sequences(tf.constant(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "INPUT:\n",
      "No estamos comiendo.\n",
      "REFERENCE TRANSLATION:\n",
      "We're not eating.\n",
      "MACHINE TRANSLATION:\n",
      "we re not eating .\n",
      "-\n",
      "INPUT:\n",
      "Está llegando el invierno.\n",
      "REFERENCE TRANSLATION:\n",
      "Winter is coming.\n",
      "MACHINE TRANSLATION:\n",
      "they re doing worse .\n",
      "-\n",
      "INPUT:\n",
      "El invierno se acerca.\n",
      "REFERENCE TRANSLATION:\n",
      "Winter is coming.\n",
      "MACHINE TRANSLATION:\n",
      "the worst is over .\n",
      "-\n",
      "INPUT:\n",
      "Tom no comio nada.\n",
      "REFERENCE TRANSLATION:\n",
      "Tom ate nothing.\n",
      "MACHINE TRANSLATION:\n",
      "tom won t answer .\n",
      "-\n",
      "INPUT:\n",
      "Su pierna mala le impidió ganar la carrera.\n",
      "REFERENCE TRANSLATION:\n",
      "His bad leg prevented him from winning the race.\n",
      "MACHINE TRANSLATION:\n",
      "his novel sold well .\n",
      "-\n",
      "INPUT:\n",
      "Su respuesta es erronea.\n",
      "REFERENCE TRANSLATION:\n",
      "Your answer is wrong.\n",
      "MACHINE TRANSLATION:\n",
      "the pen is broken .\n",
      "-\n",
      "INPUT:\n",
      "¿Qué tal si damos un paseo después del almuerzo?\n",
      "REFERENCE TRANSLATION:\n",
      "How about going for a walk after lunch?\n",
      "MACHINE TRANSLATION:\n",
      "how about i m here ?\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "    \"El invierno se acerca.\",\n",
    "    \"Tom no comio nada.\",\n",
    "    \"Su pierna mala le impidió ganar la carrera.\",\n",
    "    \"Su respuesta es erronea.\",\n",
    "    \"¿Qué tal si damos un paseo después del almuerzo?\",\n",
    "]\n",
    "\n",
    "reference_translations = [\n",
    "    \"We're not eating.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Tom ate nothing.\",\n",
    "    \"His bad leg prevented him from winning the race.\",\n",
    "    \"Your answer is wrong.\",\n",
    "    \"How about going for a walk after lunch?\",\n",
    "]\n",
    "\n",
    "machine_translations = decode_sequences(tf.constant(sentences))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(\"-\")\n",
    "    print(\"INPUT:\")\n",
    "    print(sentences[i])\n",
    "    print(\"REFERENCE TRANSLATION:\")\n",
    "    print(reference_translations[i])\n",
    "    print(\"MACHINE TRANSLATION:\")\n",
    "    print(machine_translations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric (BLEU)\n",
    "\n",
    "Unlike say, image classification, there is no one right answer for a machine translation. However our current loss metric, cross entropy, only gives credit when the machine translation matches the exact same word in the same order as the reference translation. \n",
    "\n",
    "Many attempts have been made to develop a better metric for natural language evaluation. The most popular currently is Bilingual Evaluation Understudy (BLEU).\n",
    "\n",
    "- It is quick and inexpensive to calculate.\n",
    "- It allows flexibility for the ordering of words and phrases.\n",
    "- It is easy to understand.\n",
    "- It is language independent.\n",
    "- It correlates highly with human evaluation.\n",
    "- It has been widely adopted.\n",
    "\n",
    "The score is from 0 to 1, where 1 is an exact match.\n",
    "\n",
    "It works by counting matching n-grams between the machine and reference texts, regardless of order. BLUE-4 counts matching n grams from 1-4 (1-gram, 2-gram, 3-gram and 4-gram). It is common to report both BLUE-1 and BLUE-4\n",
    "\n",
    "It still is imperfect, since it gives no credit to synonyms and so human evaluation is still best when feasible. However BLEU is commonly considered the best among bad options for an automated metric.\n",
    "\n",
    "The Hugging Face evaluate framework has an implementation that we will use.\n",
    "\n",
    "We can't run calculate BLEU during training, because at that time the correct decoder input is used. Instead we'll calculate it now.\n",
    "\n",
    "For more info: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess(sentence):\n",
    "    filtered = list(filter(lambda x: x != \"\" and x != \"<end>\", sentence))\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now average the `bleu_1` and `bleu_4` scores for all the sentence pairs in the eval set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3739.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Source:    No lo dije en serio.\n",
      "Reference: I meant it as a joke.\n",
      "Machine:   i didn t say that .\n",
      "--------------------\n",
      "Source:    Ayer me encontré con Mary.\n",
      "Reference: I met Mary yesterday.\n",
      "Machine:   i saw her tie mary .\n",
      "--------------------\n",
      "Source:    Me encontré con Tom después del trabajo.\n",
      "Reference: I met Tom after work.\n",
      "Machine:   i saw tom on mary .\n",
      "--------------------\n",
      "Source:    Encontré a Tom después del trabajo.\n",
      "Reference: I met Tom after work.\n",
      "Machine:   i saw tom on monday .\n",
      "--------------------\n",
      "Source:    Me encontré en el camino con Tom.\n",
      "Reference: I met Tom on the way.\n",
      "Machine:   i saw tom on tv .\n"
     ]
    }
   ],
   "source": [
    "NUM_EVALUATE = 1000\n",
    "\n",
    "source_sentences = []\n",
    "reference = []  # This will be a list of lists for the BLEU score calculation\n",
    "\n",
    "# Iterate over the raw validation dataset to get the source and reference texts.\n",
    "# We use .numpy().decode() to convert the EagerTensors from the dataset into strings.\n",
    "for spa, eng in tqdm(val_raw.take(NUM_EVALUATE), total=NUM_EVALUATE):\n",
    "    source_sentences.append(spa.numpy().decode(\"utf-8\"))\n",
    "    reference.append([eng.numpy().decode(\"utf-8\")])\n",
    "\n",
    "# Generate all machine translations in a single batch for better performance.\n",
    "candidate = decode_sequences(tf.constant(source_sentences))\n",
    "\n",
    "# Print a few examples to see the results\n",
    "for i in range(5):\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Source:    {source_sentences[i]}\")\n",
    "    print(f\"Reference: {reference[i][0]}\")\n",
    "    print(f\"Machine:   {candidate[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_1 = bleu.compute(predictions=candidate, references=reference, max_order=1)\n",
    "bleu_4 = bleu.compute(predictions=candidate, references=reference, max_order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3737112580575483"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_1[\"bleu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06999233485937993"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_4[\"bleu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

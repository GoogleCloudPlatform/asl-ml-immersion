{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Fraudfinder - Inference Demo (New Feature Store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "This series of labs are updated upon [FraudFinder](https://github.com/googlecloudplatform/fraudfinder) repository which builds a end-to-end real-time fraud detection system on Google Cloud. Throughout the FraudFinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook demonstrates a critical step in a real-time fraud detection system: **making a prediction on a new, incoming transaction.** We'll simulate a real-world scenario where a transaction is received and we need to quickly determine if it's fraudulent.\n",
    "\n",
    "To do this, we'll use a deployed machine learning model on a **Vertex AI Endpoint**. However, a model needs more than just the raw transaction data to make an accurate prediction; it needs **features**. These are the data points that give the model context about the transaction, the customer, and the terminal involved.\n",
    "\n",
    "This is where the **Vertex AI Feature Store** comes in. The Feature Store provides low-latency access to pre-calculated features. In this notebook, we will:\n",
    "\n",
    "1. **Simulate a new transaction** by pulling a sample transaction from a Pub/Sub topic.\n",
    "2. **Enrich the transaction data** by fetching the latest customer and terminal features from the Vertex AI Feature Store.\n",
    "3. **Construct a prediction request** with the combined transaction data and features.\n",
    "4. **Send the request to the Vertex AI Endpoint** to get a real-time fraud prediction.\n",
    "\n",
    "This entire process mirrors a production-level inference pipeline, showcasing how to leverage Vertex AI services to build a powerful and responsive fraud detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf",
    "tags": []
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries\n",
    "Here, we'll import the necessary libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform_v1 import (\n",
    "    FeatureOnlineStoreServiceClient,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    feature_online_store_service as feature_online_store_service_pb2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vertex AI SDK\n",
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_from_sub(project_id, subscription_name, messages=10):\n",
    "    \"\"\"\n",
    "    Read messages from a Pub/Sub subscription\n",
    "    Args:\n",
    "        project_id: project ID\n",
    "        subscription_name: the name of a Pub/Sub subscription in your project\n",
    "        messages: number of messages to read\n",
    "    Returns:\n",
    "        msg_data: list of messages in your Pub/Sub subscription as a Python dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    import ast\n",
    "\n",
    "    from google.api_core import retry\n",
    "    from google.cloud import pubsub_v1\n",
    "\n",
    "    subscriber = pubsub_v1.SubscriberClient()\n",
    "    subscription_path = subscriber.subscription_path(\n",
    "        project_id, subscription_name\n",
    "    )\n",
    "\n",
    "    # Wrap the subscriber in a 'with' block to automatically call close() to\n",
    "    # close the underlying gRPC channel when done.\n",
    "    with subscriber:\n",
    "        # The subscriber pulls a specific number of messages. The actual\n",
    "        # number of messages pulled may be smaller than max_messages.\n",
    "        response = subscriber.pull(\n",
    "            subscription=subscription_path,\n",
    "            max_messages=messages,\n",
    "            retry=retry.Retry(deadline=300),\n",
    "        )\n",
    "\n",
    "        if len(response.received_messages) == 0:\n",
    "            print(\"no messages\")\n",
    "            return\n",
    "\n",
    "        ack_ids = []\n",
    "        msg_data = []\n",
    "        for received_message in response.received_messages:\n",
    "            msg = ast.literal_eval(\n",
    "                received_message.message.data.decode(\"utf-8\")\n",
    "            )\n",
    "            msg_data.append(msg)\n",
    "            ack_ids.append(received_message.ack_id)\n",
    "\n",
    "        # Acknowledges the received messages so they will not be sent again.\n",
    "        subscriber.acknowledge(subscription=subscription_path, ack_ids=ack_ids)\n",
    "\n",
    "        print(\n",
    "            f\"Received and acknowledged {len(response.received_messages)} messages from {subscription_path}.\"\n",
    "        )\n",
    "\n",
    "        return msg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Deployed Model Endpoint\n",
    "\n",
    "Before we can get a prediction, we need to connect to our deployed model. In Vertex AI, deployed models are exposed as **Endpoints**. An endpoint provides a URL that you can send prediction requests to. The following code retrieves the endpoint for our trained fraud detection model. We filter by the `display_name` to ensure we get the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoints = vertex_ai.Endpoint.list(\n",
    "    filter=f\"display_name={ENDPOINT_NAME}\",  # optional: filter by specific endpoint name\n",
    "    order_by=\"update_time\",\n",
    ")\n",
    "\n",
    "ENDPOINT_ID = endpoints[-1].name\n",
    "print(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(ENDPOINT_ID)\n",
    "from google.cloud import aiplatform as aiplatform\n",
    "\n",
    "# Instantiate Vertex AI Endpoint object\n",
    "endpoint_obj = aiplatform.Endpoint(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate an Incoming Transaction\n",
    "\n",
    "In a real-world system, new transactions would be continuously streaming in. To simulate this, we'll read a single transaction from the `ff-tx-sub` Pub/Sub subscription. This subscription receives the same raw transaction data that our real-time feature engineering pipeline processes. The `read_from_sub` helper function will pull one message for us to use in our prediction request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "messages_tx = \"no messages\"\n",
    "while messages_tx == \"no messages\":\n",
    "    messages_tx = read_from_sub(\n",
    "        project_id=PROJECT_ID, subscription_name=\"ff-tx-sub\", messages=1\n",
    "    )\n",
    "    print(messages_tx)\n",
    "    time.sleep(5)  # Sleep for 5 seconds\n",
    "\n",
    "messages_tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to the Vertex AI Feature Store\n",
    "\n",
    "Now that we have our model endpoint, we need a way to get the features for our incoming transaction. This is where the **Vertex AI Feature Store** is essential. It's designed for low-latency, real-time feature lookups, which is exactly what's needed in a fraud detection scenario.\n",
    "\n",
    "To connect to the Feature Store, we'll instantiate a `FeatureOnlineStoreServiceClient`. This client will allow us to query the online store for the latest feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "\n",
    "data_client = FeatureOnlineStoreServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the Feature Store Lookup Function\n",
    "\n",
    "To make it easier to retrieve features, we'll define a helper function called `fs_features_lookup`. This function will take the Feature Store ID, the type of feature we're looking for (e.g., `customers` or `terminals`), and an entity ID (e.g., a specific customer ID) as input.\n",
    "\n",
    "Inside the function, we'll construct the full ID of the **Feature View** we want to query. A Feature View is a logical grouping of features from a single entity type. The function then uses the `data_client` to call the `fetch_feature_values` method, which returns the latest feature values for the given entity ID. This function is the core of our real-time feature enrichment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(compact=True)\n",
    "\n",
    "\n",
    "def fs_features_lookup(ff_feature_store, features_type, features_key):\n",
    "\n",
    "    FEATURE_VIEW_ID = f\"fv_fraudfinder_{features_type}\"\n",
    "    FEATURE_VIEW_FULL_ID = f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{ff_feature_store}/featureViews/{FEATURE_VIEW_ID}\"\n",
    "\n",
    "    features_map = {}\n",
    "\n",
    "    print(FEATURE_VIEW_FULL_ID)\n",
    "\n",
    "    try:\n",
    "        fe_continuous_data = data_client.fetch_feature_values(\n",
    "            request=feature_online_store_service_pb2.FetchFeatureValuesRequest(\n",
    "                feature_view=FEATURE_VIEW_FULL_ID,\n",
    "                data_key=feature_online_store_service_pb2.FeatureViewDataKey(\n",
    "                    key=features_key\n",
    "                ),\n",
    "                data_format=feature_online_store_service_pb2.FeatureViewDataFormat.PROTO_STRUCT,\n",
    "            )\n",
    "        )\n",
    "        features_map.update(\n",
    "            {k: v for k, v in fe_continuous_data.proto_struct.items()}\n",
    "        )\n",
    "    except Exception as exp:\n",
    "        print(f\"Requested entity {features_key} was not found\")\n",
    "    return features_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test the Feature Lookup\n",
    "\n",
    "Before we build the full prediction request, let's test our `fs_features_lookup` function. We'll get a recent `entity_id` from our streaming features table in BigQuery and use it to fetch the corresponding customer features from the Feature Store. This helps us verify that our connection to the Feature Store is working and see what the feature data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery customer_strm_record --project {PROJECT_ID}\n",
    "SELECT * FROM tx.t_customers_streaming_features\n",
    "ORDER BY feature_timestamp DESC\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_strm_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_key = customer_strm_record[\"entity_id\"][0]\n",
    "print(f\"entity_id={customer_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_features = fs_features_lookup(\n",
    "    FEATURESTORE_ID, \"customers\", customer_key  # customer_key\n",
    ")\n",
    "pp.pprint(customer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery terminal_strm_record --project {PROJECT_ID}\n",
    "SELECT * FROM tx.v_terminals_features\n",
    "ORDER BY feature_timestamp DESC\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminal_key = terminal_strm_record[\"entity_id\"][0]\n",
    "print(f\"entity_id={customer_key}\")\n",
    "terminal_features = fs_features_lookup(\n",
    "    FEATURESTORE_ID, \"terminals\", terminal_key\n",
    ")  # Change key values\n",
    "pp.pprint(terminal_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together: Building the Prediction Request\n",
    "\n",
    "Now we'll perform the full, end-to-end process for a single transaction:\n",
    "\n",
    "1.  **Get a transaction:** We'll use the transaction we read from Pub/Sub earlier.\n",
    "2.  **Set default feature values:** We start with a dictionary of default values for all our features. This is a good practice to ensure the model receives a complete feature vector, even if a feature lookup fails.\n",
    "3.  **Add transaction amount:** We add the `TX_AMOUNT` from our simulated transaction to the payload.\n",
    "4.  **Lookup and add customer features:** We use our `fs_features_lookup` function to get the latest features for the customer in the transaction and add them to the payload.\n",
    "5.  **Lookup and add terminal features:** We do the same for the terminal.\n",
    "6.  **Send for prediction:** Finally, we send the complete payload to our model endpoint's `predict` method.\n",
    "\n",
    "The result will be a real-time fraud prediction for our incoming transaction, enriched with the latest features from our Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(compact=True)\n",
    "\n",
    "# Payload for manual test:\n",
    "# payload_json = {\n",
    "#     \"TX_ID\": \"61210be0744c43232990152d3eb2c2deb6035d8b\",\n",
    "#     \"TX_TS\": \"2025-09-06 17:27:51\",\n",
    "#     \"CUSTOMER_ID\": \"7389471951168361\",\n",
    "#     \"TERMINAL_ID\": \"45087784\",\n",
    "#     \"TX_AMOUNT\": 32.77\n",
    "# }\n",
    "\n",
    "default_features = {\n",
    "    \"customer_id_avg_amount_14day_window\": 0,\n",
    "    \"customer_id_avg_amount_15min_window\": 0,\n",
    "    \"customer_id_avg_amount_1day_window\": 0,\n",
    "    \"customer_id_avg_amount_30min_window\": 0,\n",
    "    \"customer_id_avg_amount_60min_window\": 0,\n",
    "    \"customer_id_avg_amount_7day_window\": 0,\n",
    "    \"customer_id_nb_tx_14day_window\": 0,\n",
    "    \"customer_id_nb_tx_7day_window\": 0,\n",
    "    \"customer_id_nb_tx_15min_window\": 0,\n",
    "    \"customer_id_nb_tx_1day_window\": 0,\n",
    "    \"customer_id_nb_tx_30min_window\": 0,\n",
    "    \"customer_id_nb_tx_60min_window\": 0,\n",
    "    \"terminal_id_avg_amount_15min_window\": 0,\n",
    "    \"terminal_id_avg_amount_30min_window\": 0,\n",
    "    \"terminal_id_avg_amount_60min_window\": 0,\n",
    "    \"terminal_id_nb_tx_14day_window\": 0,\n",
    "    \"terminal_id_nb_tx_15min_window\": 0,\n",
    "    \"terminal_id_nb_tx_1day_window\": 0,\n",
    "    \"terminal_id_nb_tx_30min_window\": 0,\n",
    "    \"terminal_id_nb_tx_60min_window\": 0,\n",
    "    \"terminal_id_nb_tx_7day_window\": 0,\n",
    "    \"terminal_id_risk_14day_window\": 0,\n",
    "    \"terminal_id_risk_1day_window\": 0,\n",
    "    \"terminal_id_risk_7day_window\": 0,\n",
    "}\n",
    "\n",
    "# Reading 1-st message\n",
    "payload_json = messages_tx[0]\n",
    "\n",
    "payload = default_features\n",
    "payload[\"tx_amount\"] = payload_json[\"TX_AMOUNT\"]\n",
    "\n",
    "# look up the customer features from New Vertex AI Feature Store\n",
    "customer_features = fs_features_lookup(\n",
    "    FEATURESTORE_ID, \"customers\", payload_json[\"CUSTOMER_ID\"]\n",
    ")\n",
    "# print the customer features from Vertex AI Feature Store\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"customer_features:\")\n",
    "pp.pprint(customer_features)\n",
    "\n",
    "# look up the treminal features from New Vertex AI Feature Store\n",
    "terminal_features = fs_features_lookup(\n",
    "    FEATURESTORE_ID, \"terminals\", payload_json[\"TERMINAL_ID\"]\n",
    ")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"terminal features:\")\n",
    "pp.pprint(terminal_features)\n",
    "\n",
    "# Add customer features to payload\n",
    "payload.update(customer_features)\n",
    "\n",
    "# Add terminal features to payload\n",
    "payload.update(terminal_features)\n",
    "\n",
    "# del payload[\"feature_timestamp\"]\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"[Payload to be sent to Vertex AI endpoint]\")\n",
    "pp.pprint(payload)\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "result = endpoint_obj.predict(instances=[payload])\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "pp.pprint(f\"[Prediction result]: {result}\")\n",
    "print(\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

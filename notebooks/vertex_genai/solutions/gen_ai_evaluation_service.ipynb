{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2ccd92-c76c-4e17-8ec7-da72ca6bb709",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vertex AI GenAI Evaluation Service\n",
    "\n",
    "Vertex AI's Gen AI evaluation service empowers you to assess any generative AI model or application according to your specific requirements.\n",
    "\n",
    "Instead of relying solely on general leaderboards and reports, you can define your own evaluation criteria and directly compare how different models perform against your unique needs, use case and data.\n",
    "\n",
    "This service allows you to:\n",
    "\n",
    "- Gain a deeper understanding of model performance. And, Go beyond general metrics and understand how a model handles your specific data and tasks.\n",
    "- Make informed decisions throughout the development lifecycle. By using evaluations to guide model selection, refine prompt engineering, and optimize model customization.\n",
    "- Streamline your evaluation workflow by leveraging Vertex AI's integrated tools to easily launch and reuse evaluations as needed.\n",
    "\n",
    "Essentially, it puts you in control of evaluating generative AI, ensuring the models you choose are the best fit for your specific applications.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn:\n",
    "\n",
    "- How to use Vertex AI Gen AI Evaluation Service \n",
    "- Different types of evaluation techniques (Computation Based and Model Based)\n",
    "- How to prepare you dataset and get it ready for evaluation\n",
    "- Analyze and understand the results from evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f0365-f681-487d-9f93-35c6bfe2e6d6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d3670-b507-4c9d-b70b-df13f937a94d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Main\n",
    "from vertexai.evaluation import (\n",
    "    EvalTask,\n",
    "    MetricPromptTemplateExamples,\n",
    "    PairwiseMetric,\n",
    "    PointwiseMetric,\n",
    "    PointwiseMetricPromptTemplate,\n",
    ")\n",
    "from vertexai.generative_models import (\n",
    "    GenerativeModel,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da785841-5902-4960-b421-f47892be68f5",
   "metadata": {},
   "source": [
    "### Helper Function\n",
    "\n",
    "The function helps us display the results from the evaluation SDK in a readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b01eb7-2e08-45df-ac52-3cab2cc8f262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_eval_report(eval_result, metrics=None):\n",
    "    \"\"\"Display the evaluation results.\"\"\"\n",
    "\n",
    "    title, summary_metrics, report_df = eval_result\n",
    "    metrics_df = pd.DataFrame.from_dict(summary_metrics, orient=\"index\").T\n",
    "    if metrics:\n",
    "        metrics_df = metrics_df.filter(\n",
    "            [\n",
    "                metric\n",
    "                for metric in metrics_df.columns\n",
    "                if any(selected_metric in metric for selected_metric in metrics)\n",
    "            ]\n",
    "        )\n",
    "        report_df = report_df.filter(\n",
    "            [\n",
    "                metric\n",
    "                for metric in report_df.columns\n",
    "                if any(selected_metric in metric for selected_metric in metrics)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Display the title with Markdown for emphasis\n",
    "    display(Markdown(f\"## {title}\"))\n",
    "\n",
    "    # Display the metrics DataFrame\n",
    "    display(Markdown(\"### Summary Metrics\"))\n",
    "    display(metrics_df)\n",
    "\n",
    "    # Display the detailed report DataFrame\n",
    "    display(Markdown(\"### Report Metrics\"))\n",
    "    display(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d3eae9-0a4e-471a-9319-7169c6825138",
   "metadata": {},
   "source": [
    "## Evaluation Process\n",
    "\n",
    "Vertex AI's Gen AI evaluation service lets you assess any generative model based on your specific needs and criteria.\n",
    "\n",
    "These are the four steps you will follow to help you evaluate:\n",
    "\n",
    "1. Define: Tailor metrics to your business goals and choose your evaluation approach (Computation based vs Model Based).\n",
    "2. Prepare: Create a dataset that reflects your real-world use case.\n",
    "3. Run: Easily launch evaluations using templates or existing examples. Define your models and create reusable `EvalTasks` within Vertex AI to use in other evaluations.\n",
    "4. Analyze: Interpret your results and understand how each model performs against your specific criteria.\n",
    "\n",
    "Let's take a look at these steps one by one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012a826-4c47-4272-85f4-74225b86018d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Computation Based Metrics\n",
    "\n",
    "[Computation based metrics](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#computation-based-metrics) are computed using mathematical formulas to compare the model's output against a **ground truth or reference**. Commonly used computation-based metrics include ROUGE and BLEU. The commonly used metrics can be categorized into the following groups:\n",
    "\n",
    "- Lexicon-based metrics: Use math to calculate the string similarities between LLM-generated results and ground truth, such as `Exact Match` and `ROUGE`.\n",
    "- Count-based metrics: Aggregate the number of rows that hit or miss certain ground-truth labels, such as `F1-score`, `Accuracy`, and `Tool Name Match`.\n",
    "- Embedding-based metrics: Calculate the distance between the LLM-generated results and ground truth in the embedding space, reflecting their level of similarity. These include `cosine similarity` and `euclidean distance` \n",
    "\n",
    "Let's say that we are trying to evaluate how well different prompts works for summarization using Gemini. We'll start by defining a few articles in `context` and since we are using computation based metrics for evaluation, we will also need to define the ground truth for the summaries. This will be defined in `reference`. `eval_dataset` should be a dataframe that contains columns needed for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0593398-94d2-4a74-98c3-4a16e3f0fb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"Summarize the following article\"\n",
    "\n",
    "context = [\n",
    "    \"To make a classic spaghetti carbonara, start by bringing a large pot of salted water to a boil. While the water is heating up, cook pancetta or guanciale in a skillet with olive oil over medium heat until it's crispy and golden brown. Once the pancetta is done, remove it from the skillet and set it aside. In the same skillet, whisk together eggs, grated Parmesan cheese, and black pepper to make the sauce. When the pasta is cooked al dente, drain it and immediately toss it in the skillet with the egg mixture, adding a splash of the pasta cooking water to create a creamy sauce.\",\n",
    "    \"Preparing a perfect risotto requires patience and attention to detail. Begin by heating butter in a large, heavy-bottomed pot over medium heat. Add finely chopped onions and minced garlic to the pot, and cook until they're soft and translucent, about 5 minutes. Next, add Arborio rice to the pot and cook, stirring constantly, until the grains are coated with the butter and begin to toast slightly. Pour in a splash of white wine and cook until it's absorbed. From there, gradually add hot chicken or vegetable broth to the rice, stirring frequently, until the risotto is creamy and the rice is tender with a slight bite.\",\n",
    "    \"To bake a decadent chocolate cake from scratch, start by preheating your oven to 350°F (175°C) and greasing and flouring two 9-inch round cake pans. In a large mixing bowl, cream together softened butter and granulated sugar until light and fluffy. Beat in eggs one at a time, making sure each egg is fully incorporated before adding the next. In a separate bowl, sift together all-purpose flour, cocoa powder, baking powder, baking soda, and salt. Divide the batter evenly between the prepared cake pans and bake for 25-30 minutes, or until a toothpick inserted into the center comes out clean.\",\n",
    "]\n",
    "\n",
    "reference = [\n",
    "    \"The process of making spaghetti carbonara involves boiling pasta, crisping pancetta or guanciale, whisking together eggs and Parmesan cheese, and tossing everything together to create a creamy sauce.\",\n",
    "    \"Preparing risotto entails sautéing onions and garlic, toasting Arborio rice, adding wine and broth gradually, and stirring until creamy and tender.\",\n",
    "    \"Baking a decadent chocolate cake requires creaming butter and sugar, beating in eggs and alternating dry ingredients with buttermilk before baking until done.\",\n",
    "]\n",
    "\n",
    "eval_dataset = pd.DataFrame(\n",
    "    {\n",
    "        \"context\": context,\n",
    "        \"reference\": reference,\n",
    "        \"instruction\": [instruction] * len(context),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f1ddc-a3b6-4b12-a839-bf4b658e65c5",
   "metadata": {},
   "source": [
    "Now, we'll define different prompt templates to evaluate. This list can include all the different prompt templates you want to experiment with. For the purpose of demostration, we'll use two different prompt templates below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8649578-1f9d-44ea-a40f-bcc3aab2d9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_templates = [\n",
    "    \"Instruction: {instruction}. Article: {context}. Summary:\",\n",
    "    \"Article: {context}. Complete this task: {instruction}, in one sentence. Summary:\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425b0d3-7cae-43f3-b51b-011f32496840",
   "metadata": {},
   "source": [
    "Next, we will define the different metrics we want to measure for this task. There are different metrics that are defined by Vertex AI based on the task. You can take a look at the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#computation-based-metrics) to see the different metrics. And, take a look at the [API documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.preview.evaluation.EvalTask#vertexai_preview_evaluation_EvalTask) to refer to the right string values for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1fc16-be95-4e98-b623-0193a374b7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"rouge_1\",\n",
    "    \"rouge_l_sum\",\n",
    "    \"bleu\",\n",
    "    \"safety\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b524bce-a5ec-4237-8d6e-388087ac4583",
   "metadata": {},
   "source": [
    "We'll configure the model that we are going to use for the generating the reponses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04822606-ebf8-4138-97d6-3f8886a8d4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"temperature\": 0.3,\n",
    "}\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "gemini_model = GenerativeModel(\n",
    "    \"gemini-2.0-flash\",\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b66a31-ef55-49cc-a119-eb180761a92d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, that we have configured all the parameters, we are ready to start evaluating. We will use [EvalTask](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.preview.evaluation.EvalTask) to kick of a Vertex AI Experiment for our evaluation. \n",
    "\n",
    "#### Understand the EvalTask class\n",
    "\n",
    "The EvalTask class is a core component of the Gen AI Evaluation Service SDK framework. It allows you to define and run evaluation jobs against your Gen AI models/applications, providing a structured way to measure their performance on specific tasks. Think of an EvalTask as a blueprint for your evaluation process. Evaluation tasks must contain an evaluation dataset, and a list of metrics to evaluate. Supported metrics are documented on the Generative AI on Vertex AI [Define your evaluation metrics page](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval). The dataset can be an `pandas.DataFrame`, Python dictionary or a file path URI and can contain default column names such as `prompt`, `reference`, `response`, and `baseline_model_response`.  \n",
    "\n",
    "- Bring-your-own-response (BYOR): You already have the data that you want to evaluate stored in the dataset. You can customize the response column names for both your model and the baseline model using parameters like response_column_name and baseline_model_response_column_name or through the metric_column_mapping.\n",
    "\n",
    "- Perform model inference without a prompt template: You have a dataset containing the input prompts to the model and want to perform model inference before evaluation. A column named prompt is required in the evaluation dataset and is used directly as input to the model.\n",
    "\n",
    "- Perform model inference with a prompt template: You have a dataset containing the input variables to the prompt template and want to assemble the prompts for model inference. Evaluation dataset must contain column names corresponding to the variable names in the prompt template. For example, if prompt template is \"Instruction: {instruction}, context: {context}\", the dataset must contain instruction and context columns.\n",
    "\n",
    "EvalTask supports extensive evaluation scenarios including BYOR, model inference with Gemini models, 3P models endpoints/SDK clients, or custom model generation functions, using computation-based metrics, model-based pointwise and pairwise metrics. The evaluate() method triggers the evaluation process, optionally taking a model, prompt template, experiment logging configuartions, and other evaluation run configurations. You can view the SDK reference documentation for [Gen AI Evaluation](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.evaluation) package for more details.\n",
    "\n",
    "To start with, we'll be using model inference with prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3251b65-f688-4e4b-80d8-bd439f34c2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"prompt-engineering-eval\"\n",
    "\n",
    "summarization_eval_task = EvalTask(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    experiment=experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71ec62-81a3-4044-af49-3c35e8618889",
   "metadata": {},
   "source": [
    "Please note: The cell below takes 10-15 mins to finish executing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfde61a-20a8-4e54-a5c7-94ca0a743766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_uuid(length: int = 8) -> str:\n",
    "    \"\"\"Generate a uuid of a specified length (default=8).\"\"\"\n",
    "    return \"\".join(\n",
    "        random.choices(string.ascii_lowercase + string.digits, k=length)\n",
    "    )\n",
    "\n",
    "\n",
    "run_id = generate_uuid()\n",
    "eval_results = []\n",
    "\n",
    "\n",
    "for i, prompt_template in enumerate(prompt_templates):\n",
    "    experiment_run_name = f\"eval-prompt-engineering-{run_id}-prompt-{i}\"\n",
    "\n",
    "    eval_result = summarization_eval_task.evaluate(\n",
    "        prompt_template=prompt_template,\n",
    "        experiment_run_name=experiment_run_name,\n",
    "        model=gemini_model,\n",
    "    )\n",
    "\n",
    "    eval_results.append(\n",
    "        (f\"Prompt #{i}\", eval_result.summary_metrics, eval_result.metrics_table)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45433208-2c55-4956-9b08-7340d499d1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for eval_result in eval_results:\n",
    "    display_eval_report(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5bace3-17f2-47e8-9644-64b86e8da73a",
   "metadata": {},
   "source": [
    "You can view the results from the experiment runs using the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eda606-abcd-4a64-801f-f3fcf0e9b126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarization_eval_task.display_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1486e-b600-422a-b8e9-4ed6fa958763",
   "metadata": {},
   "source": [
    "### Model Based Metrics\n",
    "\n",
    "Now, suppose you do not have reference results or ground truths for your use case, you can evaluate and compare different models using another LLM. This technique is called **Model based evaluation**. Model-based metrics assesses your candidate model against a judge model. The judge model for most use cases is Gemini, but you can also use models such as [MetricX](https://github.com/google-research/metricx) or [COMET](https://huggingface.co/Unbabel/wmt22-comet-da) for translation use cases.\n",
    "\n",
    "You can measure model-based metrics pairwise or pointwise:\n",
    "\n",
    "- **Pointwise metrics**: Let the judge model assess the candidate model's output based on the evaluation criteria. For example, the score could be 0 to 5, where 0 means the response does not fit the criteria, while 5 means the response fits the criteria well.\n",
    "\n",
    "- **Pairwise metrics**: Let the judge model compare the responses of two models and pick the better one. This is often used when comparing a candidate model with the baseline model. Pairwise metrics are only supported with Gemini as a judge model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc09bf14-05e1-4c3d-a176-02a2953f19fb",
   "metadata": {},
   "source": [
    "#### Pointwise Metrics\n",
    "\n",
    "Let's say now that we have decided on a prompt, we want to also rate the summaries generated on two qualitative criteria such as \"fluency\" and \"entertaining\". We will define a metric called `text_quality` using those two criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be534610-d0c8-4e0b-ad24-7397cec8109d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your own definition of text_quality.\n",
    "metric_prompt_template = PointwiseMetricPromptTemplate(\n",
    "    criteria={\n",
    "        \"fluency\": \"Sentences flow smoothly and are easy to read, avoiding awkward phrasing or run-on sentences. Ideas and sentences connect logically, using transitions effectively where needed.\",\n",
    "        \"entertaining\": \"Short, amusing text that incorporates emojis, exclamations and questions to convey quick and spontaneous communication and diversion.\",\n",
    "    },\n",
    "    rating_rubric={\n",
    "        \"1\": \"The response performs well on both criteria.\",\n",
    "        \"0\": \"The response is somewhat aligned with both criteria\",\n",
    "        \"-1\": \"The response falls short on both criteria\",\n",
    "    },\n",
    ")\n",
    "\n",
    "text_quality = PointwiseMetric(\n",
    "    metric=\"text_quality\",\n",
    "    metric_prompt_template=metric_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363820c9-664e-49a4-b562-50909541dacb",
   "metadata": {},
   "source": [
    "Let's take a look at the what this `text_quality` metric looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56b490-3087-4c2e-9f17-3db01f95aaea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(text_quality.metric_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a0831-bfa1-4289-bdd6-9b60dc91994a",
   "metadata": {},
   "source": [
    "We will first need to get the responses from our model of interest (`gemini-2.0-flash`) and store it in a list. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c6968-fc5f-4bcb-967b-0099fa36758e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemini_model = GenerativeModel(\n",
    "    \"gemini-2.0-flash\",\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")\n",
    "\n",
    "context = [\n",
    "    \"To make a classic spaghetti carbonara, start by bringing a large pot of salted water to a boil. While the water is heating up, cook pancetta or guanciale in a skillet with olive oil over medium heat until it's crispy and golden brown. Once the pancetta is done, remove it from the skillet and set it aside. In the same skillet, whisk together eggs, grated Parmesan cheese, and black pepper to make the sauce. When the pasta is cooked al dente, drain it and immediately toss it in the skillet with the egg mixture, adding a splash of the pasta cooking water to create a creamy sauce.\",\n",
    "    \"Preparing a perfect risotto requires patience and attention to detail. Begin by heating butter in a large, heavy-bottomed pot over medium heat. Add finely chopped onions and minced garlic to the pot, and cook until they're soft and translucent, about 5 minutes. Next, add Arborio rice to the pot and cook, stirring constantly, until the grains are coated with the butter and begin to toast slightly. Pour in a splash of white wine and cook until it's absorbed. From there, gradually add hot chicken or vegetable broth to the rice, stirring frequently, until the risotto is creamy and the rice is tender with a slight bite.\",\n",
    "    \"For a flavorful grilled steak, start by choosing a well-marbled cut of beef like ribeye or New York strip. Season the steak generously with kosher salt and freshly ground black pepper on both sides, pressing the seasoning into the meat. Preheat a grill to high heat and brush the grates with oil to prevent sticking. Place the seasoned steak on the grill and cook for about 4-5 minutes on each side for medium-rare, or adjust the cooking time to your desired level of doneness. Let the steak rest for a few minutes before slicing against the grain and serving.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afc5da-1539-4f5b-8f8a-c7d435223d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = []\n",
    "instruction = \"Summarize the following article\"\n",
    "for article in context:\n",
    "    prompt = f\"Instruction: {instruction}. Article: {article}. Summary:\"\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    responses.append(response.text)\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd2e57-402f-42ab-a3e2-b992aa1c30d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset = pd.DataFrame(\n",
    "    {\n",
    "        \"response\": responses,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb00332-690f-4b18-8190-9a4f5dee96c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"pointwise-eval\"\n",
    "eval_task = EvalTask(\n",
    "    dataset=eval_dataset, metrics=[text_quality], experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "pointwise_eval_results = eval_task.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a662f4d-8a04-4b0d-9427-2a378a1692d0",
   "metadata": {},
   "source": [
    "You can view the `summary_metrics` for all 3 articles here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1854a-e1bf-4ba5-b300-75688a25a5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pointwise_eval_results.summary_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86e4d6-0628-4fbf-8322-636e27572ba5",
   "metadata": {},
   "source": [
    "We see that text_quality is `0.0` because based on the criteria we defined above the responses somewhat align with both creteria. To get the score for each article based on the criteria you defined in `text_quality` you can access the `metrics_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c19bde-88e6-43cc-9767-43c624ca364e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pointwise_eval_results.metrics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb72f54-f086-483e-9b0b-ee46cba78f9e",
   "metadata": {},
   "source": [
    "Based on the criteria we defined it looks like our model gave a neutral score for each of the summaries. You can see the rationale behind the models score in the `text_quality/explanation` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8c2d2-63c2-48bc-b3bb-c5d76653e95c",
   "metadata": {},
   "source": [
    "#### Pairwise Metrics - Compare Models Side-by-Side (SxS)\n",
    "\n",
    "Let's say now that we have decided on a prompt, we want to also rate the summaries generated from two different LLMs (Gemini 2.0 Flash vs Gemini 2.0 Flash Lite). You can evaluate the summaries from two different models using pairwise model evaluation and side-by-side comparison.\n",
    "\n",
    "To directly compare two models, you can define a `PairwiseMetric` within an `EvalTask` run. This approach allows for a head-to-head assessment of the models' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5319608-fdd0-4c19-9ade-f8aff154c501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"Summarize the following article\"\n",
    "prompt_template = \"{instruction}. Article: {context}. Summary:\"\n",
    "\n",
    "pairwise_eval_dataset = pd.DataFrame(\n",
    "    {\n",
    "        \"context\": context,\n",
    "        \"instruction\": [instruction] * len(context),\n",
    "        \"reference\": reference,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1cb2f5-95ff-43e3-af8a-efcd3f36df0e",
   "metadata": {},
   "source": [
    "Once we have prepared the evaluation dataset we will define the two models that we want to compare. `model_a` will be `gemini-2.0-flash` and `model_b` will be `gemini-2.0-flash-lite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359ecb5f-71b7-4902-bb92-575b2a688eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Baseline model for pairwise comparison\n",
    "model_a = GenerativeModel(\n",
    "    \"gemini-2.0-flash\",\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")\n",
    "\n",
    "# Candidate model for pairwise comparison\n",
    "model_b = GenerativeModel(\n",
    "    \"gemini-2.0-flash-lite\",\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab6aa6-e791-44df-a92c-30b27be710dd",
   "metadata": {},
   "source": [
    "Similar to how to defined the evaluation cretiria for pointwise evaluation above, you could customize your evaluation prompt. However, to save time, Vertex AI provides [predefined evaluation prompts](https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates) in `MetricPromptTemplateExamples` you could use. In this use case, we are going to be evaluating the text quality between the two models. `pointwise_text_quality` will use the following criteria to evalute the model responses.\n",
    "\n",
    "```\n",
    "STEP 1: Analyze Response A based on all the Criteria provided, including Coherence, Fluency, Instruction following, Groundedness, and Verbosity. Provide assessment according to each criterion.\n",
    "STEP 2: Analyze Response B based on all the Criteria provided, including Coherence, Fluency, Instruction following, Groundedness, and Verbosity. Provide assessment according to each criterion \n",
    "STEP 3: Compare the overall performance of Response A and Response B based on your analyses and assessment of each criterion \n",
    "STEP 4: Output your preference of \"A\", \"SAME\" or \"B\" to the pairwise_choice field according to the Rating Rubric.\n",
    "STEP 5: Output your assessment reasoning in the explanation field, justifying your choice by highlighting the specific strengths and weaknesses of each response in terms of Text Quality\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051748e-cd24-4b25-8a17-95e94b166b88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a \"Pairwise Text Quality\" metric\n",
    "text_quality_prompt_template = MetricPromptTemplateExamples.get_prompt_template(\n",
    "    \"pairwise_text_quality\"\n",
    ")\n",
    "\n",
    "pairwise_text_quality_metric = PairwiseMetric(\n",
    "    metric=\"pairwise_text_quality\",\n",
    "    metric_prompt_template=text_quality_prompt_template,\n",
    "    baseline_model=model_a,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e78ed-de92-4156-afda-32b5c0b43f4f",
   "metadata": {},
   "source": [
    "Once we have defined the dataset and the evaluation crieteria. We are ready to kick off the evaluation job.\n",
    "\n",
    "Please note: the cell below is going to take 10-15 mins to finish execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7d63e-ea3c-4f90-bdf1-9cbf485e0a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pairwise_text_quality_eval_task = EvalTask(\n",
    "    dataset=pairwise_eval_dataset,\n",
    "    metrics=[pairwise_text_quality_metric],\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")\n",
    "\n",
    "# Specify candidate model for pairwise comparison\n",
    "pairwise_text_quality_result = pairwise_text_quality_eval_task.evaluate(\n",
    "    model=model_b,\n",
    "    prompt_template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f3057-785d-4f1b-8304-9cc78c2078c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once the above experiment runs, you can view the results using the `display_eval_report` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a6675-d158-456a-8008-10dfc1d9768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_eval_report(\n",
    "    (\n",
    "        \"Side-by-side EvalTask\",\n",
    "        pairwise_text_quality_result.summary_metrics,\n",
    "        pairwise_text_quality_result.metrics_table,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef80d4-48a5-446d-ae61-bca2e03c1439",
   "metadata": {},
   "source": [
    "All of the above evaluation experiments we ran in the notebook are accessible from the [Experiments Tab](https://console.cloud.google.com/vertex-ai/experiments/experiments) in the Vertex AI UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd0436-5f7b-43f1-9ca8-287b22929900",
   "metadata": {},
   "source": [
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

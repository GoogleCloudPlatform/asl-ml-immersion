{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training at scale with the Vertex AI Training Service\n",
    "**Learning Objectives:**\n",
    "  1. Learn how to organize your training code into a Python package\n",
    "  1. Train your model using cloud infrastructure via Google Cloud Vertex AI Training Service\n",
    "  1. (optional) Learn how to run your training package using Docker containers and push training Docker images on a Docker registry\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we'll make the jump from training locally, to do training in the cloud. We'll take advantage of Google Cloud's [Vertex AI Training Service](https://cloud.google.com/vertex-ai/). \n",
    "\n",
    "Vertex AI Training Service is a managed service that allows the training and deployment of ML models without having to provision or maintain servers. The infrastructure is handled seamlessly by the managed service for us.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in the [student lab notebook](../labs/1_training_at_scale_vertex.ipynb) -- try to complete that notebook first before reviewing this solution notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify your project name and bucket name in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from google import api_core\n",
    "from google.cloud import bigquery\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following cell as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT=qwiklabs-asl-01-19968276eb55\n",
      "env: BUCKET=qwiklabs-asl-01-19968276eb55\n",
      "env: REGION=us-central1\n",
      "env: OUTDIR=gs://qwiklabs-asl-01-19968276eb55/taxifare/data\n",
      "env: TFVERSION=2.8\n"
     ]
    }
   ],
   "source": [
    "# Change below if necessary\n",
    "PROJECT = !gcloud config get-value project  # noqa: E999\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "OUTDIR = f\"gs://{BUCKET}/taxifare/data\"\n",
    "\n",
    "%env PROJECT=$PROJECT\n",
    "%env BUCKET=$BUCKET\n",
    "%env REGION=$REGION\n",
    "%env OUTDIR=$OUTDIR\n",
    "%env TFVERSION=2.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm below that the bucket is regional and its region equals to the specified region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-01-19968276eb55/ :\n",
      "\tLocation type:\t\t\tregion\n",
      "\tLocation constraint:\t\tUS-CENTRAL1\n",
      "us-central1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls -Lb gs://$BUCKET | grep \"gs://\\|Location\"\n",
    "echo $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [ai/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set ai/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BigQuery tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not already created a BigQuery dataset for our data, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n"
     ]
    }
   ],
   "source": [
    "bq = bigquery.Client(project=PROJECT)\n",
    "dataset = bigquery.Dataset(bq.dataset(\"taxifare\"))\n",
    "\n",
    "try:\n",
    "    bq.create_dataset(dataset)\n",
    "    print(\"Dataset created\")\n",
    "except api_core.exceptions.Conflict:\n",
    "    print(\"Dataset already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a table with 1 million examples.\n",
    "\n",
    "Note that the order of columns is exactly what was in our CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10aed5cd5218485f984e5ba1be1eb763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_training_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    'unused' AS key\n",
    "FROM `nyc-tlc.yellow.trips` \n",
    "WHERE ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 1000)) = 1\n",
    "AND\n",
    "    trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the validation dataset be 1/10 the size of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a521b486f85457cb44ac98663d608d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_valid_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    'unused' AS key\n",
    "FROM `nyc-tlc.yellow.trips`\n",
    "WHERE ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 10000)) = 2\n",
    "AND\n",
    "    trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the tables as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current contents of gs://qwiklabs-asl-01-19968276eb55/taxifare/data\n",
      "Extracting training data to gs://qwiklabs-asl-01-19968276eb55/taxifare/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r74c2fa7e06830783_00000196cc69b39c_1 ... (5s) Current status: DONE   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting validation data to gs://qwiklabs-asl-01-19968276eb55/taxifare/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r1c7980fe77045427_00000196cc69e1c7_1 ... (1s) Current status: DONE   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  29483349  2025-05-14T01:30:05Z  gs://qwiklabs-asl-01-19968276eb55/taxifare/data/taxi-train-000000000000.csv\n",
      "  29387484  2025-05-14T01:30:07Z  gs://qwiklabs-asl-01-19968276eb55/taxifare/data/taxi-train-000000000001.csv\n",
      "  29474402  2025-05-14T01:30:07Z  gs://qwiklabs-asl-01-19968276eb55/taxifare/data/taxi-train-000000000002.csv\n",
      "   8725746  2025-05-14T01:30:15Z  gs://qwiklabs-asl-01-19968276eb55/taxifare/data/taxi-valid-000000000000.csv\n",
      "TOTAL: 4 objects, 97070981 bytes (92.57 MiB)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Deleting current contents of $OUTDIR\"\n",
    "gsutil -m -q rm -rf $OUTDIR\n",
    "\n",
    "echo \"Extracting training data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "   --destination_format CSV  \\\n",
    "   --field_delimiter \",\" --noprint_header \\\n",
    "   taxifare.feateng_training_data \\\n",
    "   $OUTDIR/taxi-train-*.csv\n",
    "\n",
    "echo \"Extracting validation data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "   --destination_format CSV  \\\n",
    "   --field_delimiter \",\" --noprint_header \\\n",
    "   taxifare.feateng_valid_data \\\n",
    "   $OUTDIR/taxi-valid-*.csv\n",
    "\n",
    "gsutil ls -l $OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that you have created both the training and validation datasets in Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-01-19968276eb55/taxifare/data/taxi-train-000000000000.csv\n",
      "gs://qwiklabs-asl-01-19968276eb55/taxifare/data/taxi-train-000000000001.csv\n",
      "gs://qwiklabs-asl-01-19968276eb55/taxifare/data/taxi-train-000000000002.csv\n",
      "gs://qwiklabs-asl-01-19968276eb55/taxifare/data/taxi-valid-000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://$BUCKET/taxifare/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5,2012-01-12 17:53:21 UTC,-74.005409,40.748532,-74.004533,40.748225,1,unused\n",
      "2.5,2012-08-14 20:00:38 UTC,-73.974073,40.679595,-73.974937,40.680728,1,unused\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat gs://$BUCKET/taxifare/data/taxi-train-000000000000.csv | head -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5,2012-05-30 19:22:00 UTC,-73.98416,40.764733,-73.982348,40.76473,1,unused\n",
      "2.5,2013-10-27 03:25:00 UTC,-74.003092,40.727735,-74.00236,40.72805,1,unused\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat gs://$BUCKET/taxifare/data/taxi-valid-000000000000.csv | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make code compatible with Vertex AI Training Service\n",
    "In order to make our code compatible with Vertex AI Training Service we need to make the following changes:\n",
    "\n",
    "1. Upload data to Google Cloud Storage \n",
    "2. Move code into a trainer Python package\n",
    "4. Submit training job with `gcloud` to train on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move code into a python package\n",
    "\n",
    "The first thing to do is to convert your training code snippets into a regular Python package. \n",
    "\n",
    "A Python package is simply a collection of one or more `.py` files along with an `__init__.py` file to identify the containing directory as a package. The `__init__.py` sometimes contains initialization code but for our purposes an empty file suffices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the package directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our package directory contains 3 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ./taxifare/trainer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paste existing code into model.py\n",
    "\n",
    "A Python package requires our code to be in a .py file, as opposed to notebook cells. So, we simply copy and paste our existing code for the previous notebook into a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we write the contents of the cell into `model.py` packaging the model we \n",
    "developed in the previous labs so that we can deploy it to Vertex AI Training Service.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #1**: Organizing your training code into a Python package\n",
    "\n",
    "There are two places to fill in TODOs in `model.py`. \n",
    "\n",
    " * in the `build_dnn_model` function, add code to use a optimizer with a custom learning rate.\n",
    " * in the `train_and_evaluate` function, add code to define variables using the `hparams` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./taxifare/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./taxifare/trainer/model.py\n",
    "\"\"\"Data prep, train and evaluate DNN model.\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks,  models\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Discretization,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "from tensorflow.keras.layers.experimental.preprocessing import HashedCrossing\n",
    "\n",
    "logging.info(tf.version.VERSION)\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    \"fare_amount\",\n",
    "    \"pickup_datetime\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\",\n",
    "    \"key\",\n",
    "]\n",
    "\n",
    "LABEL_COLUMN = \"fare_amount\"\n",
    "DEFAULTS = [[0.0], [\"na\"], [0.0], [0.0], [0.0], [0.0], [0.0], [\"na\"]]\n",
    "UNWANTED_COLS = [\"pickup_datetime\", \"key\"]\n",
    "\n",
    "INPUT_COLS = [\n",
    "    c for c in CSV_COLUMNS if c != LABEL_COLUMN and c not in UNWANTED_COLS\n",
    "]\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size, num_repeat):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=num_repeat,\n",
    "        shuffle_buffer_size=1000000,\n",
    "    )\n",
    "    return dataset.map(features_and_labels)\n",
    "\n",
    "\n",
    "def create_train_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def create_eval_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=1)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff * londiff + latdiff * latdiff)\n",
    "\n",
    "\n",
    "def scale_longitude(lon_column):\n",
    "    return (lon_column + 78) / 8.0\n",
    "\n",
    "\n",
    "def scale_latitude(lat_column):\n",
    "    return (lat_column - 37) / 8.0\n",
    "\n",
    "\n",
    "def transform(inputs, nbuckets):\n",
    "    transformed = {}\n",
    "\n",
    "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
    "    transformed[\"scaled_plon\"] = Lambda(scale_longitude, name=\"scale_plon\")(\n",
    "        inputs[\"pickup_longitude\"]\n",
    "    )\n",
    "    transformed[\"scaled_dlon\"] = Lambda(scale_longitude, name=\"scale_dlon\")(\n",
    "        inputs[\"dropoff_longitude\"]\n",
    "    )\n",
    "\n",
    "    # Scaling latitude from range [37, 45] to [0, 1]\n",
    "    transformed[\"scaled_plat\"] = Lambda(scale_latitude, name=\"scale_plat\")(\n",
    "        inputs[\"pickup_latitude\"]\n",
    "    )\n",
    "    transformed[\"scaled_dlat\"] = Lambda(scale_latitude, name=\"scale_dlat\")(\n",
    "        inputs[\"dropoff_latitude\"]\n",
    "    )\n",
    "\n",
    "    # Apply euclidean function\n",
    "    transformed[\"euclidean_distance\"] = Lambda(euclidean, name=\"euclidean\")(\n",
    "        [\n",
    "            inputs[\"pickup_longitude\"],\n",
    "            inputs[\"pickup_latitude\"],\n",
    "            inputs[\"dropoff_longitude\"],\n",
    "            inputs[\"dropoff_latitude\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    latbuckets = np.linspace(start=0.0, stop=1.0, num=nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(start=0.0, stop=1.0, num=nbuckets).tolist()\n",
    "\n",
    "    # Bucketization with Discretization layer\n",
    "    plon = Discretization(lonbuckets, name=\"plon_bkt\")(\n",
    "        transformed[\"scaled_plon\"]\n",
    "    )\n",
    "    plat = Discretization(latbuckets, name=\"plat_bkt\")(\n",
    "        transformed[\"scaled_plat\"]\n",
    "    )\n",
    "    dlon = Discretization(lonbuckets, name=\"dlon_bkt\")(\n",
    "        transformed[\"scaled_dlon\"]\n",
    "    )\n",
    "    dlat = Discretization(latbuckets, name=\"dlat_bkt\")(\n",
    "        transformed[\"scaled_dlat\"]\n",
    "    )\n",
    "\n",
    "    # Feature Cross with HashedCrossing layer\n",
    "    p_fc = HashedCrossing(num_bins=nbuckets * nbuckets, name=\"p_fc\")(\n",
    "        (plon, plat)\n",
    "    )\n",
    "    d_fc = HashedCrossing(num_bins=nbuckets * nbuckets, name=\"d_fc\")(\n",
    "        (dlon, dlat)\n",
    "    )\n",
    "    pd_fc = HashedCrossing(num_bins=nbuckets**4, name=\"pd_fc\")((p_fc, d_fc))\n",
    "\n",
    "    # Embedding with Embedding layer\n",
    "    transformed[\"pd_embed\"] = Flatten()(\n",
    "        Embedding(input_dim=nbuckets**4, output_dim=10, name=\"pd_embed\")(\n",
    "            pd_fc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    transformed[\"passenger_count\"] = inputs[\"passenger_count\"]\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_dnn_model(nbuckets, nnsize, lr):\n",
    "    inputs = {\n",
    "        colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "        for colname in INPUT_COLS\n",
    "    }\n",
    "\n",
    "    # transforms\n",
    "    transformed = transform(inputs, nbuckets)\n",
    "    dnn_inputs = Concatenate()(transformed.values())\n",
    "\n",
    "    x = dnn_inputs\n",
    "    for layer, nodes in enumerate(nnsize):\n",
    "        x = Dense(nodes, activation=\"relu\", name=f\"h{layer}\")(x)\n",
    "    output = Dense(1, name=\"fare\")(x)\n",
    "\n",
    "    model = models.Model(inputs, output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"mse\", )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    # TODO 1b: Your code here\n",
    "    nbuckets = hparams[\"nbuckets\"]\n",
    "    nnsize = hparams[\"nnsize\"]\n",
    "    lr = hparams[\"lr\"]\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "\n",
    "    nnsize = [int(s) for s in hparams[\"nnsize\"].split()]\n",
    "    eval_data_path = hparams[\"eval_data_path\"]\n",
    "    num_evals = hparams[\"num_evals\"]\n",
    "    num_examples_to_train_on = hparams[\"num_examples_to_train_on\"]\n",
    "    output_dir = hparams[\"output_dir\"]\n",
    "    train_data_path = hparams[\"train_data_path\"]\n",
    "\n",
    "    model_export_path = os.path.join(output_dir, \"savedmodel\")\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "    tensorboard_path = os.path.join(output_dir, \"tensorboard\")\n",
    "\n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "\n",
    "    model = build_dnn_model(nbuckets, nnsize, lr)\n",
    "    logging.info(model.summary())\n",
    "\n",
    "    trainds = create_train_dataset(train_data_path, batch_size)\n",
    "    evalds = create_eval_dataset(eval_data_path, batch_size)\n",
    "\n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "\n",
    "    checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, save_weights_only=True, verbose=1\n",
    "    )\n",
    "    tensorboard_cb = callbacks.TensorBoard(tensorboard_path, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb],\n",
    "    )\n",
    "\n",
    "    # Exporting the model with default serving function.\n",
    "    model.save(model_export_path)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify code to read data from and write checkpoint files to GCS \n",
    "\n",
    "If you look closely above, you'll notice a new function, `train_and_evaluate` that wraps the code that actually trains the model. This allows us to parametrize the training by passing a dictionary of parameters to this function (e.g, `batch_size`, `num_examples_to_train_on`, `train_data_path` etc.)\n",
    "\n",
    "This is useful because the output directory, data paths and number of train steps will be different depending on whether we're training locally or in the cloud. Parametrizing allows us to use the same code for both.\n",
    "\n",
    "We specify these parameters at run time via the command line. Which means we need to add code to parse command line parameters and invoke `train_and_evaluate()` with those params. This is the job of the `task.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifare/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifare/trainer/task.py\n",
    "\"\"\"Argument definitions for model training code in `trainer.model`.\"\"\"\n",
    "\n",
    "import argparse\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Batch size for training steps\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location pattern of eval files\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        default=\"32 8\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nbuckets\",\n",
    "        help=\"Number of buckets to divide lat and lon with\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", help=\"learning rate for optimizer\", type=float, default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help=\"Number of times to evaluate model on eval data training.\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help=\"Number of examples to train on.\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location pattern of train files containing eval URLs\",\n",
    "        required=True,\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "\n",
    "    model.train_and_evaluate(hparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trainer module package locally\n",
    "\n",
    "Now we can test our training code locally as follows using the local test data. We'll run a very small training job over a single file with a small batch size and one eval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " pickup_longitude (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_longitude (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " pickup_latitude (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_latitude (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " scale_plon (Lambda)            (None, 1)            0           ['pickup_longitude[0][0]']       \n",
      "                                                                                                  \n",
      " scale_dlon (Lambda)            (None, 1)            0           ['dropoff_longitude[0][0]']      \n",
      "                                                                                                  \n",
      " scale_plat (Lambda)            (None, 1)            0           ['pickup_latitude[0][0]']        \n",
      "                                                                                                  \n",
      " scale_dlat (Lambda)            (None, 1)            0           ['dropoff_latitude[0][0]']       \n",
      "                                                                                                  \n",
      " plon_bkt (Discretization)      (None, 1)            0           ['scale_plon[0][0]']             \n",
      "                                                                                                  \n",
      " plat_bkt (Discretization)      (None, 1)            0           ['scale_plat[0][0]']             \n",
      "                                                                                                  \n",
      " dlon_bkt (Discretization)      (None, 1)            0           ['scale_dlon[0][0]']             \n",
      "                                                                                                  \n",
      " dlat_bkt (Discretization)      (None, 1)            0           ['scale_dlat[0][0]']             \n",
      "                                                                                                  \n",
      " p_fc (HashedCrossing)          (None, 1)            0           ['plon_bkt[0][0]',               \n",
      "                                                                  'plat_bkt[0][0]']               \n",
      "                                                                                                  \n",
      " d_fc (HashedCrossing)          (None, 1)            0           ['dlon_bkt[0][0]',               \n",
      "                                                                  'dlat_bkt[0][0]']               \n",
      "                                                                                                  \n",
      " pd_fc (HashedCrossing)         (None, 1)            0           ['p_fc[0][0]',                   \n",
      "                                                                  'd_fc[0][0]']                   \n",
      "                                                                                                  \n",
      " pd_embed (Embedding)           (None, 1, 10)        100000      ['pd_fc[0][0]']                  \n",
      "                                                                                                  \n",
      " euclidean (Lambda)             (None, 1)            0           ['pickup_longitude[0][0]',       \n",
      "                                                                  'pickup_latitude[0][0]',        \n",
      "                                                                  'dropoff_longitude[0][0]',      \n",
      "                                                                  'dropoff_latitude[0][0]']       \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 10)           0           ['pd_embed[0][0]']               \n",
      "                                                                                                  \n",
      " passenger_count (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 16)           0           ['scale_plon[0][0]',             \n",
      "                                                                  'scale_dlon[0][0]',             \n",
      "                                                                  'scale_plat[0][0]',             \n",
      "                                                                  'scale_dlat[0][0]',             \n",
      "                                                                  'euclidean[0][0]',              \n",
      "                                                                  'flatten[0][0]',                \n",
      "                                                                  'passenger_count[0][0]']        \n",
      "                                                                                                  \n",
      " h0 (Dense)                     (None, 32)           544         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " h1 (Dense)                     (None, 8)            264         ['h0[0][0]']                     \n",
      "                                                                                                  \n",
      " fare (Dense)                   (None, 1)            9           ['h1[0][0]']                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 100,817\n",
      "Trainable params: 100,817\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Epoch 1: saving model to ./taxifare-model/checkpoints\n",
      "2000/2000 - 28s - loss: 83.2679 - val_loss: 87.5721 - 28s/epoch - 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "EVAL_DATA_PATH=../data/taxi-traffic-valid*\n",
    "TRAIN_DATA_PATH=../data/taxi-traffic-train*\n",
    "OUTPUT_DIR=./taxifare-model\n",
    "\n",
    "test ${OUTPUT_DIR} && rm -rf ${OUTPUT_DIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "    \n",
    "python3 -m trainer.task \\\n",
    "--eval_data_path $EVAL_DATA_PATH \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--train_data_path $TRAIN_DATA_PATH \\\n",
    "--batch_size 5 \\\n",
    "--num_examples_to_train_on 10000 \\\n",
    "--num_evals 1 \\\n",
    "--nbuckets 10 \\\n",
    "--lr 0.001 \\\n",
    "--nnsize \"32 8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your training package on Vertex AI using a pre-built container\n",
    "\n",
    "Once the code works in standalone mode locally, you can run it on the Cloud using Vertex AI and use pre-built containers. First, we need to package our code as a source distribution. For this, we can use `setuptools`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing taxifare/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifare/setup.py\n",
    "\"\"\"Using `setuptools` to create a source distribution.\"\"\"\n",
    "\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "setup(\n",
    "    name=\"taxifare_trainer\",\n",
    "    version=\"0.1\",\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description=\"Taxifare model training application.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "creating taxifare_trainer.egg-info\n",
      "writing taxifare_trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to taxifare_trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to taxifare_trainer.egg-info/top_level.txt\n",
      "writing manifest file 'taxifare_trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'taxifare_trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'taxifare_trainer.egg-info/SOURCES.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running check\n",
      "creating taxifare_trainer-0.1\n",
      "creating taxifare_trainer-0.1/taxifare_trainer.egg-info\n",
      "creating taxifare_trainer-0.1/trainer\n",
      "copying files to taxifare_trainer-0.1...\n",
      "copying setup.py -> taxifare_trainer-0.1\n",
      "copying taxifare_trainer.egg-info/PKG-INFO -> taxifare_trainer-0.1/taxifare_trainer.egg-info\n",
      "copying taxifare_trainer.egg-info/SOURCES.txt -> taxifare_trainer-0.1/taxifare_trainer.egg-info\n",
      "copying taxifare_trainer.egg-info/dependency_links.txt -> taxifare_trainer-0.1/taxifare_trainer.egg-info\n",
      "copying taxifare_trainer.egg-info/top_level.txt -> taxifare_trainer-0.1/taxifare_trainer.egg-info\n",
      "copying trainer/__init__.py -> taxifare_trainer-0.1/trainer\n",
      "copying trainer/model.py -> taxifare_trainer-0.1/trainer\n",
      "copying trainer/task.py -> taxifare_trainer-0.1/trainer\n",
      "copying taxifare_trainer.egg-info/SOURCES.txt -> taxifare_trainer-0.1/taxifare_trainer.egg-info\n",
      "Writing taxifare_trainer-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'taxifare_trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd taxifare\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store our package in the Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://taxifare/dist/taxifare_trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  3.4 KiB/  3.4 KiB]                                                \n",
      "Operation completed over 1 objects/3.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp taxifare/dist/taxifare_trainer-0.1.tar.gz gs://${BUCKET}/taxifare/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Custom Job using the `gcloud` CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit this source distribution the Cloud we use [`gcloud ai custom-jobs create`](https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create) and simply specify some additional parameters for Vertex AI Training service:\n",
    "- job_name: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- region: Cloud region to train in. See [here](https://cloud.google.com/vertex-ai/docs/general/locations) for supported Vertex AI Custom model training regions\n",
    "\n",
    "The arguments within `--args` are sent to our `task.py`.\n",
    "\n",
    "Because this is on the entire dataset, it will take a while. You can monitor the job from the GCP console in the Vertex AI Training section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #2**: Train your model using cloud infrastructure via Google Cloud Vertex AI Training Service\n",
    "Fill in the TODOs in the code below to submit your job for training on Vertex AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-01-19968276eb55/taxifare/trained_model_20250514_025029 us-central1 taxifare_20250514_025029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/604342147284/locations/us-central1/customJobs/6477529367835574272] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/604342147284/locations/us-central1/customJobs/6477529367835574272\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/604342147284/locations/us-central1/customJobs/6477529367835574272\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Output directory and jobID\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model_$TIMESTAMP\n",
    "JOB_NAME=taxifare_$TIMESTAMP\n",
    "echo ${OUTDIR} ${REGION} ${JOB_NAME}\n",
    "\n",
    "PYTHON_PACKAGE_URIS=gs://${BUCKET}/taxifare/taxifare_trainer-0.1.tar.gz\n",
    "MACHINE_TYPE=n1-standard-4\n",
    "REPLICA_COUNT=1\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=50\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=5000\n",
    "NUM_EVALS=100\n",
    "NBUCKETS=10\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "WORKER_POOL_SPEC=\"machine-type=$MACHINE_TYPE,\\\n",
    "replica-count=$REPLICA_COUNT,\\\n",
    "executor-image-uri=$PYTHON_PACKAGE_EXECUTOR_IMAGE_URI,\\\n",
    "python-module=$PYTHON_MODULE\"\n",
    "\n",
    "ARGS=\"--eval_data_path=$EVAL_DATA_PATH,\\\n",
    "--output_dir=$OUTDIR,\\\n",
    "--train_data_path=$TRAIN_DATA_PATH,\\\n",
    "--batch_size=$BATCH_SIZE,\\\n",
    "--num_examples_to_train_on=$NUM_EXAMPLES_TO_TRAIN_ON,\\\n",
    "--num_evals=$NUM_EVALS,\\\n",
    "--nbuckets=$NBUCKETS,\\\n",
    "--lr=$LR,\\\n",
    "--nnsize=$NNSIZE\"\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --python-package-uris=$PYTHON_PACKAGE_URIS \\\n",
    "  --worker-pool-spec=$WORKER_POOL_SPEC \\\n",
    "  --args=\"$ARGS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your training package using a custom container\n",
    "\n",
    "Vertex AI Training also supports training in custom containers, allowing users to bring their own Docker containers with any pre-installed ML framework or algorithm to run on Vertex AI Training. \n",
    "\n",
    "In this last section, we'll see how to submit a Cloud training job using a customized Docker image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Containerizing our `./taxifare/trainer` package involves 3 steps:\n",
    "\n",
    "* Writing a Dockerfile in `./taxifare`\n",
    "* Building the Docker image\n",
    "* Pushing it to the Google Cloud Artifact Registry in our GCP project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dockerfile` specifies\n",
    "1. How the container needs to be provisioned so that all the dependencies in our code are satisfied\n",
    "2. Where to copy our trainer Package in the container\n",
    "3. What command to run when the container is ran (the `ENTRYPOINT` line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #3**: Running your training package using Docker containers.\n",
    "Fill in the TODOs in the code below for Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./taxifare/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./taxifare/Dockerfile\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest\n",
    "\n",
    "COPY . /code\n",
    "\n",
    "WORKDIR /code\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/jupyter/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credential-stores\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 181B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load metadata for us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest\n",
      "#2 DONE 0.2s\n",
      "\n",
      "#3 [internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [internal] load build context\n",
      "#4 transferring context: 106.31kB done\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [1/3] FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest@sha256:edfbaf60dff875781ddd03a7e5683e03feaea0e99a3452db6cf9ba3077fa0e3f\n",
      "#5 resolve us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest@sha256:edfbaf60dff875781ddd03a7e5683e03feaea0e99a3452db6cf9ba3077fa0e3f 0.0s done\n",
      "#5 sha256:edfbaf60dff875781ddd03a7e5683e03feaea0e99a3452db6cf9ba3077fa0e3f 5.96kB / 5.96kB done\n",
      "#5 sha256:7a4eda0e06178110d3c1ab6255fb12d88111745d0a643eef1ef4a9732a72e272 14.18kB / 14.18kB done\n",
      "#5 sha256:7478e0ac0f23f94b2f27848fbcdf804a670fbf8d4bab26df842d40a10cd33059 12.36MB / 30.44MB 0.2s\n",
      "#5 sha256:2fc1f4ab054ff97709a042230ff516042d66c5869f2d81b3aeba5f7a4915d4a2 4.09kB / 4.09kB 0.1s done\n",
      "#5 sha256:6f5ed47aea6f50d3b65184cd6436f6ad61a0cecc8a0c2a9435cf45cbebcdfe24 324B / 324B 0.2s done\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 0B / 108.19MB 0.2s\n",
      "#5 sha256:7478e0ac0f23f94b2f27848fbcdf804a670fbf8d4bab26df842d40a10cd33059 28.31MB / 30.44MB 0.3s\n",
      "#5 sha256:9d3e371b78d162759f4cb1f0a5005a35337efd85f9e34ece0dc0b1908d552767 1.44kB / 1.44kB 0.2s done\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 0B / 207.22MB 0.3s\n",
      "#5 sha256:7478e0ac0f23f94b2f27848fbcdf804a670fbf8d4bab26df842d40a10cd33059 30.44MB / 30.44MB 0.4s done\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 15.73MB / 108.19MB 0.4s\n",
      "#5 sha256:3686bcc9798ebeb93088bb5a90d83af2752b85e14f11cb1597632a89cdb5ff22 0B / 184.84kB 0.4s\n",
      "#5 extracting sha256:7478e0ac0f23f94b2f27848fbcdf804a670fbf8d4bab26df842d40a10cd33059\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 27.10MB / 108.19MB 0.5s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 17.83MB / 207.22MB 0.5s\n",
      "#5 sha256:3686bcc9798ebeb93088bb5a90d83af2752b85e14f11cb1597632a89cdb5ff22 184.84kB / 184.84kB 0.5s done\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 0B / 413.52MB 0.5s\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 37.75MB / 108.19MB 0.6s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 28.31MB / 207.22MB 0.6s\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 47.19MB / 108.19MB 0.7s\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 57.67MB / 108.19MB 0.8s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 45.09MB / 207.22MB 0.8s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 20.86MB / 413.52MB 0.8s\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 76.55MB / 108.19MB 1.0s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 62.91MB / 207.22MB 1.0s\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 83.89MB / 108.19MB 1.1s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 49.28MB / 413.52MB 1.1s\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 99.61MB / 108.19MB 1.3s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 88.08MB / 207.22MB 1.3s\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 108.00MB / 108.19MB 1.4s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 74.35MB / 413.52MB 1.4s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 106.95MB / 207.22MB 1.5s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 123.73MB / 207.22MB 1.7s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 101.48MB / 413.52MB 1.7s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 134.22MB / 207.22MB 1.8s\n",
      "#5 sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 108.19MB / 108.19MB 1.8s done\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 154.14MB / 207.22MB 2.0s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 126.98MB / 413.52MB 2.0s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 5.24MB / 997.10MB 2.0s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 169.87MB / 207.22MB 2.2s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 153.09MB / 413.52MB 2.3s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 184.55MB / 207.22MB 2.4s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 201.33MB / 207.22MB 2.6s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 178.26MB / 413.52MB 2.6s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 58.72MB / 997.10MB 2.6s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 214.15MB / 413.52MB 3.0s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 114.29MB / 997.10MB 3.2s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 241.17MB / 413.52MB 3.3s\n",
      "#5 sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 207.22MB / 207.22MB 3.5s done\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 269.31MB / 413.52MB 3.6s\n",
      "#5 sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 0B / 32B 3.6s\n",
      "#5 sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 32B / 32B 3.7s done\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 169.87MB / 997.10MB 3.8s\n",
      "#5 sha256:b9e00bb7091e876872f60df382fb8bea9830f9b328c2a6733cfacfc073dbeca6 0B / 128B 3.8s\n",
      "#5 extracting sha256:7478e0ac0f23f94b2f27848fbcdf804a670fbf8d4bab26df842d40a10cd33059 3.4s done\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 305.14MB / 413.52MB 4.0s\n",
      "#5 sha256:b9e00bb7091e876872f60df382fb8bea9830f9b328c2a6733cfacfc073dbeca6 128B / 128B 3.8s done\n",
      "#5 sha256:5dba004932da027ded77e99caf6b6d94788cb958a03e871e29fb599ddc3c8625 899B / 899B 3.9s done\n",
      "#5 sha256:b1daad8f3b0b12574801378a6cc34fb716889e899b886884822a0b4b194ad51b 0B / 445B 4.0s\n",
      "#5 sha256:b1daad8f3b0b12574801378a6cc34fb716889e899b886884822a0b4b194ad51b 445B / 445B 4.1s done\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 333.45MB / 413.52MB 4.3s\n",
      "#5 extracting sha256:2fc1f4ab054ff97709a042230ff516042d66c5869f2d81b3aeba5f7a4915d4a2 done\n",
      "#5 extracting sha256:6f5ed47aea6f50d3b65184cd6436f6ad61a0cecc8a0c2a9435cf45cbebcdfe24 done\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 7.59MB / 254.06MB 4.3s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 230.69MB / 997.10MB 4.5s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 23.07MB / 254.06MB 4.5s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 356.52MB / 413.52MB 4.6s\n",
      "#5 extracting sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 0.1s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 44.04MB / 254.06MB 4.8s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 390.07MB / 413.52MB 5.0s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 60.82MB / 254.06MB 5.0s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 284.16MB / 997.10MB 5.1s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 413.52MB / 413.52MB 5.3s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 84.79MB / 254.06MB 5.3s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 102.76MB / 254.06MB 5.5s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 346.03MB / 997.10MB 5.7s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 123.73MB / 254.06MB 5.7s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 155.19MB / 254.06MB 6.0s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 177.21MB / 254.06MB 6.2s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 412.09MB / 997.10MB 6.4s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 200.28MB / 254.06MB 6.4s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 230.69MB / 254.06MB 6.7s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 475.00MB / 997.10MB 7.0s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 254.06MB / 254.06MB 7.0s\n",
      "#5 sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 413.52MB / 413.52MB 7.1s done\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 0B / 246.79MB 7.1s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 27.26MB / 246.79MB 7.4s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 525.34MB / 997.10MB 7.5s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 58.72MB / 246.79MB 7.7s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 80.74MB / 246.79MB 7.9s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 584.06MB / 997.10MB 8.1s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 102.76MB / 246.79MB 8.1s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 115.34MB / 246.79MB 8.2s\n",
      "#5 sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 254.06MB / 254.06MB 8.2s done\n",
      "#5 sha256:fc9291f81db757137ab2224dbf72230d002f374f9b76d8525b61d445f4037695 0B / 618B 8.3s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 144.70MB / 246.79MB 8.5s\n",
      "#5 sha256:fc9291f81db757137ab2224dbf72230d002f374f9b76d8525b61d445f4037695 618B / 618B 8.3s done\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 4.19MB / 233.96MB 8.5s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 157.29MB / 246.79MB 8.6s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 641.73MB / 997.10MB 8.7s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 18.87MB / 233.96MB 8.7s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 177.21MB / 246.79MB 8.8s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 31.46MB / 233.96MB 8.9s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 201.33MB / 246.79MB 9.1s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 44.04MB / 233.96MB 9.1s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 701.61MB / 997.10MB 9.4s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 229.64MB / 246.79MB 9.4s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 66.38MB / 233.96MB 9.4s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 243.27MB / 246.79MB 9.6s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 82.84MB / 233.96MB 9.6s\n",
      "#5 extracting sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 5.3s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 112.20MB / 233.96MB 9.9s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 759.17MB / 997.10MB 10.0s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 131.07MB / 233.96MB 10.1s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 153.09MB / 233.96MB 10.3s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 810.55MB / 997.10MB 10.5s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 176.16MB / 233.96MB 10.5s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 195.04MB / 233.96MB 10.7s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 216.01MB / 233.96MB 10.9s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 867.17MB / 997.10MB 11.1s\n",
      "#5 sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 246.79MB / 246.79MB 11.0s done\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 231.74MB / 233.96MB 11.1s\n",
      "#5 sha256:8c081523305867b1fda7bddedc768d574a0ad9d437758b8a82a6c437cec22737 0B / 176B 11.3s\n",
      "#5 sha256:8c081523305867b1fda7bddedc768d574a0ad9d437758b8a82a6c437cec22737 176B / 176B 11.6s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 936.25MB / 997.10MB 11.8s\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 987.76MB / 997.10MB 12.3s\n",
      "#5 sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 233.96MB / 233.96MB 12.3s done\n",
      "#5 sha256:8c081523305867b1fda7bddedc768d574a0ad9d437758b8a82a6c437cec22737 176B / 176B 12.3s done\n",
      "#5 sha256:adc20cdd144991dceb862633833723011645c34cc881adb96b83c32605cdbd3d 156B / 156B 12.5s\n",
      "#5 sha256:1dd6c37c36aeee46977a5989d0fca6cf3c45980834c41ace34d5a1a134e67e68 4.67kB / 4.67kB 12.5s\n",
      "#5 extracting sha256:9c6f9f8406bac57c9203ac532117dc788d894d8758124c9ebd6c4c4ec6858b2a 9.1s done\n",
      "#5 extracting sha256:9d3e371b78d162759f4cb1f0a5005a35337efd85f9e34ece0dc0b1908d552767 done\n",
      "#5 sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 997.10MB / 997.10MB 16.7s done\n",
      "#5 sha256:adc20cdd144991dceb862633833723011645c34cc881adb96b83c32605cdbd3d 156B / 156B 16.8s done\n",
      "#5 sha256:1dd6c37c36aeee46977a5989d0fca6cf3c45980834c41ace34d5a1a134e67e68 4.67kB / 4.67kB 16.8s done\n",
      "#5 sha256:167dd2ad3411a57fdc7a24636c73202e267a33f80ee7ddcfda1dd94b3bba57f8 0B / 154B 16.8s\n",
      "#5 extracting sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08\n",
      "#5 sha256:167dd2ad3411a57fdc7a24636c73202e267a33f80ee7ddcfda1dd94b3bba57f8 154B / 154B 16.9s\n",
      "#5 sha256:88a5c36ba2926cf07bd7ed5787a14d75b7de00d4c7ab04e3197ad100b4ffff07 0B / 160B 16.9s\n",
      "#5 sha256:aa0eae780ea1755c84561d2e50a1f8d79ad5dd51f0b8f5f41287cb249ae3b8ed 11.72kB / 11.72kB 16.9s done\n",
      "#5 sha256:caeb9ac3c4842526e1d3737860f415263105d1a7e8fb2d35bdad31d829aede1a 0B / 1.38kB 16.9s\n",
      "#5 sha256:167dd2ad3411a57fdc7a24636c73202e267a33f80ee7ddcfda1dd94b3bba57f8 154B / 154B 16.9s done\n",
      "#5 sha256:88a5c36ba2926cf07bd7ed5787a14d75b7de00d4c7ab04e3197ad100b4ffff07 160B / 160B 16.9s done\n",
      "#5 sha256:caeb9ac3c4842526e1d3737860f415263105d1a7e8fb2d35bdad31d829aede1a 1.38kB / 1.38kB 17.0s done\n",
      "#5 extracting sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 5.2s\n",
      "#5 extracting sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 10.3s\n",
      "#5 extracting sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 15.4s\n",
      "#5 extracting sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 20.4s\n",
      "#5 extracting sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 25.4s\n",
      "#5 extracting sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 31.0s\n",
      "#5 extracting sha256:78ee7c299e7f457d232f7cedd86ae492f61ee7dc6c1680635228a6dc04114b08 34.0s done\n",
      "#5 extracting sha256:3686bcc9798ebeb93088bb5a90d83af2752b85e14f11cb1597632a89cdb5ff22\n",
      "#5 extracting sha256:3686bcc9798ebeb93088bb5a90d83af2752b85e14f11cb1597632a89cdb5ff22 0.0s done\n",
      "#5 extracting sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 0.1s\n",
      "#5 extracting sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 5.2s\n",
      "#5 extracting sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 10.3s\n",
      "#5 extracting sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 15.4s\n",
      "#5 extracting sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 21.5s\n",
      "#5 extracting sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 26.6s\n",
      "#5 extracting sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 31.9s\n",
      "#5 extracting sha256:62ea6c4483ee681b3e54996c93abbe395edcb94937ead277671129d3c5a41270 32.6s done\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 5.1s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 10.2s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 15.2s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 20.2s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 25.3s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 30.5s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 35.8s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 40.8s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 45.8s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 50.8s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 55.9s\n",
      "#5 extracting sha256:d3f0eb5c883f4d717cd3aec2889b96b26fe93b7131e502d8cee16727c7976de5 57.4s done\n",
      "#5 extracting sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1\n",
      "#5 extracting sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 done\n",
      "#5 extracting sha256:b9e00bb7091e876872f60df382fb8bea9830f9b328c2a6733cfacfc073dbeca6 done\n",
      "#5 extracting sha256:5dba004932da027ded77e99caf6b6d94788cb958a03e871e29fb599ddc3c8625 done\n",
      "#5 extracting sha256:b1daad8f3b0b12574801378a6cc34fb716889e899b886884822a0b4b194ad51b\n",
      "#5 extracting sha256:b1daad8f3b0b12574801378a6cc34fb716889e899b886884822a0b4b194ad51b done\n",
      "#5 extracting sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f\n",
      "#5 extracting sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 5.0s\n",
      "#5 extracting sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 10.1s\n",
      "#5 extracting sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 15.3s\n",
      "#5 extracting sha256:2bc6bb9773e0006aa0beea666b873cb9ca40c9c0e3bb06ad33cc386dacde080f 15.7s done\n",
      "#5 extracting sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d\n",
      "#5 extracting sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 5.0s\n",
      "#5 extracting sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 10.1s\n",
      "#5 extracting sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 15.3s\n",
      "#5 extracting sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 20.8s\n",
      "#5 extracting sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 25.9s\n",
      "#5 extracting sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 30.9s\n",
      "#5 extracting sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 36.0s\n",
      "#5 extracting sha256:6e26c7a5782305785fb72eaea73e6cbb36eb41d94d4c51fa71a9fd9c0cecd92d 39.3s done\n",
      "#5 extracting sha256:fc9291f81db757137ab2224dbf72230d002f374f9b76d8525b61d445f4037695\n",
      "#5 extracting sha256:fc9291f81db757137ab2224dbf72230d002f374f9b76d8525b61d445f4037695 done\n",
      "#5 extracting sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049\n",
      "#5 extracting sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 5.1s\n",
      "#5 extracting sha256:830591ff9e7ab5cedc4576fcf1fb7d4e78771f3b6b3f20613b61c85809e75049 7.0s done\n",
      "#5 extracting sha256:8c081523305867b1fda7bddedc768d574a0ad9d437758b8a82a6c437cec22737\n",
      "#5 extracting sha256:8c081523305867b1fda7bddedc768d574a0ad9d437758b8a82a6c437cec22737 done\n",
      "#5 extracting sha256:adc20cdd144991dceb862633833723011645c34cc881adb96b83c32605cdbd3d done\n",
      "#5 extracting sha256:1dd6c37c36aeee46977a5989d0fca6cf3c45980834c41ace34d5a1a134e67e68 done\n",
      "#5 extracting sha256:167dd2ad3411a57fdc7a24636c73202e267a33f80ee7ddcfda1dd94b3bba57f8\n",
      "#5 extracting sha256:167dd2ad3411a57fdc7a24636c73202e267a33f80ee7ddcfda1dd94b3bba57f8 done\n",
      "#5 extracting sha256:88a5c36ba2926cf07bd7ed5787a14d75b7de00d4c7ab04e3197ad100b4ffff07 done\n",
      "#5 extracting sha256:aa0eae780ea1755c84561d2e50a1f8d79ad5dd51f0b8f5f41287cb249ae3b8ed done\n",
      "#5 extracting sha256:caeb9ac3c4842526e1d3737860f415263105d1a7e8fb2d35bdad31d829aede1a\n",
      "#5 extracting sha256:caeb9ac3c4842526e1d3737860f415263105d1a7e8fb2d35bdad31d829aede1a done\n",
      "#5 DONE 215.2s\n",
      "\n",
      "#6 [2/3] COPY . /code\n",
      "#6 DONE 0.1s\n",
      "\n",
      "#7 [3/3] WORKDIR /code\n",
      "#7 DONE 0.0s\n",
      "\n",
      "#8 exporting to image\n",
      "#8 exporting layers 0.1s done\n",
      "#8 writing image sha256:26447ff2dde49bc758a067fe8d557eccb8d588a0b586788332b3dfd81c764d6f done\n",
      "#8 naming to us-docker.pkg.dev/qwiklabs-asl-01-19968276eb55/asl-artifact-repo/taxifare_training_container:latest done\n",
      "#8 DONE 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-docker.pkg.dev/qwiklabs-asl-01-19968276eb55/asl-artifact-repo/taxifare_training_container]\n",
      "5f70bf18a086: Preparing\n",
      "7fb0116cb477: Preparing\n",
      "e42695c7b436: Preparing\n",
      "e42695c7b436: Preparing\n",
      "7e34967c8575: Preparing\n",
      "4e316f7a8cfd: Preparing\n",
      "aabd5006c821: Preparing\n",
      "4d19c75cc81d: Preparing\n",
      "4d19c75cc81d: Preparing\n",
      "43787889dfe1: Preparing\n",
      "e51967d24b89: Preparing\n",
      "34555790f018: Preparing\n",
      "8536d85d1fc7: Preparing\n",
      "5d5447dbfb9f: Preparing\n",
      "17cade2859ad: Preparing\n",
      "e432beef45ab: Preparing\n",
      "f4dec8971824: Preparing\n",
      "f4dec8971824: Preparing\n",
      "4b4661d32244: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "4d8c0fa3291e: Preparing\n",
      "6647ad4a8716: Preparing\n",
      "6bb43c4536cc: Preparing\n",
      "49fd3d481bad: Preparing\n",
      "2158942577bb: Preparing\n",
      "6fadc2de3475: Preparing\n",
      "c2000232ca5b: Preparing\n",
      "d84a35e62906: Preparing\n",
      "2573e0d81582: Preparing\n",
      "aabd5006c821: Waiting\n",
      "4d19c75cc81d: Waiting\n",
      "43787889dfe1: Waiting\n",
      "e51967d24b89: Waiting\n",
      "34555790f018: Waiting\n",
      "8536d85d1fc7: Waiting\n",
      "5d5447dbfb9f: Waiting\n",
      "4b4661d32244: Waiting\n",
      "17cade2859ad: Waiting\n",
      "4d8c0fa3291e: Waiting\n",
      "6647ad4a8716: Waiting\n",
      "e432beef45ab: Waiting\n",
      "6bb43c4536cc: Waiting\n",
      "f4dec8971824: Waiting\n",
      "d84a35e62906: Waiting\n",
      "2573e0d81582: Waiting\n",
      "49fd3d481bad: Waiting\n",
      "2158942577bb: Waiting\n",
      "6fadc2de3475: Waiting\n",
      "c2000232ca5b: Waiting\n",
      "5f70bf18a086: Layer already exists\n",
      "4e316f7a8cfd: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "7fb0116cb477: Pushed\n",
      "e42695c7b436: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "7e34967c8575: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "aabd5006c821: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "43787889dfe1: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "4d19c75cc81d: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "34555790f018: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "e51967d24b89: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "8536d85d1fc7: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "17cade2859ad: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "5d5447dbfb9f: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "e432beef45ab: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "f4dec8971824: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "4b4661d32244: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "4d8c0fa3291e: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "6647ad4a8716: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "6bb43c4536cc: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "2158942577bb: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "49fd3d481bad: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "c2000232ca5b: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "6fadc2de3475: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "2573e0d81582: Layer already exists\n",
      "d84a35e62906: Mounted from vertex-ai/training/tf-cpu.2-12.py310\n",
      "latest: digest: sha256:cc82ee864c6d0b36bc45538c25e9d5c75fe2961103b1dbfcfd8d633749524d6e size: 6379\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "PROJECT_DIR=$(cd ./taxifare && pwd)\n",
    "ARTIFACT_REGISTRY_DIR=asl-artifact-repo\n",
    "IMAGE_NAME=taxifare_training_container\n",
    "DOCKERFILE=$PROJECT_DIR/Dockerfile\n",
    "IMAGE_URI=us-docker.pkg.dev/$PROJECT/$ARTIFACT_REGISTRY_DIR/$IMAGE_NAME:latest\n",
    "\n",
    "# Authorize Artifact Registry\n",
    "gcloud auth print-access-token | docker login -u oauth2accesstoken --password-stdin https://us-docker.pkg.dev\n",
    "\n",
    "docker build $PROJECT_DIR -f $DOCKERFILE -t $IMAGE_URI\n",
    "\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** If you prefer to build the container image from the command line, we have written a script for that `./taxifare/scripts/build.sh`. This script reads its configuration from the file `./taxifare/scripts/env.sh`. You can configure these arguments the way you want in that file. You can also simply type `make build` from within `./taxifare` to build the image (which will invoke the build script). Similarly, we wrote the script `./taxifare/scripts/push.sh` to push the Docker image, which you can also trigger by typing `make push` from within `./taxifare`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train using a custom container on Vertex AI\n",
    "\n",
    "TODO: To submit to the Cloud we use [`gcloud ai custom-jobs create`](https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create) and simply specify some additional parameters for Vertex AI Training Service:\n",
    "- job_name: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- image-uri: The uri of the Docker image we pushed in the Google Cloud registry\n",
    "- region: Cloud region to train in. See [here](https://cloud.google.com/vertex-ai/docs/general/locations) for supported Vertex AI Training Service regions\n",
    "\n",
    "The arguments within `--args` are sent to our `task.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track your job and view logs using [cloud console](https://console.cloud.google.com/mlengine/jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-01-19968276eb55/taxifare/trained_model_20250514_025453 us-central1 taxifare_container_20250514_025453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/604342147284/locations/us-central1/customJobs/949360825238290432] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/604342147284/locations/us-central1/customJobs/949360825238290432\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/604342147284/locations/us-central1/customJobs/949360825238290432\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Output directory and jobID\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model_$TIMESTAMP\n",
    "JOB_NAME=taxifare_container_$TIMESTAMP\n",
    "echo ${OUTDIR} ${REGION} ${JOB_NAME}\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=50\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=5000\n",
    "NUM_EVALS=100\n",
    "NBUCKETS=10\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# Vertex AI machines to use for training\n",
    "MACHINE_TYPE=n1-standard-4\n",
    "REPLICA_COUNT=1\n",
    "\n",
    "# GCS paths.\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "ARTIFACT_REGISTRY_DIR=asl-artifact-repo\n",
    "IMAGE_NAME=taxifare_training_container\n",
    "IMAGE_URI=us-docker.pkg.dev/$PROJECT/$ARTIFACT_REGISTRY_DIR/$IMAGE_NAME:latest\n",
    "\n",
    "WORKER_POOL_SPEC=\"machine-type=$MACHINE_TYPE,\\\n",
    "replica-count=$REPLICA_COUNT,\\\n",
    "container-image-uri=$IMAGE_URI\"\n",
    "\n",
    "ARGS=\"--eval_data_path=$EVAL_DATA_PATH,\\\n",
    "--output_dir=$OUTDIR,\\\n",
    "--train_data_path=$TRAIN_DATA_PATH,\\\n",
    "--batch_size=$BATCH_SIZE,\\\n",
    "--num_examples_to_train_on=$NUM_EXAMPLES_TO_TRAIN_ON,\\\n",
    "--num_evals=$NUM_EVALS,\\\n",
    "--nbuckets=$NBUCKETS,\\\n",
    "--lr=$LR,\\\n",
    "--nnsize=$NNSIZE\"\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=$REGION \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --worker-pool-spec=$WORKER_POOL_SPEC \\\n",
    "  --args=\"$ARGS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

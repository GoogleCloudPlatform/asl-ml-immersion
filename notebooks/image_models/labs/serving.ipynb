{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e35ea3a-590f-4c90-9c48-862bd106e964",
   "metadata": {},
   "source": [
    "# Serving Models\n",
    "## Learning Objectives\n",
    "1. Learn how to export TensorFlow models in SavedModel format\n",
    "2. Learn how to load SavedModel\n",
    "3. Learn how to customize signatures\n",
    "4. Learn how to deploy models to Vertex AI\n",
    "5. Learn how to use a deployed model in online and batch prediction\n",
    "\n",
    "In this lab, you will learn how to serve models after training. <br>\n",
    "\n",
    "Serving machine learning models requires infrastructure. Vertex AI makes this simple by providing autoscaling services that reduce setup and maintenance effort.\n",
    "\n",
    "To use Vertex AI, we will look at how to export a TensorFlow model in SavedModel format and deploy it into Vertex AI. Along the way, you learn about signatures, how to customize them, and how to get predictions out of a deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91e398-20db-4ea5-a2c2-1a964390b5e7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9450e3d-7a36-4e02-b56d-6d1c3166264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cbc70-5260-4232-ae05-7843b874c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from google.cloud import aiplatform\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fd1a7-19f6-4cfb-bae5-2c744b5c1a68",
   "metadata": {},
   "source": [
    "## Build and Train a Model\n",
    "Model training is not a focus in this lab, so let's create a simple Mobilenet-based model and use transfer learning to train quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad79e4-b229-4939-a7cf-14d4eb38c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT + \"-flowers\"\n",
    "FILE_DIR = f\"gs://{BUCKET}/data\"\n",
    "\n",
    "CLASSES = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b3ba5-66d7-4270-958d-d0c261015c24",
   "metadata": {},
   "source": [
    "If you haven't run [create_tfrecords_at_scale.ipynb](https://github.com/GoogleCloudPlatform/asl-ml-immersion/blob/master/notebooks/image_models/solutions/create_tfrecords_at_scale.ipynb) notebook, please uncomment the cell below and copy the data from `gs://asl-public` bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2ad96-4535-48bf-b7d6-48ca27d73ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil mb gs://{BUCKET}\n",
    "# !gsutil cp gs://asl-public/data/flowers/tfrecords/* {FILE_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1798282-a8fe-4a7d-9499-7a8ef57622bd",
   "metadata": {},
   "source": [
    "This dataset contains images of flowers that have been serialized to TFRecords. Use the tf.data API to read and parse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef642f7-ef5b-4443-9169-ad4a0c9144f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {FILE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b825b7d-2ccd-425b-802c-c2401aaf9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATTERN = FILE_DIR + \"/train*\"\n",
    "EVAL_PATTERN = FILE_DIR + \"/eval*\"\n",
    "\n",
    "\n",
    "def parse_example(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    example[\"image\"] = tf.image.resize(\n",
    "        example[\"image\"], [IMG_HEIGHT, IMG_WIDTH]\n",
    "    )\n",
    "    example[\"image\"] = example[\"image\"] / 255\n",
    "    return example[\"image\"], example[\"label\"]\n",
    "\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.TFRecordDataset(tf.io.gfile.glob(TRAIN_PATTERN))\n",
    "    .map(parse_example)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "eval_ds = (\n",
    "    tf.data.TFRecordDataset(tf.io.gfile.glob(EVAL_PATTERN))\n",
    "    .map(parse_example)\n",
    "    .batch(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904b0b0-3e34-43e1-a643-9fa3058c240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_selection = \"mobilenet_v2_100_224\"\n",
    "module_handle = \"https://tfhub.dev/google/imagenet/{}/feature_vector/4\".format(\n",
    "    module_selection\n",
    ")\n",
    "\n",
    "transfer_model = tf.keras.Sequential(\n",
    "    [\n",
    "        hub.KerasLayer(module_handle, trainable=True),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(\n",
    "            len(CLASSES),\n",
    "            activation=\"softmax\",\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transfer_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b4fb7-1812-4241-82e4-64873489d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=eval_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7bed7-b254-4eea-8c48-fb4e8f27b899",
   "metadata": {},
   "source": [
    "## Export & load SavedModel\n",
    "Now we have a trained model. In this section, we will export the model in SavedModel format.\n",
    "\n",
    "Also, we will look at how to extend our model with an additional serving function. Additional serving functions allow you to provide preprocessing and/or postprocessing logic to a model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc6f76-ad38-4d39-be73-1e1fb2a8e87f",
   "metadata": {},
   "source": [
    "### Export in SavedModel \n",
    "A SavedModel contains a complete TensorFlow program, including trained parameters  (i.e., tf.Variables) and the computation graph. We don't need the original model building code to run a model exported with SavedModel, making it useful for sharing or deploying with TFLite, TensorFlow.js, TensorFlow Serving, or TensorFlow Hub.\n",
    "\n",
    "You can save and load a model in the SavedModel format using the following APIs:\n",
    "\n",
    "- Low-level [tf.saved_model](https://www.tensorflow.org/api_docs/python/tf/saved_model) API. This document describes how to use this API in detail.\n",
    "  - Save: [tf.saved_model.save(model, path_to_dir)](https://www.tensorflow.org/api_docs/python/tf/saved_model/save)\n",
    "  - Load: model = [tf.saved_model.load(path_to_dir)](https://www.tensorflow.org/api_docs/python/tf/saved_model/load)\n",
    "- High-level [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly) API. We recommend to use this API for Keras model since it output additional metadata related to Keras model. For detail refer to the [keras save and serialize guide](https://www.tensorflow.org/guide/keras/save_and_serialize).\n",
    "\n",
    "If you just want to save/load weights during training, [Checkpoint](https://www.tensorflow.org/guide/checkpoint) would be sufficient.\n",
    "\n",
    "Here let's export our trained model to a SavedModel with a high-level API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39fc25-30a2-47c7-b395-de7c94be310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"export\", ignore_errors=True)\n",
    "os.mkdir(\"export\")\n",
    "transfer_model.save(\"export/flowers_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f6bb4-4070-4aef-ad08-dec47ee5abf2",
   "metadata": {},
   "source": [
    "Let's take a look at the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3523e375-1b30-47c9-9634-399b10749acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls export/flowers_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9b9d9-f6ec-4e04-a0ad-f3f027baa2c2",
   "metadata": {},
   "source": [
    "We can see multiple files in the directory.\n",
    "\n",
    "- `saved_model.pb` is the SavedModel main file which contains the actual TensorFlow program, or model, and a set of named signatures, each identifying a function that accepts tensor inputs and produces tensor outputs.\n",
    "- `keras_metadata.pb` file is created only with tf.keras.Model.save() function. It contains metadata regarding the Keras model.\n",
    "- `variables` directory contains all the variables of the model.\n",
    "- `assets` directory contains arbitrary files, called assets, that are needed for SavedModel. For example, a vocabulary file used to initialize a lookup table. Upon loading, the assets and the serialized functions that depend on them will refer to the correct file paths inside the SavedModel directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e28749-7773-41c3-b995-52a31555e34e",
   "metadata": {},
   "source": [
    "### Investigate a SavedModel with `saved_model_cli` command\n",
    "\n",
    "If you installed TensorFlow through a pre-built TensorFlow binary, then the SavedModel CLI is already installed on your system at pathname `bin/saved_model_cli`.\n",
    "\n",
    "The SavedModel CLI supports the following two commands on a SavedModel:\n",
    "\n",
    "- `show`, which shows the computations available from a SavedModel.\n",
    "- `run`, which runs a computation from a SavedModel.\n",
    "\n",
    "Here let's investigate the SavedModel with `saved_model_cli show` command with `--dir` option to specify the file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775b6df-d05e-4ca9-917f-08339c24d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/flowers_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26701e-c8df-4842-b70b-3119bf3245c2",
   "metadata": {},
   "source": [
    "The command above was not specific enough to investigate a model graph. <br>\n",
    "A SavedModel may contain multiple model variants identified by their tag-sets. But this is very rare.\n",
    "\n",
    "Usually, we can simply specify a default tag-set name, `serve`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36020ab8-3301-4737-9fec-428ac12e00d7",
   "metadata": {},
   "source": [
    "**Exercise**: Complete `saved_model_cli` command and get model information under the `serve` tag.\n",
    "\n",
    "**Hint**: You can run `saved_model_cli show -h` to check command help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63091066-ff7c-424f-b87d-85d8f8cb7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/flowers_model # TODO: Specify serve tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375b7c5-4eb2-4cf1-9a2e-705ebeef499f",
   "metadata": {},
   "source": [
    "By adding a signature in the signature_def option, we can identify a specific model variant to be called.\n",
    "\n",
    "The name of the default serving function is `serving_default`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc31ded-b4bd-4b31-9787-306ac72b648d",
   "metadata": {},
   "source": [
    "**Exercise**: Complete `saved_model_cli` command and get model information of the `serving_default` signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c6dc1-0cca-475e-b4d4-29db6beeec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/flowers_model # TODO: Specify serve tag, and serving_default signature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a884a2-dcbd-4361-bc2b-447a4f99a9bf",
   "metadata": {},
   "source": [
    "Now we can see the concrete function's descriptions.\n",
    "\n",
    "- Its input shape is a batch(`-1`) of  224x224 images with 3 channels, named `'keras_layer_input'`.\n",
    "- Its output shape is a batched 5 float values (that represent probabilities of 5 flowers), named `'dense'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8111b3-5286-4647-9842-190f351ab29e",
   "metadata": {},
   "source": [
    "### load and predict\n",
    "\n",
    "Once exported as SavedModel, we can load and use the model in a program.\n",
    "\n",
    "If it is a Python program where the TensorFlow module is installed, you can just call `tf.saved_model.load(path)` for model loading.\n",
    "\n",
    "Also, in the prediction phase, we sometimes (like in the Web API case) cannot expect that our model always receives preprocessed TFRecords data or batched `(224, 224, 3)` Tensors that we used in the training phase.<br>\n",
    "Let's say the model receives file paths to image data. Then we need to add preprocessing operations to handle the image paths before calling the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12801dfb-aea1-4b97-8214-74db2551ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"gs://asl-public/data/flowers/jpegs/10172567486_2748826a8b.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10386503264_e05387e1f7_m.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10391248763_1d16681106_n.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10712722853_5632165b04.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10778387133_9141024b10.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/112334842_3ecf7585dd.jpg\",\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(img_bytes):\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=IMG_CHANNELS)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_from_jpegfile(filename):\n",
    "    # same code as in 05_create_dataset/jpeg_to_tfrecord.py\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = preprocess(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "serving_model = tf.keras.models.load_model(\"export/flowers_model\")\n",
    "input_images = [read_from_jpegfile(f) for f in filenames]\n",
    "\n",
    "f, ax = plt.subplots(1, 6, figsize=(15, 15))\n",
    "for idx, img in enumerate(input_images):\n",
    "    ax[idx].imshow(img.numpy())\n",
    "    batch_image = tf.expand_dims(img, axis=0)\n",
    "    batch_pred = serving_model.predict(batch_image)\n",
    "    pred = batch_pred[0]\n",
    "    pred_label_index = tf.math.argmax(pred).numpy()\n",
    "    pred_label = CLASSES[pred_label_index]\n",
    "    prob = pred[pred_label_index]\n",
    "    ax[idx].set_title(f\"{pred_label} ({prob:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04613d35-f175-40ac-a771-4a649cac3a0f",
   "metadata": {},
   "source": [
    "### Additional Serving signature\n",
    "\n",
    "So do we need to write this boiler-plate preprocessing code each time?<br>\n",
    "The answer is no! By adding a custom serving signature, we can incorporate these additional preprocessing or postprocessing functions into the SavedModel itself.\n",
    "\n",
    "Once compiled in a SavedModel, you can run the same logic in a different environment, including edge devices, C++ code, and Javascript programs.\n",
    "\n",
    "Let's assume we deploy this model to a web server and provide prediction via a web API.\n",
    "\n",
    "What kind of signature would be easiest for API clients to use? Instead of asking them to send us tensors of the image contents, we can simply ask them for a GCS JPEG file path, for example. <br>\n",
    "And instead of returning a tensor of 5 probabilities, we can send back easy-to-understand information extracted from the probabilities.\n",
    "\n",
    "We should wrap our processing code with the [`@tf.function()`](https://www.tensorflow.org/api_docs/python/tf/function?version=nightly) decorator.\n",
    "\n",
    "By doing so, the tf.function-decorated methods are saved along with a SavedModel. However, any Python attributes, functions, and data are lost. This means that when a `tf.function` is exported, it saves a compiled graph, but not Python code itself.\n",
    "\n",
    "Briefly, `tf.function` works by tracing the Python code to generate a `ConcreteFunction` (a callable wrapper around `tf.Graph`). \n",
    "\n",
    "To learn more about the relationship between `tf.function` and ConcreteFunctions, refer to the [tf.function guide](https://www.tensorflow.org/guide/function).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba09f1-6a4b-4293-a30d-3139fbd98652",
   "metadata": {},
   "source": [
    "**Exercise**: Complete `predict_from_filename`.\n",
    "- Define `input_signature` type by adding a list of [`tf.TensorSpec()` objects](https://www.tensorflow.org/api_docs/python/tf/TensorSpec). Note that this new signature receives only one input that represents a data path.\n",
    "- Define a pre-processing logic. You can use [`tf.map_fn` method](https://www.tensorflow.org/api_docs/python/tf/map_fn) to apply `read_from_jpegfile` to each image data path.\n",
    "- Call `postprocess` function after the model prediction. Here you don't need to use `tf.map_fn` as the function can take a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10226025-2295-436e-a998-ef49f5b92129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(pred):\n",
    "    top_prob = tf.math.reduce_max(pred, axis=[1])\n",
    "    pred_label_index = tf.math.argmax(pred, axis=1)\n",
    "    pred_label = tf.gather(tf.convert_to_tensor(CLASSES), pred_label_index)\n",
    "\n",
    "    # custom output\n",
    "    return {\n",
    "        \"probability\": top_prob,\n",
    "        \"flower_type_int\": pred_label_index,\n",
    "        \"flower_type_str\": pred_label,\n",
    "    }\n",
    "\n",
    "\n",
    "# fmt: off\n",
    "# this function receives 1 string value.\n",
    "@tf.function(input_signature=[]) #TODO: Add tf.TensorSpecs\n",
    "def predict_from_filename(filenames):\n",
    "\n",
    "    # TODO: custom pre-process\n",
    "    # input_images =\n",
    "\n",
    "    # model\n",
    "    batch_pred = transfer_model(input_images)  # same as model.predict()\n",
    "\n",
    "    # TODO: custom post-process\n",
    "    # processed =\n",
    "    return processed\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fd534-17a9-40ea-a3cc-f4f285d496bd",
   "metadata": {},
   "source": [
    "Additionally, let's define another serving function that can receive and preprocess base64 encoded image data.<br>\n",
    "This is useful when we want to send raw image data in online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b5547-83bc-4186-bff6-97c34126295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "@tf.function(input_signature=[tf.TensorSpec([None,], dtype=tf.string)])\n",
    "def predict_from_b64(img_bytes):\n",
    "\n",
    "    # custom pre-process\n",
    "    input_images = tf.map_fn(\n",
    "        preprocess, img_bytes, fn_output_signature=tf.float32\n",
    "    )\n",
    "\n",
    "    # model\n",
    "    batch_pred = transfer_model(input_images)  # same as model.predict()\n",
    "\n",
    "    # custom post-process\n",
    "    processed = postprocess(batch_pred)\n",
    "    return processed\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59d4cd-6ce5-45d2-abfd-eca92457a8b2",
   "metadata": {},
   "source": [
    "Now we can save the model with signatures by specifying signature names and ConcreteFunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65899f-e0da-493d-91f9-838306caba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model with a new serving function.\n",
    "transfer_model.save(\n",
    "    \"export/flowers_model_with_signature\",\n",
    "    signatures={\n",
    "        \"serving_default\": predict_from_filename,\n",
    "        \"predict_base64\": predict_from_b64,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de45e28-3bd3-4f32-9faf-238dd839d0f3",
   "metadata": {},
   "source": [
    "Let's take a look at the SavedModel description again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd500139-feb5-4173-a9cb-5fa2a54cc86b",
   "metadata": {},
   "source": [
    "**Exercise**: Complete `saved_model_cli` command and get model information of the `serving_default` signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff6efc-7a4b-4be7-9416-811e59105023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/flowers_model_with_signature # TODO: Specify serve tag, and serving_default signature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27db260-09f7-438b-93c2-443372c84919",
   "metadata": {},
   "source": [
    "Now we can see our new `serving_default` gets file paths and return dictionaries with three keys (`flower_type_int`, `flower_type_str`, and `probability`).\n",
    "\n",
    "Let's try to load and use this SavedModel.<br>\n",
    "Notice that now we don't need to write additional preprocessing or postprocessing codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9810ae4-6dc9-48ab-82f4-605d42886cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = tf.keras.models.load_model(\n",
    "    \"export/flowers_model_with_signature\"\n",
    ").signatures[\"serving_default\"]\n",
    "\n",
    "pred = serving_fn(tf.convert_to_tensor(filenames))\n",
    "\n",
    "# print custom outputs\n",
    "for k in pred.keys():\n",
    "    print(f\"{k:15}: {pred[k].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258c940-161b-4d20-870e-042a3b09adaa",
   "metadata": {},
   "source": [
    "These outputs look more useful for API clients than a vector of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f76d9-7f25-431d-9600-93e115eacea6",
   "metadata": {},
   "source": [
    "And let's check `predict_base64` signature as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e71ed-fa67-494c-8ba9-a177869c7160",
   "metadata": {},
   "source": [
    "**Exercise**: Complete `saved_model_cli` command and get model information under `serve` tag and `predict_base64` signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c3707-8354-4f85-9ec8-5638da6da2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/flowers_model_with_signature # TODO: Specify serve tag, and predict_base64 signature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c69ef5-73b6-4fe1-87de-9b9207ea97d7",
   "metadata": {},
   "source": [
    "## Vertex AI Prediction\n",
    "\n",
    "Now our model is ready for deployment!\n",
    "\n",
    "In this notebook, we deploy our model to the scalable Vertex AI service.\n",
    "Vertex AI supports both Batch Prediction and Online Prediction. \n",
    "\n",
    "First, let's upload the SavedModel to Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2061728-976a-4811-bd3d-b718feb76282",
   "metadata": {},
   "source": [
    "### Upload model to Vertex AI Prediction service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69a247-619e-4f13-aac5-c5bda7ee2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "MODEL_DISPLAYNAME = f\"flower_classifier-{TIMESTAMP}\"\n",
    "\n",
    "print(f\"MODEL_DISPLAYNAME: {MODEL_DISPLAYNAME}\")\n",
    "\n",
    "# from https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-11:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9785c2-fb5a-4cd9-8b9e-5deba5d635dd",
   "metadata": {},
   "source": [
    "We upload the SavedModel to a GCS bucket at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae34dd1-0a73-43c3-aa62-0a5acd03a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -R export/flowers_model_with_signature gs://{BUCKET}/{MODEL_DISPLAYNAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629d5c4-7eed-4b53-9792-69249ebcfbef",
   "metadata": {},
   "source": [
    "We can use Python SDK to upload models.\n",
    "\n",
    "Here we are specifying `display_name`, `artifact_uri`, which is the path of SavedModel, and `serving_container_image_uri`, which is a container environment on which our model runs (pre-build container is selected in this case, but you can use a custom container if needed).\n",
    "\n",
    "For more detail, please refer to [the SDK document](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_upload)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8bf4d-6cee-4bfc-b927-a85879c5dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAYNAME,\n",
    "    artifact_uri=f\"gs://{BUCKET}/{MODEL_DISPLAYNAME}\",\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03382fbf-d5a5-48a9-8718-86f284d21f58",
   "metadata": {},
   "source": [
    "After uploading it, you can check your model on the console UI by clicking Vertex AI -> Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be32d9-b537-4694-aca6-baf3467c2b25",
   "metadata": {},
   "source": [
    "### Batch Prediction\n",
    "\n",
    "In batch prediction, we can pass a large dataset to our model and predict as a batch.<br>\n",
    "\n",
    "#### Create a prediction file\n",
    "[Batch Prediction](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions) service accepts JSON Lines, TF Records, CSV, or simple text file list format.\n",
    "\n",
    "Here we create a simple dataset containing many image file paths in [JSON Lines](https://jsonlines.org/) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3c974-fe22-4c43-9939-5dcbb2b5a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !gsutil ls -r gs://asl-public/data/flowers/jpegs/*.jpg\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d126fbb-57c5-4959-8094-7ba3a6a8d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILE = \"batch_prediction.jsonl\"\n",
    "\n",
    "with open(JSON_FILE, \"w\") as f:\n",
    "    for file in files:\n",
    "        f.write(json.dumps({\"filenames\": file}) + \"\\n\")\n",
    "\n",
    "!head {JSON_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b570f68-55af-478a-9583-818fdb5a7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp {JSON_FILE} {FILE_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa689f69-960f-449d-b859-444361111187",
   "metadata": {},
   "source": [
    "#### Send a Batch Prediction Job\n",
    "Let's call a batch prediction job with [`aiplatform.batch_predict()`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_batch_predict) function.\n",
    "\n",
    "Note that we can specify machine type and accelerator as needed.<br>\n",
    "This is very useful when we want to process a large amount of data in a limited time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff555e-8ff0-47ae-b987-a04f99cd2903",
   "metadata": {},
   "source": [
    "**Exercise**: Complete `batch_predict` function referring to the [API document](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model)\n",
    "- `job_display_name`: Display name of the batch prediction job.\n",
    "- `gcs_source`: File path of data. Here specify the JSON Lines file path.\n",
    "- `gcs_destination_prefix`: Prefix of the file path where the result will be saved.\n",
    "- `machine_type`: GCE Machine type to run batch prediction job. Specify `\"n1-standard-4\"`.\n",
    "- `accelerator_type` and `accelerator_count`: The type and the number of the accelerator to use. Specify 1 `\"NVIDIA_TESLA_T4\"` for this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28827b2b-952b-4005-b473-b5aab86a0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "JOB_DISPLAY_NAME = \"flower_classification_batch\"\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "batch_pred_job = uploaded_model.batch_predict(\n",
    "    # TODO: Specify the parameters.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6bbf35-a067-481f-8ae1-0686022ced3f",
   "metadata": {},
   "source": [
    "**Notice it takes around 20 minutes. Please wait until that or move forward to the Online Prediction section and return to the next cell later. You can check the status on Vertex AI -> Batch Predictions page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d4087-1f06-4b9e-8f78-faf011eec740",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_pred_job.output_info:\n",
    "    output_dir = batch_pred_job.output_info.gcs_output_directory\n",
    "    results = !gsutil cat {output_dir}/prediction.results*\n",
    "    for r in results[:5]:\n",
    "        r = json.loads(r)\n",
    "        print(f\"filename       : {r['instance']['filenames']}\")\n",
    "        for k in r[\"prediction\"].keys():\n",
    "            print(f\"{k:15}: {r['prediction'][k]}\")\n",
    "        print(\"*\" * 30)\n",
    "else:\n",
    "    print(f\"This job is still running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e051da5-d8ed-41fa-bac4-dba2fe2bae79",
   "metadata": {},
   "source": [
    "### Online Prediction\n",
    "\n",
    "In the Online Prediction option, you can create a dedicated endpoint for your model, and use it as a web API.\n",
    "\n",
    "Let's create an endpoint and link your model to it by [`aiplatform.Model.deploy`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_deploy) function. Here you can also specify the machine type and the accelerators.\n",
    "\n",
    "**The command below takes around 10 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240afcaf-b590-460d-83f0-bee6b4b76651",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = uploaded_model.deploy(\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5e103-f36e-464f-92ba-8f9af9d17caf",
   "metadata": {},
   "source": [
    "After the deployment, we can simply call the endpoint and retrieve the result. <br>\n",
    "You can check the endpoint details by visiting the Vertex AI -> Endpoints page.\n",
    "\n",
    "Here we stick with the [Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_predict), but note that you can call the endpoint from any environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee891ca7-7c0d-445a-aede-fbea146ca5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [{\"filenames\": f} for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5efa7-e289-4c57-9e4b-fcf8e62c07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = endpoint.predict(instances=instances)\n",
    "\n",
    "# print custom outputs\n",
    "for p in pred.predictions:\n",
    "    for k in p.keys():\n",
    "        print(f\"{k:15}: {p[k]}\")\n",
    "    print(\"*\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52abb6-4dc8-4712-8e7a-115e47d3cf82",
   "metadata": {},
   "source": [
    "### Online Prediction with raw images\n",
    "\n",
    "Next, let's call `predict_base64` signature and pass raw image data. <br>\n",
    "In order to submit raw image data via API, you must Base64 encode the data and encapsulate it in a JSON object having b64 as the key as follows:\n",
    "\n",
    "```python\n",
    "{ \"b64\": <base64 encoded string> }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348836b1-48ca-4516-85df-ab8a1d1f2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample image from GCS.\n",
    "!gsutil cp gs://asl-public/data/flowers/jpegs/10172567486_2748826a8b.jpg sample.jpg\n",
    "\n",
    "\n",
    "def b64encode(filename):\n",
    "    with open(filename, \"rb\") as ifp:\n",
    "        img_bytes = ifp.read()\n",
    "        return base64.b64encode(img_bytes).decode()\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"signature_name\": \"predict_base64\",\n",
    "    \"instances\": [{\"img_bytes\": {\"b64\": b64encode(\"./sample.jpg\")}}],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e498d3-a0ab-45ad-9eef-a9d9eb618851",
   "metadata": {},
   "source": [
    "Since Python SDK `endpoint.predict` supports only `serving_default` signature, here let's define an API call directly with general Python `request` module, and use `rawPredict` API to spefify other signatures instead.\n",
    "\n",
    "In order to do so, we need to define an authorization token and wrap it in a request header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8789d1-49c7-49e3-8e8d-3a7ff9b02887",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = (\n",
    "    GoogleCredentials.get_application_default().get_access_token().access_token\n",
    ")\n",
    "headers = {\"Authorization\": \"Bearer \" + token}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e2c95-83fa-42ea-8ef1-31152989d5f1",
   "metadata": {},
   "source": [
    "The endpoint URL is `https://<region>-aiplatform.googleapis.com/v1/projects/<project id>/locations/<region>/endpoints/<endpoint id>:rawPredict`. <br>\n",
    "Let's define accordingly and send the encoded raw image to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c5622-1faa-4595-80c9-c05802a8fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = \"https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/endpoints/{}:rawPredict\".format(\n",
    "    REGION, PROJECT, REGION, endpoint.name\n",
    ")\n",
    "\n",
    "response = requests.post(api, json=data, headers=headers)\n",
    "json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4329b92-80fe-474b-ad3f-aa060727eeb6",
   "metadata": {},
   "source": [
    "Now we could get the result from the Vertex AI online prediction service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e032c-16c0-4143-9578-bf71af10b74e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We learned how to:\n",
    "- export and load a SavedModel\n",
    "- customize the serving function to control a SavedModel behavior\n",
    "- deploy a SavedModel to Vertex AI\n",
    "- Use deployed model both for Batch and Online predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0908e0-478f-4833-9bcc-8fe91bd2c5fb",
   "metadata": {},
   "source": [
    "Copyright 2022 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

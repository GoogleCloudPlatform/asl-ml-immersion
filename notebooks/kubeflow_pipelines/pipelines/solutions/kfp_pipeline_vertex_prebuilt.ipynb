{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Continuous Training with Kubeflow Pipeline and Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to use KF pre-built components\n",
    "1. Learn how to build a KF pipeline with these components\n",
    "1. Learn how to compile, upload, and run a KF pipeline\n",
    "\n",
    "\n",
    "In this lab, you will build, deploy, and run a KFP pipeline that orchestrates the **Vertex AI** services to train, tune, and deploy a **scikit-learn** model using the Google pre-built components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:.\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow implemented by the pipeline is defined using a Python based Domain Specific Language (DSL). The pipeline's DSL is in the `pipeline_vertex/pipeline_prebuilt.py` file that we will generate below.\n",
    "\n",
    "The pipeline's DSL has been designed to avoid hardcoding any environment specific settings like file paths or connection strings. These settings are provided to the pipeline code through a set of environment variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the trainer image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training step in the pipeline will require a custom training container. The custom training image is defined in `trainer_image_vertex/Dockerfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0\n",
      "RUN pip install -U fire cloudml-hypertune scikit-learn==1.2.2\n",
      "WORKDIR /app\n",
      "COPY train.py .\n",
      "\n",
      "ENTRYPOINT [\"python\", \"train.py\"]\n"
     ]
    }
   ],
   "source": [
    "!cat trainer_image_vertex/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build and push this trainer container to the Artifact Registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-docker.pkg.dev/qwiklabs-asl-01-19968276eb55/asl-artifact-repo/trainer_image_covertype_vertex:latest'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTIFACT_REGISTRY_DIR = \"asl-artifact-repo\"\n",
    "IMAGE_NAME = \"trainer_image_covertype_vertex\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "TRAINING_CONTAINER_IMAGE_URI = f\"us-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REGISTRY_DIR}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 3.3 KiB before compression.\n",
      "Uploading tarball of [trainer_image_vertex] to [gs://qwiklabs-asl-01-19968276eb55_cloudbuild/source/1747285619.776449-0fcf0415d3174a7db600964097440ebe.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-01-19968276eb55/locations/global/builds/b7a9c5b1-3352-4577-b33f-a66a5aa10748].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/b7a9c5b1-3352-4577-b33f-a66a5aa10748?project=604342147284 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"b7a9c5b1-3352-4577-b33f-a66a5aa10748\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-01-19968276eb55_cloudbuild/source/1747285619.776449-0fcf0415d3174a7db600964097440ebe.tgz#1747285620012164\n",
      "Copying gs://qwiklabs-asl-01-19968276eb55_cloudbuild/source/1747285619.776449-0fcf0415d3174a7db600964097440ebe.tgz#1747285620012164...\n",
      "/ [1 files][  1.6 KiB/  1.6 KiB]                                                \n",
      "Operation completed over 1 objects/1.6 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/5 : FROM us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0\n",
      "latest: Pulling from vertex-ai/training/sklearn-cpu.1-0\n",
      "3153aa388d02: Pulling fs layer\n",
      "9824d45cbc78: Pulling fs layer\n",
      "a2c893dfdd76: Pulling fs layer\n",
      "3fc58b7cea6b: Pulling fs layer\n",
      "065e5d8a1c31: Pulling fs layer\n",
      "3b608554e986: Pulling fs layer\n",
      "c3fb815e9074: Pulling fs layer\n",
      "22edc53d84f1: Pulling fs layer\n",
      "fa13d373537f: Pulling fs layer\n",
      "1ff377ed9aa4: Pulling fs layer\n",
      "2854f30de59f: Pulling fs layer\n",
      "5883a3d3350c: Pulling fs layer\n",
      "b9f16134ecfb: Pulling fs layer\n",
      "739cdac78559: Pulling fs layer\n",
      "482590cd5c44: Pulling fs layer\n",
      "fcd7dfefd59a: Pulling fs layer\n",
      "d7818bca27c1: Pulling fs layer\n",
      "af8e5ef74ae3: Pulling fs layer\n",
      "ee71ca03e59f: Pulling fs layer\n",
      "b531e85b45eb: Pulling fs layer\n",
      "aa0eae780ea1: Pulling fs layer\n",
      "caeb9ac3c484: Pulling fs layer\n",
      "3fc58b7cea6b: Waiting\n",
      "065e5d8a1c31: Waiting\n",
      "3b608554e986: Waiting\n",
      "c3fb815e9074: Waiting\n",
      "22edc53d84f1: Waiting\n",
      "fa13d373537f: Waiting\n",
      "1ff377ed9aa4: Waiting\n",
      "2854f30de59f: Waiting\n",
      "5883a3d3350c: Waiting\n",
      "b9f16134ecfb: Waiting\n",
      "739cdac78559: Waiting\n",
      "482590cd5c44: Waiting\n",
      "fcd7dfefd59a: Waiting\n",
      "d7818bca27c1: Waiting\n",
      "af8e5ef74ae3: Waiting\n",
      "ee71ca03e59f: Waiting\n",
      "b531e85b45eb: Waiting\n",
      "aa0eae780ea1: Waiting\n",
      "caeb9ac3c484: Waiting\n",
      "a2c893dfdd76: Verifying Checksum\n",
      "a2c893dfdd76: Download complete\n",
      "9824d45cbc78: Verifying Checksum\n",
      "9824d45cbc78: Download complete\n",
      "065e5d8a1c31: Verifying Checksum\n",
      "065e5d8a1c31: Download complete\n",
      "3153aa388d02: Verifying Checksum\n",
      "3153aa388d02: Download complete\n",
      "c3fb815e9074: Verifying Checksum\n",
      "c3fb815e9074: Download complete\n",
      "3fc58b7cea6b: Verifying Checksum\n",
      "3fc58b7cea6b: Download complete\n",
      "22edc53d84f1: Verifying Checksum\n",
      "22edc53d84f1: Download complete\n",
      "1ff377ed9aa4: Verifying Checksum\n",
      "1ff377ed9aa4: Download complete\n",
      "3b608554e986: Verifying Checksum\n",
      "3b608554e986: Download complete\n",
      "2854f30de59f: Verifying Checksum\n",
      "2854f30de59f: Download complete\n",
      "5883a3d3350c: Verifying Checksum\n",
      "5883a3d3350c: Download complete\n",
      "739cdac78559: Verifying Checksum\n",
      "739cdac78559: Download complete\n",
      "b9f16134ecfb: Download complete\n",
      "fcd7dfefd59a: Verifying Checksum\n",
      "fcd7dfefd59a: Download complete\n",
      "3153aa388d02: Pull complete\n",
      "d7818bca27c1: Verifying Checksum\n",
      "d7818bca27c1: Download complete\n",
      "af8e5ef74ae3: Verifying Checksum\n",
      "af8e5ef74ae3: Download complete\n",
      "ee71ca03e59f: Verifying Checksum\n",
      "ee71ca03e59f: Download complete\n",
      "b531e85b45eb: Download complete\n",
      "aa0eae780ea1: Verifying Checksum\n",
      "aa0eae780ea1: Download complete\n",
      "caeb9ac3c484: Verifying Checksum\n",
      "caeb9ac3c484: Download complete\n",
      "9824d45cbc78: Pull complete\n",
      "a2c893dfdd76: Pull complete\n",
      "482590cd5c44: Verifying Checksum\n",
      "482590cd5c44: Download complete\n",
      "fa13d373537f: Verifying Checksum\n",
      "fa13d373537f: Download complete\n",
      "3fc58b7cea6b: Pull complete\n",
      "065e5d8a1c31: Pull complete\n",
      "3b608554e986: Pull complete\n",
      "c3fb815e9074: Pull complete\n",
      "22edc53d84f1: Pull complete\n",
      "fa13d373537f: Pull complete\n",
      "1ff377ed9aa4: Pull complete\n",
      "2854f30de59f: Pull complete\n",
      "5883a3d3350c: Pull complete\n",
      "b9f16134ecfb: Pull complete\n",
      "739cdac78559: Pull complete\n",
      "482590cd5c44: Pull complete\n",
      "fcd7dfefd59a: Pull complete\n",
      "d7818bca27c1: Pull complete\n",
      "af8e5ef74ae3: Pull complete\n",
      "ee71ca03e59f: Pull complete\n",
      "b531e85b45eb: Pull complete\n",
      "aa0eae780ea1: Pull complete\n",
      "caeb9ac3c484: Pull complete\n",
      "Digest: sha256:c1bc11aed8635650a6a1a1aebe7cabae3b2019c6e08f7aabf3d395b73442ba4f\n",
      "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\n",
      " ---> 57d74f998f2a\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==1.2.2\n",
      " ---> Running in 5ba49c4254ce\n",
      "Collecting fire\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.2/87.2 kB 5.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.10/site-packages (0.1.0.dev6)\n",
      "Collecting scikit-learn==1.2.2\n",
      "  Obtaining dependency information for scikit-learn==1.2.2 from https://files.pythonhosted.org/packages/fa/1e/36d7609e84b50d4a2e5bc43cd5013d9ea885799e5813a1e9cf5bb1afd3f4/scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.25.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (3.2.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire) (2.3.0)\n",
      "Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 62.7 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114247 sha256=26b2960dd1f530cab860daed55d3babfbe753a1f54853ba75567d7115d8ef86b\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
      "Successfully built fire\n",
      "Installing collected packages: fire, scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "Successfully installed fire-0.7.0 scikit-learn-1.2.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 5ba49c4254ce\n",
      " ---> 6caebf183217\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in f0e77d778ceb\n",
      "Removing intermediate container f0e77d778ceb\n",
      " ---> a4a67a73ee58\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 1060ff839a11\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 411ad2c6a2e1\n",
      "Removing intermediate container 411ad2c6a2e1\n",
      " ---> 7b4d99650590\n",
      "Successfully built 7b4d99650590\n",
      "Successfully tagged us-docker.pkg.dev/qwiklabs-asl-01-19968276eb55/asl-artifact-repo/trainer_image_covertype_vertex:latest\n",
      "PUSH\n",
      "Pushing us-docker.pkg.dev/qwiklabs-asl-01-19968276eb55/asl-artifact-repo/trainer_image_covertype_vertex:latest\n",
      "The push refers to repository [us-docker.pkg.dev/qwiklabs-asl-01-19968276eb55/asl-artifact-repo/trainer_image_covertype_vertex]\n",
      "6b002daca174: Preparing\n",
      "a95ac28d7589: Preparing\n",
      "804e9805fff1: Preparing\n",
      "e42695c7b436: Preparing\n",
      "e42695c7b436: Preparing\n",
      "7e34967c8575: Preparing\n",
      "685157cbfd1c: Preparing\n",
      "b3f395f9c6db: Preparing\n",
      "1335b11763b0: Preparing\n",
      "1335b11763b0: Preparing\n",
      "ff1b3a6f8e32: Preparing\n",
      "a4f92d8c3713: Preparing\n",
      "91315c4af0d6: Preparing\n",
      "f52f425121a7: Preparing\n",
      "76305005ccce: Preparing\n",
      "9c8c4140a22f: Preparing\n",
      "cb24f57d328e: Preparing\n",
      "cb24f57d328e: Preparing\n",
      "e9ac191ee4d5: Preparing\n",
      "e366492c2e8f: Preparing\n",
      "b3b47f6e19c9: Preparing\n",
      "9fc6ad9fa2bc: Preparing\n",
      "926e17c0ab1b: Preparing\n",
      "299edaab5a5d: Preparing\n",
      "662e1ac55b04: Preparing\n",
      "a1741b153e96: Preparing\n",
      "a8bc7d9be9d9: Preparing\n",
      "59c56aee1fb4: Preparing\n",
      "685157cbfd1c: Waiting\n",
      "b3f395f9c6db: Waiting\n",
      "1335b11763b0: Waiting\n",
      "ff1b3a6f8e32: Waiting\n",
      "a4f92d8c3713: Waiting\n",
      "91315c4af0d6: Waiting\n",
      "f52f425121a7: Waiting\n",
      "76305005ccce: Waiting\n",
      "9c8c4140a22f: Waiting\n",
      "cb24f57d328e: Waiting\n",
      "e9ac191ee4d5: Waiting\n",
      "e366492c2e8f: Waiting\n",
      "b3b47f6e19c9: Waiting\n",
      "9fc6ad9fa2bc: Waiting\n",
      "926e17c0ab1b: Waiting\n",
      "299edaab5a5d: Waiting\n",
      "662e1ac55b04: Waiting\n",
      "a1741b153e96: Waiting\n",
      "a8bc7d9be9d9: Waiting\n",
      "59c56aee1fb4: Waiting\n",
      "e42695c7b436: Layer already exists\n",
      "7e34967c8575: Layer already exists\n",
      "b3f395f9c6db: Layer already exists\n",
      "685157cbfd1c: Layer already exists\n",
      "1335b11763b0: Layer already exists\n",
      "ff1b3a6f8e32: Layer already exists\n",
      "a95ac28d7589: Pushed\n",
      "6b002daca174: Pushed\n",
      "91315c4af0d6: Layer already exists\n",
      "a4f92d8c3713: Layer already exists\n",
      "f52f425121a7: Layer already exists\n",
      "9c8c4140a22f: Layer already exists\n",
      "cb24f57d328e: Layer already exists\n",
      "e9ac191ee4d5: Layer already exists\n",
      "76305005ccce: Layer already exists\n",
      "e366492c2e8f: Layer already exists\n",
      "b3b47f6e19c9: Layer already exists\n",
      "9fc6ad9fa2bc: Layer already exists\n",
      "926e17c0ab1b: Layer already exists\n",
      "662e1ac55b04: Layer already exists\n",
      "a8bc7d9be9d9: Layer already exists\n",
      "a1741b153e96: Layer already exists\n",
      "299edaab5a5d: Layer already exists\n",
      "59c56aee1fb4: Layer already exists\n",
      "804e9805fff1: Pushed\n",
      "latest: digest: sha256:4826fb3f352ec8821ac8ee2a29f476f0862ab194310004dace980d272b7fa5a2 size: 6171\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "INFO: The service account running this build projects/qwiklabs-asl-01-19968276eb55/serviceAccounts/604342147284-compute@developer.gserviceaccount.com does not have permission to write logs to Cloud Logging. To fix this, grant the Logs Writer (roles/logging.logWriter) role to the service account.\n",
      "\n",
      "1 message(s) issued.\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                                                     STATUS\n",
      "b7a9c5b1-3352-4577-b33f-a66a5aa10748  2025-05-15T05:07:00+00:00  2M31S     gs://qwiklabs-asl-01-19968276eb55_cloudbuild/source/1747285619.776449-0fcf0415d3174a7db600964097440ebe.tgz  us-docker.pkg.dev/qwiklabs-asl-01-19968276eb55/asl-artifact-repo/trainer_image_covertype_vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINING_CONTAINER_IMAGE_URI trainer_image_vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match the ml framework version we use at training time while serving the model, we will have to supply the following serving container to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you change the version of the training ml framework you'll have to supply a serving container with matching version (see [pre-built containers for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and deploying the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the pipeline to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline_vertex/pipeline_prebuilt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_vertex/pipeline_prebuilt.py\n",
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n",
    "# use this file except in compliance with the License. You may obtain a copy of\n",
    "# the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\"\"\"Kubeflow Covertype Pipeline.\"\"\"\n",
    "import os\n",
    "\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    ")\n",
    "from google_cloud_pipeline_components.v1.hyperparameter_tuning_job import (\n",
    "    HyperparameterTuningJobRunOp,\n",
    "    serialize_metrics,\n",
    "    serialize_parameters,\n",
    ")\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from kfp import dsl\n",
    "from retrieve_best_hptune_component import retrieve_best_hptune_result\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE_URI = os.getenv(\"TRAINING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_CONTAINER_IMAGE_URI = os.getenv(\"SERVING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_MACHINE_TYPE = os.getenv(\"SERVING_MACHINE_TYPE\", \"n1-standard-16\")\n",
    "\n",
    "TRAINING_FILE_PATH = os.getenv(\"TRAINING_FILE_PATH\")\n",
    "VALIDATION_FILE_PATH = os.getenv(\"VALIDATION_FILE_PATH\")\n",
    "\n",
    "MAX_TRIAL_COUNT = int(os.getenv(\"MAX_TRIAL_COUNT\", \"5\"))\n",
    "PARALLEL_TRIAL_COUNT = int(os.getenv(\"PARALLEL_TRIAL_COUNT\", \"5\"))\n",
    "THRESHOLD = float(os.getenv(\"THRESHOLD\", \"0.6\"))\n",
    "\n",
    "PIPELINE_NAME = os.getenv(\"PIPELINE_NAME\", \"covertype\")\n",
    "BASE_OUTPUT_DIR = os.getenv(\"BASE_OUTPUT_DIR\", PIPELINE_ROOT)\n",
    "MODEL_DISPLAY_NAME = os.getenv(\"MODEL_DISPLAY_NAME\", PIPELINE_NAME)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"{PIPELINE_NAME}-kfp-pipeline\",\n",
    "    description=\"Kubeflow pipeline that tunes, trains, and deploys on Vertex\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def create_pipeline():\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                # Enable if you want to use GPU.\n",
    "                # \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "                # \"accelerator_count\": 1,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": TRAINING_CONTAINER_IMAGE_URI,\n",
    "                \"args\": [\n",
    "                    f\"--training_dataset_path={TRAINING_FILE_PATH}\",\n",
    "                    f\"--validation_dataset_path={VALIDATION_FILE_PATH}\",\n",
    "                    \"--hptune\",\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    metric_spec = serialize_metrics({\"accuracy\": \"maximize\"})\n",
    "\n",
    "    parameter_spec = serialize_parameters(\n",
    "        {\n",
    "            \"alpha\": hpt.DoubleParameterSpec(\n",
    "                min=1.0e-4, max=1.0e-1, scale=\"log\"\n",
    "            ),\n",
    "            \"max_iter\": hpt.DiscreteParameterSpec(\n",
    "                values=[1, 2], scale=\"linear\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    hp_tuning_task = HyperparameterTuningJobRunOp(\n",
    "        display_name=f\"{PIPELINE_NAME}-kfp-tuning-job\",\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        study_spec_metrics=metric_spec,\n",
    "        study_spec_parameters=parameter_spec,\n",
    "        max_trial_count=MAX_TRIAL_COUNT,\n",
    "        parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
    "        base_output_directory=PIPELINE_ROOT,\n",
    "    )\n",
    "\n",
    "    best_retrieval_task = retrieve_best_hptune_result(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        gcp_resources=hp_tuning_task.outputs[\"gcp_resources\"],\n",
    "        container_uri=TRAINING_CONTAINER_IMAGE_URI,\n",
    "        training_file_path=TRAINING_FILE_PATH,\n",
    "        validation_file_path=VALIDATION_FILE_PATH,\n",
    "    )\n",
    "\n",
    "    training_task = CustomTrainingJobOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=f\"{PIPELINE_NAME}-kfp-training-job\",\n",
    "        worker_pool_specs=best_retrieval_task.outputs[\"best_worker_pool_spec\"],\n",
    "        base_output_directory=BASE_OUTPUT_DIR,\n",
    "    )\n",
    "\n",
    "    importer_spec = dsl.importer(\n",
    "        artifact_uri=f\"{BASE_OUTPUT_DIR}/model\",\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\"containerSpec\": {\"imageUri\": SERVING_CONTAINER_IMAGE_URI}},\n",
    "    )\n",
    "    importer_spec.after(training_task)\n",
    "\n",
    "    model_upload_task = ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=f\"{PIPELINE_NAME}-kfp-model-upload-job\",\n",
    "        unmanaged_container_model=importer_spec.output,\n",
    "    )\n",
    "\n",
    "    endpoint_create_task = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=f\"{PIPELINE_NAME}-kfp-create-endpoint-job\",\n",
    "    )\n",
    "    endpoint_create_task.after(model_upload_task)\n",
    "\n",
    "    model_deploy_op = ModelDeployOp(  # pylint: disable=unused-variable\n",
    "        model=model_upload_task.outputs[\"model\"],\n",
    "        endpoint=endpoint_create_task.outputs[\"endpoint\"],\n",
    "        deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "        dedicated_resources_machine_type=SERVING_MACHINE_TYPE,\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let stat by defining the environment variables that will be passed to the pipeline compiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ROOT=gs://qwiklabs-asl-01-19968276eb55-kfp-artifact-store/pipeline\n",
      "env: PROJECT_ID=qwiklabs-asl-01-19968276eb55\n",
      "env: REGION=us-central1\n",
      "env: SERVING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\n",
      "env: TRAINING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/qwiklabs-asl-01-19968276eb55/asl-artifact-repo/trainer_image_covertype_vertex:latest\n",
      "env: TRAINING_FILE_PATH=gs://qwiklabs-asl-01-19968276eb55-kfp-artifact-store/data/training/dataset.csv\n",
      "env: VALIDATION_FILE_PATH=gs://qwiklabs-asl-01-19968276eb55-kfp-artifact-store/data/validation/dataset.csv\n",
      "env: BASE_OUTPUT_DIR=gs://qwiklabs-asl-01-19968276eb55-kfp-artifact-store/models/20250515051322\n"
     ]
    }
   ],
   "source": [
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\"\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BASE_OUTPUT_DIR = f\"{ARTIFACT_STORE}/models/{TIMESTAMP}\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env REGION={REGION}\n",
    "%env SERVING_CONTAINER_IMAGE_URI={SERVING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_CONTAINER_IMAGE_URI={TRAINING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_FILE_PATH={TRAINING_FILE_PATH}\n",
    "%env VALIDATION_FILE_PATH={VALIDATION_FILE_PATH}\n",
    "%env BASE_OUTPUT_DIR={BASE_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that the `ARTIFACT_STORE` has been created, and let us create it if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-01-19968276eb55-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, this notebook assumes the dataset is already created and stored in Google Cloud Storage following the instructions covered in the [walkthrough notebook](https://github.com/GoogleCloudPlatform/asl-ml-immersion/blob/master/notebooks/kubeflow_pipelines/walkthrough/solutions/kfp_walkthrough_vertex.ipynb).\n",
    "\n",
    "If you haven't run it, please run the cell below and create the dataset before running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://asl-public/data/covertype/training/dataset.csv [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  2.1 MiB/  2.1 MiB]                                                \n",
      "Operation completed over 1 objects/2.1 MiB.                                      \n",
      "Copying gs://asl-public/data/covertype/validation/dataset.csv [Content-Type=application/octet-stream]...\n",
      "/ [1 files][529.7 KiB/529.7 KiB]                                                \n",
      "Operation completed over 1 objects/529.7 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp gs://asl-public/data/covertype/training/dataset.csv $TRAINING_FILE_PATH\n",
    "gsutil cp gs://asl-public/data/covertype/validation/dataset.csv $VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In case the artifact store was not created and properly set before hand, you may need\n",
    "to run in **CloudShell** the following command to allow Vertex AI to access it:\n",
    "\n",
    "```\n",
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "PROJECT_NUMBER=$(gcloud projects list --filter=\"name=$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\")\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com\" \\\n",
    "    --role=\"roles/storage.objectAdmin\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CLI compiler to compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the pipeline from the Python file we generated into a YAML description using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_YAML = \"covertype_kfp_pipeline.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/asl-ml-immersion/notebooks/kubeflow_pipelines/pipelines/solutions/covertype_kfp_pipeline.yaml\n"
     ]
    }
   ],
   "source": [
    "!kfp dsl compile --py pipeline_vertex/pipeline_prebuilt.py --output $PIPELINE_YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can also use the Python SDK to compile the pipeline:\n",
    "\n",
    "```python\n",
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=create_pipeline, \n",
    "    package_path=PIPELINE_YAML,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the pipeline file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PIPELINE DEFINITION\n",
      "# Name: covertype-kfp-pipeline\n",
      "# Description: Kubeflow pipeline that tunes, trains, and deploys on Vertex\n",
      "components:\n",
      "  comp-custom-training-job:\n",
      "    executorLabel: exec-custom-training-job\n",
      "    inputDefinitions:\n",
      "      parameters:\n",
      "        base_output_directory:\n",
      "          defaultValue: ''\n"
     ]
    }
   ],
   "source": [
    "!head {PIPELINE_YAML}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/covertype-kfp-pipeline-20250515051509?project=604342147284\n",
      "PipelineJob projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/604342147284/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20250515051509\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"covertype_kfp_pipeline\",\n",
    "    template_path=PIPELINE_YAML,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "source": [
    "# Vertex pipelines\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "Use components from `google_cloud_pipeline_components` to create a Vertex Pipeline which will\n",
    "  1. train a custom model on Vertex AI\n",
    "  1. create an endpoint to host the model \n",
    "  1. upload the trained model, and \n",
    "  1. deploy the uploaded model to the endpoint for serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBRcgrOk7CUf"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows how to use the components defined in [`google_cloud_pipeline_components`](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud) in conjunction with an experimental `run_as_aiplatform_custom_job` method, to build a [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines) workflow that trains a [custom model](https://cloud.google.com/vertex-ai/docs/training/containers-overview), uploads the model, creates an endpoint, and deploys the model to the endpoint. \n",
    "\n",
    "We'll use the `kfp.v2.google.experimental.run_as_aiplatform_custom_job` method to train a custom model.\n",
    "\n",
    "The google cloud pipeline components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.1.2/). From this [github page](...) you can also find other examples in how to build a Vertex pipeline with AutoML [here](https://github.com/GoogleCloudPlatform/ai-platform-samples/tree/master/ai-platform-unified/notebooks/official/pipelines). You can see other available methods from the [Vertex AI SDK](https://googleapis.dev/python/aiplatform/latest/aiplatform.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment and install necessary packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxtzwPPNZ-SH",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip3 install --user google-cloud-pipeline-components==0.1.1 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFDUBveR5UfJ",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google import experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the versions of the packages you installed.  The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN0mULkEeb84"
   },
   "outputs": [],
   "source": [
    "print(f\"KFP SDK version: {kfp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your environment variables\n",
    "Next, we'll set up our project variables, like GCP project ID, the bucket and region. Also, to avoid name collisions between resources created, we'll create a timestamp and append it onto the name of resources we create in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "# Change below if necessary\n",
    "PROJECT = !gcloud config get-value project  # noqa: E999\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/pipeline_root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "We'll save pipeline artifacts in a directory called `pipeline_root` within our bucket. Validate access to your Cloud Storage bucket by examining its contents. It should be empty at this stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "!gsutil ls -la gs://{BUCKET}/pipeline_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give your default service account storage bucket access\n",
    "This pipeline will read `.csv` files from Cloud storage for training and will write model checkpoints and artifacts to a specified bucket. So, we need to give our default service account `storage.objectAdmin` access. You can do this by running the command below in Cloud Shell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "PROJECT=$(gcloud config get-value project)\n",
    "PROJECT_NUMBER=$(gcloud projects list --filter=\"name=$PROJECT\" --format=\"value(PROJECT_NUMBER)\")\n",
    "gcloud projects add-iam-policy-binding $PROJECT \\\n",
    "    --member=\"serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com\" \\\n",
    "    --role=\"roles/storage.objectAdmin\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, it may take some time for the permissions to propogate to the service account. You can confirm the status from the [IAM page here](https://console.cloud.google.com/iam-admin/iam). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4MjdglUT3Sw"
   },
   "source": [
    "## Define a pipeline that uses the components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpaD14TxZyVm"
   },
   "source": [
    "We'll start by defining a component with which the custom training job is run.  For this example, this component doesn't do anything (but run a print statement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5m_ZU0GzBMRi"
   },
   "outputs": [],
   "source": [
    "@component\n",
    "def training_op(input1: str):\n",
    "    print(f\"VertexAI pipeline: {input1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fjGiImBezMo"
   },
   "source": [
    "Now, you define the pipeline.  \n",
    "\n",
    "The `experimental.run_as_aiplatform_custom_job` method takes as args the component defined above, and the list of `worker_pool_specs`— in this case  one— with which the custom training job is configured. \n",
    "See [full function code here](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/v2/google/experimental/custom_job.py)\n",
    "\n",
    "Then, [`google_cloud_pipeline_components`](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud) components are used to define the rest of the pipeline: upload the model, create an endpoint, and deploy the model to the endpoint. (While not shown in this example, the model deploy will create an endpoint if one is not provided). \n",
    "\n",
    "Note that the code we're using the exact same code that we developed in the previous lab [`1_training_at_scale_vertex.ipynb`](1_training_at_scale_vertex.ipynb). In fact, we are pulling the same python package executor image URI that we pushed to Cloud storage in that lab. Note that we also include the `SERVING_CONTAINER_IMAGE_URI` since we'll need to specify that when uploading and deploying our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory and job_name\n",
    "OUTDIR = f\"gs://{BUCKET}/taxifare/trained_model_{TIMESTAMP}\"\n",
    "MODEL_DISPLAY_NAME = f\"taxifare_{TIMESTAMP}\"\n",
    "\n",
    "PYTHON_PACKAGE_URIS = f\"gs://{BUCKET}/taxifare/taxifare_trainer-0.1.tar.gz\"\n",
    "MACHINE_TYPE = \"n1-standard-16\"\n",
    "REPLICA_COUNT = 1\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-3:latest\"\n",
    ")\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\"\n",
    ")\n",
    "PYTHON_MODULE = \"trainer.task\"\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE = 500\n",
    "NUM_EXAMPLES_TO_TRAIN_ON = 10000\n",
    "NUM_EVALS = 1000\n",
    "NBUCKETS = 10\n",
    "LR = 0.001\n",
    "NNSIZE = \"32 8\"\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH = f\"gs://{BUCKET}/taxifare\"\n",
    "DATA_PATH = f\"{GCS_PROJECT_PATH}/data\"\n",
    "TRAIN_DATA_PATH = f\"{DATA_PATH}/taxi-train*\"\n",
    "EVAL_DATA_PATH = f\"{DATA_PATH}/taxi-valid*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task #1. \n",
    "\n",
    "In the cell below we define the pipeline for training and deploying our taxifare model. Fill in the code to accomplish four things:\n",
    "1. define the approrpriate `worker_pool_spec` for the training job\n",
    "1. use `ModelUploadOp` to upload the model artifacts after training to create the model in Vertex AI\n",
    "1. create an endpoing using `EndpointCreateOp`\n",
    "1. finally, deploy the model you uploaded to the endpoint you created in the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwBLkQygbxjM"
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"taxifare--train-upload-endpoint-deploy\")\n",
    "def pipeline(\n",
    "    project: str = PROJECT,\n",
    "    model_display_name: str = MODEL_DISPLAY_NAME,\n",
    "):\n",
    "    train_task = training_op(\"taxifare training pipeline\")\n",
    "    experimental.run_as_aiplatform_custom_job(\n",
    "        train_task,\n",
    "        display_name=f\"pipelines-train-{TIMESTAMP}\",\n",
    "        worker_pool_specs= \n",
    "        # TODO: Your code goes here.\n",
    "    )\n",
    "\n",
    "    model_upload_op = gcc_aip.ModelUploadOp(\n",
    "        # TODO: Your code goes here.\n",
    "    )\n",
    "    model_upload_op.after(train_task)\n",
    "\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "        # TODO: Your code goes here.\n",
    "    )\n",
    "\n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "        # TODO: Your code goes here.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hl1iYEKSzjP"
   },
   "source": [
    "## Compile and run the pipeline\n",
    "\n",
    "Now, you're ready to compile the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycRc83B6bbfO"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"vertex_pipelines\"):\n",
    "    os.mkdir(\"vertex_pipelines\")\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"./vertex_pipelines/train_upload_endpoint_deploy.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfNuzFswBB4g"
   },
   "source": [
    "The pipeline compilation generates the `train_upload_endpoint_deploy.json` job spec file.\n",
    "\n",
    "Next, instantiate the pipeline job object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task #2.\n",
    "\n",
    "Complete the code in the cell below to fill in the missing arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hl5Q74_gkW2c"
   },
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.pipeline_jobs.PipelineJob(\n",
    "    display_name=  # TODO: Your code goes here.\n",
    "    template_path=  # TODO: Your code goes here.\n",
    "    pipeline_root=  # TODO: Your code goes here.\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jrn6saiQsPh"
   },
   "source": [
    "Then, you run the defined pipeline like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4Ha4FoDQpkd"
   },
   "outputs": [],
   "source": [
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvBTCP318RKs"
   },
   "source": [
    "Click on the generated link above starting with `https://console.cloud.google.com/vertex-ai/locations/[location]/pipelines/runs/` to see your run in the Cloud Console.  It should look something like this:\n",
    "\n",
    "<img src='../assets/taxifare_vertex_pipeline.png' width='80%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4jxmfyT26gj"
   },
   "source": [
    "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "google_cloud_pipeline_components_model_train_upload_deploy.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

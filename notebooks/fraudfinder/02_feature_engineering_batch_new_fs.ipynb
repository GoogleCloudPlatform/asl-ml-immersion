{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# FraudFinder - Feature Engineering (batch) (New Feature Store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This series of labs are updated upon [FraudFinder](https://github.com/googlecloudplatform/fraudfinder) repository which builds a end-to-end real-time fraud detection system on Google Cloud. Throughout the FraudFinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model.\n",
    "\n",
    "\n",
    "In this notebook, we'll focus on a critical step in any machine learning project: **feature engineering**. You'll learn how to transform raw transaction data into meaningful features that can be used to train a powerful fraud detection model. We'll be using BigQuery for batch feature engineering and Vertex AI Feature Store to manage and serve our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25113176ad6a"
   },
   "source": [
    "### Objective\n",
    "\n",
    "The primary goal of this notebook is to introduce you to the concepts of batch feature engineering and the Vertex AI Feature Store. You will learn how to:\n",
    "\n",
    "* **Understand the difference between batch and streaming feature engineering.** We'll explore why both are essential for building a real-time fraud detection system.\n",
    "* **Create powerful features using SQL in BigQuery.** You'll write queries to extract valuable insights from historical transaction data, such as customer spending habits and terminal risk profiles.\n",
    "* **Leverage the Vertex AI Feature Store for efficient feature management.** You'll learn how to create a feature store, define feature groups, and ingest your newly created features for both training and online serving.\n",
    "* **Prepare for real-time feature engineering.** We'll also set the stage for the next notebook in this series, where you'll learn how to create streaming features using Dataflow.\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "- [BigQuery](https://cloud.google.com/bigquery/)\n",
    "- [Vertex AI Feature Store](https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "- Build customer and terminal-related features\n",
    "- Create Feature store, entities and features\n",
    "- Ingest feature values in Feature store from BigQuery table\n",
    "- Read features from the feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "811f5d41ab7f",
    "tags": []
   },
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d31d1487ac34",
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Union\n",
    "\n",
    "# Data Engineering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "\n",
    "# Vertex AI and Vertex AI Feature Store\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Moq4QZKjY4fv"
   },
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-4MU7kF3t4x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the date range of transactions for feature engineering (last 10 days up until yesterday)\n",
    "YESTERDAY = datetime.today() - timedelta(days=1)\n",
    "YEAR_MONTH_PREFIX = YESTERDAY.strftime(\"%Y-%m\")\n",
    "DATAPROCESSING_START_DATE = (YESTERDAY - timedelta(days=10)).strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ")\n",
    "DATAPROCESSING_END_DATE = YESTERDAY.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define BiqQuery dataset and tables to calculate features.\n",
    "RAW_BQ_TRANSACTION_TABLE_URI = f\"{PROJECT_ID}.tx.tx\"\n",
    "\n",
    "INGESTION_BQ_TRANSACTION_TABLE_URI = f\"{PROJECT_ID}.tx.ingestion_tx_records\"\n",
    "INGESTION_BQ_LABELS_TABLE_URI = f\"{PROJECT_ID}.tx.ingestion_tx_labels\"\n",
    "\n",
    "RAW_BQ_LABELS_TABLE_URI = f\"{PROJECT_ID}.tx.txlabels\"\n",
    "FEATURES_BQ_TABLE_URI = f\"{PROJECT_ID}.tx.wide_features_table\"\n",
    "\n",
    "# Define Vertex AI Feature store tables and views.\n",
    "\n",
    "CUSTOMERS_FE_BQ_VIEW_URI = f\"{PROJECT_ID}.tx.v_customers_features\"\n",
    "CUSTOMERS_FE_BQ_BATCH_TABLE_URI = f\"{PROJECT_ID}.tx.t_customers_batch_features\"\n",
    "\n",
    "TERMINALS_TABLE_NAME = f\"terminals_{DATAPROCESSING_END_DATE.replace('-', '')}\"\n",
    "\n",
    "TERMINALS_FE_BQ_VIEW_URI = f\"{PROJECT_ID}.tx.v_terminals_features\"\n",
    "TERMINALS_FE_BQ_BATCH_TABLE_URI = f\"{PROJECT_ID}.tx.t_terminals_batch_features\"\n",
    "\n",
    "CUSTOMERS_STREAMING_FE_TABLE_URI = (\n",
    "    f\"{PROJECT_ID}.tx.t_customers_streaming_features\"\n",
    ")\n",
    "TERMINALS_STREAMING_FE_TABLE_URI = (\n",
    "    f\"{PROJECT_ID}.tx.t_terminals_streaming_features\"\n",
    ")\n",
    "\n",
    "ONLINE_STORAGE_NODES = 1\n",
    "FEATURE_TIME = \"feature_ts\"\n",
    "CUSTOMER_ENTITY_ID = \"customer\"\n",
    "TERMINAL_ENTITY_ID = \"terminal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bedfb5ba7f57"
   },
   "source": [
    "### Helpers\n",
    "\n",
    "Define a set of helper functions to run BigQuery query and create features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b81dbbaa636",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_bq_query(sql: str, show=False) -> Union[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "    Args:\n",
    "        sql: SQL query, as a string, to execute in BigQuery\n",
    "        show: A flag to show query result in a Pandas Dataframe\n",
    "    Returns:\n",
    "        df: DataFrame of results from query,  or error, if any\n",
    "    \"\"\"\n",
    "\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # If dry run succeeds without errors, proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    job_id = client_result.job_id\n",
    "\n",
    "    # Wait for query/job to finish running. then get & return data frame\n",
    "    result = client_result.result()\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "\n",
    "    if show:\n",
    "        df = result.to_arrow().to_pandas()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating Destination Tables for the Ingestion Pipeline\n",
    "\n",
    "Before we start engineering features, we need a place to store our raw transaction data. In this section, we'll create two BigQuery tables to serve as the destination for our ingestion pipeline. These tables will store the raw transaction records and their corresponding labels (i.e., whether a transaction is fraudulent or not). This separation of raw data from engineered features is a good practice that helps with data organization and reusability.\n",
    "\n",
    "### Creating a Table for Raw Transaction Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_ingestion_tx_records_table = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{INGESTION_BQ_TRANSACTION_TABLE_URI}`\n",
    "(\n",
    "  TX_ID STRING OPTIONS(description=\"Unique transaction identifier\"),\n",
    "  TX_TS TIMESTAMP OPTIONS(description=\"Timestamp of the transaction\"),\n",
    "  CUSTOMER_ID STRING OPTIONS(description=\"Unique customer identifier\"),\n",
    "  TERMINAL_ID STRING OPTIONS(description=\"Unique terminal identifier\"),\n",
    "  TX_AMOUNT FLOAT64 OPTIONS(description=\"The monetary value of the transaction\")\n",
    ")\n",
    "PARTITION BY\n",
    "  DATE(TX_TS)\n",
    "CLUSTER BY\n",
    "  CUSTOMER_ID\n",
    "OPTIONS (\n",
    "  description = \"A table to store customer transaction data, partitioned by day and clustered by customer.\"\n",
    ")\"\"\"\n",
    "print(create_ingestion_tx_records_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(create_ingestion_tx_records_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Table for input labels records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_ingestion_tx_labels_table = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{INGESTION_BQ_LABELS_TABLE_URI}`\n",
    "(\n",
    "  TX_ID STRING OPTIONS(description=\"Unique transaction identifier\"),\n",
    "  TX_FRAUD INT64 OPTIONS(description=\"The label for the transaction, 1-fraud/ 0-not a fraud\")\n",
    ")\n",
    "OPTIONS (\n",
    "  description = \"A table to store fraud labels for transaction data\"\n",
    ")\"\"\"\n",
    "print(create_ingestion_tx_labels_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(create_ingestion_tx_labels_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKaROgJZ3t4y"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NDaDFyj3t4z"
   },
   "source": [
    "### Batch Feature Engineering with BigQuery\n",
    "\n",
    "Now it's time to dive into the core of this notebook: **batch feature engineering**. In this section, we'll use the power of SQL and BigQuery to create insightful features from our historical transaction data. These features will capture patterns in customer behavior and terminal activity that can help our machine learning model distinguish between legitimate and fraudulent transactions.\n",
    "\n",
    "We'll be creating two main types of features:\n",
    "\n",
    "**1. Customer-Related Features:** These features will focus on the spending habits of individual customers. By analyzing their transaction history over different time windows (e.g., the last 1, 7, and 14 days), we can identify unusual patterns that might indicate fraud. For example, a sudden spike in the number of transactions or the average transaction amount could be a red flag.\n",
    "\n",
    "**2. Terminal-Related Features:** These features will assess the risk associated with different transaction terminals. Some terminals might be more susceptible to fraud than others. By analyzing the history of fraudulent transactions at each terminal, we can create a risk score that can be used as a powerful feature in our model.\n",
    "\n",
    "To create these features, we'll be using SQL window functions in BigQuery. These functions allow us to perform calculations across a set of table rows that are somehow related to the current row. This is perfect for our use case, as it allows us to easily calculate aggregated statistics (e.g., the average transaction amount) over different time windows.\n",
    "\n",
    "Let's get started! In the following cells, we'll walk you through the process of creating these features step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43265bd3002f"
   },
   "source": [
    "#### Creating Batch Features with SQL\n",
    "\n",
    "Now, let's get our hands dirty and write some SQL! In the following cells, we'll define the queries to create our customer and terminal-related features. We'll be using Common Table Expressions (CTEs) to make our queries more readable and organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the Date Range for Feature Engineering**\n",
    "\n",
    "First, let's define the time window for which we want to create features. We'll use the last 10 days of transaction data as our source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "DATAPROCESSING_START_DATE: {DATAPROCESSING_START_DATE}\n",
    "DATAPROCESSING_END_DATE: {DATAPROCESSING_END_DATE}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append historical records to the input tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For transactions table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insert_transactions_historical_data = f\"\"\"INSERT INTO `{INGESTION_BQ_TRANSACTION_TABLE_URI}`\n",
    " (TX_ID,\n",
    "  TX_TS,\n",
    "  CUSTOMER_ID,\n",
    "  TERMINAL_ID,\n",
    "  TX_AMOUNT)\n",
    "SELECT\n",
    "  TX_ID,\n",
    "  TX_TS,\n",
    "  CUSTOMER_ID,\n",
    "  TERMINAL_ID,\n",
    "  TX_AMOUNT\n",
    "FROM `{RAW_BQ_TRANSACTION_TABLE_URI}`\n",
    "WHERE TX_TS BETWEEN TIMESTAMP_SUB(current_timestamp(), INTERVAL 15 DAY) AND current_timestamp()\n",
    "\"\"\"\n",
    "print(insert_transactions_historical_data)\n",
    "run_bq_query(insert_transactions_historical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for labels table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For labels table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insert_labels_historical_data = f\"\"\"INSERT INTO `{INGESTION_BQ_LABELS_TABLE_URI}`\n",
    " (TX_ID,\n",
    "  TX_FRAUD)\n",
    "  SELECT\n",
    "    raw_tx.TX_ID,\n",
    "    raw_lb.TX_FRAUD\n",
    "  FROM\n",
    "      `{INGESTION_BQ_TRANSACTION_TABLE_URI}` as raw_tx\n",
    "  INNER JOIN \n",
    "    `{RAW_BQ_LABELS_TABLE_URI}` as raw_lb\n",
    "  ON raw_tx.TX_ID = raw_lb.TX_ID\n",
    "\"\"\"\n",
    "print(insert_labels_historical_data)\n",
    "run_bq_query(insert_labels_historical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch Table\n",
    "\n",
    "#### Terminal feature table\n",
    "\n",
    "Customer table SQL query string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_customer_batch_features_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{CUSTOMERS_FE_BQ_BATCH_TABLE_URI}` AS\n",
    "WITH\n",
    "  -- CTE 1: Select raw transaction data from the source table\n",
    "  get_raw_table AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    TX_AMOUNT\n",
    "  FROM `{INGESTION_BQ_TRANSACTION_TABLE_URI}`),\n",
    "\n",
    "  -- CTE 2: Calculate customer spending behavior using window functions\n",
    "  get_customer_spending_behaviour AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    TX_AMOUNT,\n",
    "    \n",
    "    -- Calculate the number of transactions for each customer over 1, 7, and 14-day windows\n",
    "    COUNT(TX_ID) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 86400 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_1DAY_WINDOW,\n",
    "    COUNT(TX_ID) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_7DAY_WINDOW,\n",
    "    COUNT(TX_ID) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_14DAY_WINDOW,\n",
    "      \n",
    "    -- Calculate the average transaction amount for each customer over 1, 7, and 14-day windows\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 86400 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW,\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW,\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_14DAY_WINDOW\n",
    "  FROM get_raw_table)\n",
    "\n",
    "-- Final SELECT statement: Create the customer features table\n",
    "SELECT\n",
    "  PARSE_TIMESTAMP(\"%Y-%m-%d %H:%M:%S\", FORMAT_TIMESTAMP(\"%Y-%m-%d %H:%M:%S\", TX_TS, \"UTC\")) as feature_timestamp,\n",
    "  CUSTOMER_ID AS entity_id,\n",
    "  CAST(CUSTOMER_ID_NB_TX_1DAY_WINDOW AS INT64) AS customer_id_nb_tx_1day_window,\n",
    "  CAST(CUSTOMER_ID_NB_TX_7DAY_WINDOW AS INT64) AS customer_id_nb_tx_7day_window,\n",
    "  CAST(CUSTOMER_ID_NB_TX_14DAY_WINDOW AS INT64) AS customer_id_nb_tx_14day_window,\n",
    "  CAST(CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_1day_window,\n",
    "  CAST(CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_7day_window,\n",
    "  CAST(CUSTOMER_ID_AVG_AMOUNT_14DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_14day_window\n",
    "FROM\n",
    "  get_customer_spending_behaviour\n",
    "\"\"\"\n",
    "print(create_customer_batch_features_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Run the query \n",
    "\n",
    "You create the initial customer features table based on provided historical records snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(create_customer_batch_features_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Inspect the result \n",
    "\n",
    "You can query some data rows to validate the result of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(\n",
    "    f\"SELECT * FROM `{CUSTOMERS_FE_BQ_BATCH_TABLE_URI}` LIMIT 10\", show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Terminal feature table\n",
    "\n",
    "Terminal table SQL query string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_terminal_batch_features_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{TERMINALS_FE_BQ_BATCH_TABLE_URI}` AS\n",
    "WITH\n",
    "  -- CTE 1: Join transaction data with fraud labels\n",
    "  get_raw_table AS (\n",
    "  SELECT\n",
    "    raw_tx.TX_TS,\n",
    "    raw_tx.TX_ID,\n",
    "    raw_tx.CUSTOMER_ID,\n",
    "    raw_tx.TERMINAL_ID,\n",
    "    raw_tx.TX_AMOUNT,\n",
    "    raw_lb.TX_FRAUD\n",
    "  FROM `{INGESTION_BQ_TRANSACTION_TABLE_URI}` raw_tx\n",
    "  LEFT JOIN \n",
    "    `{INGESTION_BQ_LABELS_TABLE_URI}` as raw_lb\n",
    "  ON raw_tx.TX_ID = raw_lb.TX_ID),\n",
    "\n",
    "  -- CTE 2: Calculate delayed window variables for terminal risk assessment\n",
    "  get_variables_delay_window AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    \n",
    "    -- Calculate the number of fraudulent transactions and total transactions over a 7-day delay period\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_DELAY,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_DELAY,\n",
    "      \n",
    "    -- Calculate the number of fraudulent transactions and total transactions over 1, 7, and 14-day delayed windows\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 691200 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_1_DELAY_WINDOW,\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_7_DELAY_WINDOW,\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1814400 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_14_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 691200 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_1_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_7_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1814400 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_14_DELAY_WINDOW\n",
    "  FROM get_raw_table),\n",
    "\n",
    "  -- CTE 3: Calculate terminal risk factors\n",
    "  get_risk_factors AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    -- Calculate the number of fraudulent transactions for each terminal over 1, 7, and 14-day windows\n",
    "    NB_FRAUD_1_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_1DAY_WINDOW,\n",
    "    NB_FRAUD_7_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_7DAY_WINDOW,\n",
    "    NB_FRAUD_14_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_14DAY_WINDOW,\n",
    "    -- Calculate the total number of transactions for each terminal over 1, 7, and 14-day windows\n",
    "    NB_TX_1_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_1DAY_WINDOW,\n",
    "    NB_TX_7_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_7DAY_WINDOW,\n",
    "    NB_TX_14_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_14DAY_WINDOW\n",
    "      FROM\n",
    "    get_variables_delay_window),\n",
    "\n",
    "  -- CTE 4: Calculate the terminal risk index\n",
    "  get_risk_index AS (\n",
    "    SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    TERMINAL_ID_NB_TX_1DAY_WINDOW,\n",
    "    TERMINAL_ID_NB_TX_7DAY_WINDOW,\n",
    "    TERMINAL_ID_NB_TX_14DAY_WINDOW,\n",
    "    -- Calculate the risk index for each terminal over 1, 7, and 14-day windows\n",
    "    (TERMINAL_ID_NB_FRAUD_1DAY_WINDOW/(TERMINAL_ID_NB_TX_1DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_1DAY_WINDOW,\n",
    "    (TERMINAL_ID_NB_FRAUD_7DAY_WINDOW/(TERMINAL_ID_NB_TX_7DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_7DAY_WINDOW,\n",
    "    (TERMINAL_ID_NB_FRAUD_14DAY_WINDOW/(TERMINAL_ID_NB_TX_14DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_14DAY_WINDOW\n",
    "    FROM get_risk_factors \n",
    "  )\n",
    "\n",
    "-- Final SELECT statement: Create the terminal features table\n",
    "SELECT\n",
    "  PARSE_TIMESTAMP(\"%Y-%m-%d %H:%M:%S\", FORMAT_TIMESTAMP(\"%Y-%m-%d %H:%M:%S\", TX_TS, \"UTC\")) as feature_timestamp,\n",
    "  TERMINAL_ID AS entity_id,\n",
    "  CAST(TERMINAL_ID_NB_TX_1DAY_WINDOW AS INT64) AS terminal_id_nb_tx_1day_window,\n",
    "  CAST(TERMINAL_ID_NB_TX_7DAY_WINDOW AS INT64) AS terminal_id_nb_tx_7day_window,\n",
    "  CAST(TERMINAL_ID_NB_TX_14DAY_WINDOW AS INT64) AS terminal_id_nb_tx_14day_window,\n",
    "  CAST(TERMINAL_ID_RISK_1DAY_WINDOW AS FLOAT64) AS terminal_id_risk_1day_window,\n",
    "  CAST(TERMINAL_ID_RISK_7DAY_WINDOW AS FLOAT64) AS terminal_id_risk_7day_window,\n",
    "  CAST(TERMINAL_ID_RISK_14DAY_WINDOW AS FLOAT64) AS terminal_id_risk_14day_window\n",
    "FROM\n",
    "  get_risk_index\n",
    "\"\"\"\n",
    "print(create_terminal_batch_features_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the query \n",
    "\n",
    "You create the customer features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(create_terminal_batch_features_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect the result \n",
    "\n",
    "You can query some data rows to validate the result of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(\n",
    "    f\"SELECT * FROM `{TERMINALS_FE_BQ_BATCH_TABLE_URI}` LIMIT 10\", show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating BigQuery Views for Feature Tables\n",
    "\n",
    "Now that we have created the initial batch feature tables, we will create BigQuery views on top of them. A **BigQuery view** is a virtual table defined by a SQL query. It allows you to encapsulate the logic for generating features and provides a simplified, consistent interface for querying them. Views do not store any data themselves; instead, they run the underlying query every time they are accessed, ensuring that you always get the latest data.\n",
    "\n",
    "In this notebook, we will create two views:\n",
    "- `v_customers_features`: A view for the customer-related batch features.\n",
    "- `v_terminals_features`: A view for the terminal-related batch features.\n",
    "\n",
    "These views will be used as the source for our Vertex AI Feature Store, and they will also be used to create our final training dataset.\n",
    "\n",
    "#### Customer feature \n",
    "\n",
    "This view will provide a real-time look at customer spending behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43Ck_vAc3t4z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_customer_view_query = f\"\"\"\n",
    "CREATE OR REPLACE VIEW `{CUSTOMERS_FE_BQ_VIEW_URI}` AS\n",
    "WITH\n",
    "  -- query to join labels with features -------------------------------------------------------------------------------------------\n",
    "  get_raw_table AS (\n",
    "  SELECT\n",
    "    raw_tx.TX_TS,\n",
    "    raw_tx.TX_ID,\n",
    "    raw_tx.CUSTOMER_ID,\n",
    "    raw_tx.TERMINAL_ID,\n",
    "    raw_tx.TX_AMOUNT\n",
    "  FROM (\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      `{INGESTION_BQ_TRANSACTION_TABLE_URI}`\n",
    "    WHERE\n",
    "      TX_TS BETWEEN TIMESTAMP_SUB(current_timestamp(), INTERVAL 15 DAY) AND current_timestamp()\n",
    "    ) raw_tx),\n",
    "\n",
    "  -- query to calculate CUSTOMER spending behaviour --------------------------------------------------------------------------------\n",
    "  get_customer_spending_behaviour AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    TX_AMOUNT,\n",
    "    \n",
    "    # calc the number of customer tx over daily windows per customer (1, 7 and 15 days, expressed in seconds)\n",
    "    COUNT(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 86400 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_1DAY_WINDOW,\n",
    "    COUNT(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_7DAY_WINDOW,\n",
    "    COUNT(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_14DAY_WINDOW,\n",
    "      \n",
    "    # calc the customer average tx amount over daily windows per customer (1, 7 and 15 days, expressed in seconds, in dollars ($))\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 86400 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW,\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW,\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_14DAY_WINDOW,\n",
    "  FROM get_raw_table)\n",
    "\n",
    "# Create the table with CUSTOMER  features ----------------------------------------------------------------------------\n",
    "SELECT\n",
    "  current_timestamp() as feature_timestamp,\n",
    "  CUSTOMER_ID AS entity_id,\n",
    "  CAST(CUSTOMER_ID_NB_TX_1DAY_WINDOW AS INT64) AS customer_id_nb_tx_1day_window,\n",
    "  CAST(CUSTOMER_ID_NB_TX_7DAY_WINDOW AS INT64) AS customer_id_nb_tx_7day_window,\n",
    "  CAST(CUSTOMER_ID_NB_TX_14DAY_WINDOW AS INT64) AS customer_id_nb_tx_14day_window,\n",
    "  CAST(CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_1day_window,\n",
    "  CAST(CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_7day_window,\n",
    "  CAST(CUSTOMER_ID_AVG_AMOUNT_14DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_14day_window,\n",
    "FROM\n",
    "  get_customer_spending_behaviour\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(create_customer_view_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the query \n",
    "\n",
    "You create the customer features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(create_customer_view_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect the result \n",
    "\n",
    "You can query some data rows to validate the result of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(f\"SELECT * FROM `{CUSTOMERS_FE_BQ_VIEW_URI}` LIMIT 10\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminal feature table\n",
    "\n",
    "Terminal table SQL query string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43Ck_vAc3t4z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_terminal_view_query = f\"\"\"\n",
    "# query to calculate TERMINAL spending behaviour --------------------------------------------------------------------------------\n",
    "CREATE OR REPLACE VIEW `{TERMINALS_FE_BQ_VIEW_URI}` AS\n",
    "WITH\n",
    "  -- query to join labels with features -------------------------------------------------------------------------------------------\n",
    "  get_raw_table AS (\n",
    "  SELECT\n",
    "    raw_tx.TX_TS,\n",
    "    raw_tx.TX_ID,\n",
    "    raw_tx.CUSTOMER_ID,\n",
    "    raw_tx.TERMINAL_ID,\n",
    "    raw_tx.TX_AMOUNT,\n",
    "    raw_lb.TX_FRAUD\n",
    "  FROM (\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      `{INGESTION_BQ_TRANSACTION_TABLE_URI}`\n",
    "    WHERE\n",
    "      TX_TS BETWEEN TIMESTAMP_SUB(current_timestamp(), INTERVAL 15 DAY) AND current_timestamp()\n",
    "    ) raw_tx\n",
    "  LEFT JOIN \n",
    "    `{INGESTION_BQ_LABELS_TABLE_URI}` as raw_lb\n",
    "  ON raw_tx.TX_ID = raw_lb.TX_ID),\n",
    "\n",
    "  # query to calculate TERMINAL spending behaviour --------------------------------------------------------------------------------\n",
    "  get_variables_delay_window AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    \n",
    "    # calc total amount of fraudulent tx and the total number of tx over the delay period per terminal (7 days - delay, expressed in seconds)\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_DELAY,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_DELAY,\n",
    "      \n",
    "    # calc total amount of fraudulent tx and the total number of tx over the delayed window per terminal (window + 7 days - delay, expressed in seconds)\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 691200 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_1_DELAY_WINDOW,\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_7_DELAY_WINDOW,\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1814400 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_14_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 691200 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_1_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_7_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1814400 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_14_DELAY_WINDOW,\n",
    "  FROM get_raw_table),\n",
    "\n",
    "  # query to calculate TERMINAL risk factors ---------------------------------------------------------------------------------------\n",
    "  get_risk_factors AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    # calculate numerator of risk index\n",
    "    NB_FRAUD_1_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_1DAY_WINDOW,\n",
    "    NB_FRAUD_7_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_7DAY_WINDOW,\n",
    "    NB_FRAUD_14_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_14DAY_WINDOW,\n",
    "    # calculate denominator of risk index\n",
    "    NB_TX_1_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_1DAY_WINDOW,\n",
    "    NB_TX_7_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_7DAY_WINDOW,\n",
    "    NB_TX_14_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_14DAY_WINDOW,\n",
    "      FROM\n",
    "    get_variables_delay_window),\n",
    "\n",
    "  # query to calculate the TERMINAL risk index -------------------------------------------------------------------------------------\n",
    "  get_risk_index AS (\n",
    "    SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    TERMINAL_ID_NB_TX_1DAY_WINDOW,\n",
    "    TERMINAL_ID_NB_TX_7DAY_WINDOW,\n",
    "    TERMINAL_ID_NB_TX_14DAY_WINDOW,\n",
    "    # calculate the risk index\n",
    "    (TERMINAL_ID_NB_FRAUD_1DAY_WINDOW/(TERMINAL_ID_NB_TX_1DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_1DAY_WINDOW,\n",
    "    (TERMINAL_ID_NB_FRAUD_7DAY_WINDOW/(TERMINAL_ID_NB_TX_7DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_7DAY_WINDOW,\n",
    "    (TERMINAL_ID_NB_FRAUD_14DAY_WINDOW/(TERMINAL_ID_NB_TX_14DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_14DAY_WINDOW\n",
    "    FROM get_risk_factors \n",
    "  )\n",
    "\n",
    "# Create the table with CUSTOMER and TERMINAL features ----------------------------------------------------------------------------\n",
    "SELECT\n",
    "  current_timestamp() as feature_timestamp,\n",
    "  # TERMINAL_ID AS terminal_id,\n",
    "  TERMINAL_ID AS entity_id,\n",
    "  CAST(TERMINAL_ID_NB_TX_1DAY_WINDOW AS INT64) AS terminal_id_nb_tx_1day_window,\n",
    "  CAST(TERMINAL_ID_NB_TX_7DAY_WINDOW AS INT64) AS terminal_id_nb_tx_7day_window,\n",
    "  CAST(TERMINAL_ID_NB_TX_14DAY_WINDOW AS INT64) AS terminal_id_nb_tx_14day_window,\n",
    "  CAST(TERMINAL_ID_RISK_1DAY_WINDOW AS FLOAT64) AS terminal_id_risk_1day_window,\n",
    "  CAST(TERMINAL_ID_RISK_7DAY_WINDOW AS FLOAT64) AS terminal_id_risk_7day_window,\n",
    "  CAST(TERMINAL_ID_RISK_14DAY_WINDOW AS FLOAT64) AS terminal_id_risk_14day_window,\n",
    "FROM\n",
    "  get_risk_index\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(create_terminal_view_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the query \n",
    "\n",
    "You create the customer features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(create_terminal_view_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect the result \n",
    "\n",
    "You can query some data rows to validate the result of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(f\"SELECT * FROM `{TERMINALS_FE_BQ_VIEW_URI}` LIMIT 10\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating Feature Updates with BigQuery Scheduled Queries\n",
    "\n",
    "To ensure our batch features remain up-to-date, we need a way to periodically refresh them with the latest transaction data. Manually re-running our feature generation queries would be inefficient and error-prone. Instead, we can automate this process using **BigQuery scheduled queries**.\n",
    "\n",
    "A scheduled query is a query that runs automatically on a recurring basis. In our case, we'll set up scheduled queries that run every 15 minutes. Each time they run, they will:\n",
    "\n",
    "1.  Execute the query within our `v_customers_features` and `v_terminals_features` views to calculate the latest feature values.\n",
    "2.  Append these new feature values to our batch feature tables (`t_customers_batch_features` and `t_terminals_batch_features`).\n",
    "\n",
    "This ensures that our feature store always has access to fresh batch features, which is crucial for making accurate, timely fraud predictions. We'll use the `bq mk --transfer_config` command to create these scheduled queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo \"{CUSTOMERS_FE_BQ_VIEW_URI}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!bq mk --transfer_config \\\n",
    "--data_source='scheduled_query' \\\n",
    "--display_name='Append Customers Features to Batch Features Table' \\\n",
    "--target_dataset='tx' \\\n",
    "--schedule='every 15 mins' \\\n",
    "--params='{\"query\": \"INSERT INTO `{CUSTOMERS_FE_BQ_BATCH_TABLE_URI}` SELECT * FROM `{CUSTOMERS_FE_BQ_VIEW_URI}`;\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!bq mk --transfer_config \\\n",
    "--data_source='scheduled_query' \\\n",
    "--display_name='Append Terminal Features to Batch Features Table' \\\n",
    "--target_dataset='tx' \\\n",
    "--schedule='every 15 mins' \\\n",
    "--params='{\"query\": \"INSERT INTO `{TERMINALS_FE_BQ_BATCH_TABLE_URI}` SELECT * FROM `{TERMINALS_FE_BQ_VIEW_URI}`;\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "612e2fa65748"
   },
   "source": [
    "### Initializing Tables for Real-Time (Streaming) Features\n",
    "\n",
    "While this notebook focuses on batch features, our end-to-end fraud detection system will also use real-time features calculated over very short time windows (e.g., the last 15, 30, and 60 minutes). These features will be generated by a streaming pipeline using Dataflow, which is covered in the next notebook (`03_feature_engineering_streaming_new_fs.ipynb`).\n",
    "\n",
    "However, we need to create the destination tables for these features *now*. The following queries will create empty tables in BigQuery with the correct schema for our future streaming features. This step is important because:\n",
    "\n",
    "1.  **It defines the contract:** It establishes the schema that the streaming pipeline will write to.\n",
    "2.  **It enables the training view:** It allows our final training dataset view (`v_ff_training_dataset`) to be created successfully, as it can reference these tables even though they are currently empty. The `IFNULL` function in the view will handle the absence of data, ensuring the query doesn't fail.\n",
    "\n",
    "These tables will be populated with data once we run the Dataflow streaming pipeline in the next lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer feature table\n",
    "\n",
    "Customer table SQL query string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1aed0001232",
    "tags": []
   },
   "outputs": [],
   "source": [
    "initiate_real_time_customer_features_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{CUSTOMERS_STREAMING_FE_TABLE_URI}`\n",
    "(\n",
    "    entity_id STRING,\n",
    "    feature_timestamp TIMESTAMP,\n",
    "    customer_id_nb_tx_15min_window INT64,\n",
    "    customer_id_nb_tx_30min_window INT64,\n",
    "    customer_id_nb_tx_60min_window INT64,\n",
    "    customer_id_avg_amount_15min_window FLOAT64,\n",
    "    customer_id_avg_amount_30min_window FLOAT64,\n",
    "    customer_id_avg_amount_60min_window FLOAT64\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initiate_real_time_terminal_features_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{TERMINALS_STREAMING_FE_TABLE_URI}`\n",
    "(\n",
    "    entity_id STRING,\n",
    "    feature_timestamp TIMESTAMP,\n",
    "    terminal_id_nb_tx_15min_window INT64,\n",
    "    terminal_id_nb_tx_30min_window INT64,\n",
    "    terminal_id_nb_tx_60min_window INT64,\n",
    "    terminal_id_avg_amount_15min_window FLOAT64,\n",
    "    terminal_id_avg_amount_30min_window FLOAT64,\n",
    "    terminal_id_avg_amount_60min_window FLOAT64\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d35d1e69d72"
   },
   "source": [
    "#### Run the query above to initialize the real-time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aae355c66e4a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for query in [\n",
    "    initiate_real_time_customer_features_query,\n",
    "    initiate_real_time_terminal_features_query,\n",
    "]:\n",
    "    run_bq_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76733185d022"
   },
   "source": [
    "#### Inspect BigQuery features tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "774a90225747",
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(\n",
    "    f\"SELECT * FROM `{CUSTOMERS_STREAMING_FE_TABLE_URI}` LIMIT 5\", show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(\n",
    "    f\"SELECT * FROM `{TERMINALS_STREAMING_FE_TABLE_URI}` LIMIT 5\", show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the final schema of the features table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Final Training Dataset View\n",
    "\n",
    "With our batch and streaming feature tables in place, we can now create a final BigQuery view that will serve as the source for our model training. This view, `v_ff_training_dataset`, will join the raw transaction data with the corresponding feature values from our batch and streaming tables.\n",
    "\n",
    "A key challenge when creating a training dataset is ensuring that you are not introducing **data leakage**. Data leakage occurs when your training data contains information that would not be available at the time of prediction. For example, if we were to simply join our transaction data with the latest feature values, we would be leaking information from the future into the past.\n",
    "\n",
    "To prevent this, we will use the `ML.ENTITY_FEATURES_AT_TIME` function in BigQuery. This function allows us to perform a **point-in-time lookup**, which means that for each transaction, we will retrieve the feature values that were valid at the time the transaction occurred. This ensures that our model is trained on the same data that it will see in a real-world prediction scenario, which is crucial for building a robust and accurate fraud detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_customers_features_table = f\"{PROJECT}.tx.t_customers_batch_features\"\n",
    "batch_terminals_features_table = f\"{PROJECT}.tx.t_terminals_batch_features\"\n",
    "\n",
    "stream_customers_features_table = f\"{PROJECT}.tx.t_customers_streaming_features\"\n",
    "stream_terminals_features_table = f\"{PROJECT}.tx.t_terminals_streaming_features\"\n",
    "\n",
    "train_dataset_view_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW tx.v_ff_training_dataset AS\n",
    "    WITH\n",
    "      # -------------------------------------------\n",
    "      # Using raw transaction table as a base table\n",
    "      # filtered by specific time range\n",
    "      # joined with labels data\n",
    "      raw_tx_labled_range_table AS (\n",
    "      SELECT\n",
    "        raw_tx.TX_TS AS tx_timestamp,\n",
    "        raw_tx.CUSTOMER_ID AS customer_id,\n",
    "        raw_tx.TERMINAL_ID AS terminal_id,\n",
    "        raw_tx.TX_AMOUNT AS tx_amount,\n",
    "        raw_lb.TX_FRAUD AS tx_fraud,\n",
    "      FROM\n",
    "        `{INGESTION_BQ_TRANSACTION_TABLE_URI}` AS raw_tx\n",
    "      LEFT JOIN\n",
    "        `{INGESTION_BQ_LABELS_TABLE_URI}` AS raw_lb\n",
    "      ON\n",
    "        raw_tx.TX_ID = raw_lb.TX_ID\n",
    "      WHERE\n",
    "        raw_tx.TX_TS BETWEEN TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY) AND CURRENT_TIMESTAMP()),\n",
    "      # ---------------------------------------------\n",
    "      # Using base transaction table\n",
    "      # to create a entity_id with timestamp pairs\n",
    "      # to lookup coresponding feautres for terminals\n",
    "      terminals_time_table AS (\n",
    "      SELECT\n",
    "        raw_tx_labled_range_table.tx_timestamp AS `time`,\n",
    "        raw_tx_labled_range_table.TERMINAL_ID AS entity_id,\n",
    "      FROM\n",
    "        raw_tx_labled_range_table),\n",
    "      # ---------------------------------------------\n",
    "      # Using base transaction table\n",
    "      # to create a entity_id with timestamp pairs\n",
    "      # to lookup coresponding feautres for customers\n",
    "      customers_time_table AS (\n",
    "      SELECT\n",
    "        raw_tx_labled_range_table.tx_timestamp AS `time`,\n",
    "        raw_tx_labled_range_table.CUSTOMER_ID AS entity_id,\n",
    "      FROM\n",
    "        raw_tx_labled_range_table)\n",
    "\n",
    "    SELECT\n",
    "    # Features from raw transaction:\n",
    "    raw_tx_labled_range_table.tx_amount,\n",
    "    raw_tx_labled_range_table.tx_fraud,\n",
    "    raw_tx_labled_range_table.tx_timestamp AS `timestamp`,\n",
    "\n",
    "    # Features from customers batch pipeline:\n",
    "    IFNULL(f_customers.customer_id_avg_amount_1day_window, 0.0) as customer_id_avg_amount_1day_window,\n",
    "    IFNULL(f_customers.customer_id_avg_amount_7day_window, 0.0) as customer_id_avg_amount_7day_window,\n",
    "    IFNULL(f_customers.customer_id_avg_amount_14day_window, 0.0) as customer_id_avg_amount_14day_window,\n",
    "    IFNULL(CAST(f_customers.customer_id_nb_tx_1day_window AS FLOAT64), 0.0) as customer_id_nb_tx_1day_window,\n",
    "    IFNULL(CAST(f_customers.customer_id_nb_tx_7day_window AS FLOAT64), 0.0) as customer_id_nb_tx_7day_window,\n",
    "    IFNULL(CAST(f_customers.customer_id_nb_tx_14day_window AS FLOAT64), 0.0) as customer_id_nb_tx_14day_window,\n",
    "\n",
    "    # Features from terminals batch pipeline:\n",
    "    IFNULL(f_terminals.terminal_id_risk_1day_window, 0.0)  AS terminal_id_risk_1day_window,\n",
    "    IFNULL(f_terminals.terminal_id_risk_7day_window, 0.0)  AS terminal_id_risk_7day_window,\n",
    "    IFNULL(f_terminals.terminal_id_risk_14day_window, 0.0)  AS terminal_id_risk_14day_window,\n",
    "    IFNULL(CAST(f_terminals.terminal_id_nb_tx_1day_window AS FLOAT64), 0.0)  AS terminal_id_nb_tx_1day_window,\n",
    "    IFNULL(CAST(f_terminals.terminal_id_nb_tx_7day_window AS FLOAT64), 0.0)  AS terminal_id_nb_tx_7day_window,\n",
    "    IFNULL(CAST(f_terminals.terminal_id_nb_tx_14day_window AS FLOAT64), 0.0)  AS terminal_id_nb_tx_14day_window,\n",
    "  \n",
    "    # Features from customers streaming pipeline:\n",
    "    IFNULL(f_customers_stream.customer_id_avg_amount_15min_window, 0.0) as customer_id_avg_amount_15min_window,\n",
    "    IFNULL(f_customers_stream.customer_id_avg_amount_30min_window, 0.0) as customer_id_avg_amount_30min_window,\n",
    "    IFNULL(f_customers_stream.customer_id_avg_amount_60min_window, 0.0) as customer_id_avg_amount_60min_window,\n",
    "    IFNULL(CAST(f_customers_stream.customer_id_nb_tx_15min_window AS FLOAT64), 0.0)  AS customer_id_nb_tx_15min_window,\n",
    "    IFNULL(CAST(f_customers_stream.customer_id_nb_tx_30min_window AS FLOAT64), 0.0)  AS customer_id_nb_tx_30min_window,\n",
    "    IFNULL(CAST(f_customers_stream.customer_id_nb_tx_60min_window AS FLOAT64), 0.0)  AS customer_id_nb_tx_60min_window,\n",
    "  \n",
    "    # Features from terminals streaming pipeline:\n",
    "    IFNULL(f_terminals_stream.terminal_id_avg_amount_15min_window, 0.0) as terminal_id_avg_amount_15min_window,\n",
    "    IFNULL(f_terminals_stream.terminal_id_avg_amount_30min_window, 0.0) as terminal_id_avg_amount_30min_window,\n",
    "    IFNULL(f_terminals_stream.terminal_id_avg_amount_60min_window, 0.0) as terminal_id_avg_amount_60min_window,\n",
    "    IFNULL(CAST(terminal_id_nb_tx_15min_window AS FLOAT64), 0.0)  AS terminal_id_nb_tx_15min_window,\n",
    "    IFNULL(CAST(terminal_id_nb_tx_30min_window AS FLOAT64), 0.0)  AS terminal_id_nb_tx_30min_window,\n",
    "    IFNULL(CAST(f_terminals_stream.terminal_id_nb_tx_60min_window AS FLOAT64), 0.0)  AS terminal_id_nb_tx_60min_window\n",
    "      \n",
    "    FROM\n",
    "      raw_tx_labled_range_table\n",
    "      \n",
    "    LEFT JOIN\n",
    "      ML.ENTITY_FEATURES_AT_TIME( TABLE `{batch_customers_features_table}`,\n",
    "        TABLE customers_time_table,\n",
    "        num_rows => 1,\n",
    "        ignore_feature_nulls => TRUE) AS f_customers\n",
    "    ON\n",
    "      raw_tx_labled_range_table.customer_id = f_customers.entity_id\n",
    "      AND raw_tx_labled_range_table.tx_timestamp = f_customers.feature_timestamp\n",
    "      \n",
    "    LEFT JOIN\n",
    "\n",
    "      ML.ENTITY_FEATURES_AT_TIME( TABLE `{batch_terminals_features_table}`,\n",
    "        TABLE terminals_time_table,\n",
    "        num_rows => 1,\n",
    "        ignore_feature_nulls => TRUE) AS f_terminals\n",
    "    ON\n",
    "      raw_tx_labled_range_table.terminal_id = f_terminals.entity_id\n",
    "      AND raw_tx_labled_range_table.tx_timestamp = f_terminals.feature_timestamp\n",
    "      \n",
    "    LEFT JOIN\n",
    "      ML.ENTITY_FEATURES_AT_TIME( TABLE `{stream_customers_features_table}`,\n",
    "        TABLE customers_time_table,\n",
    "        num_rows => 1,\n",
    "        ignore_feature_nulls => TRUE) AS f_customers_stream\n",
    "    ON\n",
    "      raw_tx_labled_range_table.customer_id = f_customers_stream.entity_id\n",
    "      AND raw_tx_labled_range_table.tx_timestamp = f_customers_stream.feature_timestamp\n",
    "    LEFT JOIN\n",
    "      ML.ENTITY_FEATURES_AT_TIME( TABLE `{stream_terminals_features_table}`,\n",
    "        TABLE terminals_time_table,\n",
    "        num_rows => 1,\n",
    "        ignore_feature_nulls => TRUE) AS f_terminals_stream\n",
    "    ON\n",
    "      raw_tx_labled_range_table.terminal_id = f_terminals_stream.entity_id\n",
    "      AND raw_tx_labled_range_table.tx_timestamp = f_terminals_stream.feature_timestamp\n",
    "\"\"\"\n",
    "print(train_dataset_view_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(train_dataset_view_sql, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect BigQuery training dataset view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(\"SELECT * FROM tx.v_ff_training_dataset LIMIT 5\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Initialize Vertex AI SDK\n",
    "\n",
    "Initialize the Vertex AI SDK to get access to Vertex AI services programmatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c85ab891607c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfKeQmf63t42"
   },
   "source": [
    "## Managing Features with Vertex AI Feature Store\n",
    "\n",
    "Now that we've engineered our features, we need a robust and efficient way to manage them. This is where the **Vertex AI Feature Store** comes in. A feature store is a centralized repository for storing, serving, and managing machine learning features. It plays a crucial role in the MLOps lifecycle by providing a single source of truth for features, which helps to ensure consistency between training and serving, prevent feature leakage, and promote feature reuse across different models and projects.\n",
    "\n",
    "### Key Concepts in Vertex AI Feature Store\n",
    "\n",
    "Before we start creating our feature store, let's quickly go over some key concepts:\n",
    "\n",
    "*   **Feature Store**: A top-level container for organizing and managing your features.\n",
    "*   **Entity Type**: A collection of semantically related features. In our case, we'll have two entity types: `customer` and `terminal`.\n",
    "*   **Feature**: A measurable property or characteristic of an entity. For example, `customer_id_nb_tx_1day_window` is a feature of the `customer` entity type.\n",
    "*   **Feature View**: A logical view of features from a data source. It defines how features are synced from the data source to the online store for serving.\n",
    "\n",
    "### Benefits of Using a Feature Store\n",
    "\n",
    "Using a feature store offers several advantages, including:\n",
    "\n",
    "*   **Preventing Training-Serving Skew**: By using the same feature store for both training and serving, you can ensure that your model is using the exact same features in both environments, which helps to prevent performance degradation due to inconsistencies.\n",
    "*   **Promoting Feature Reuse**: A centralized feature store makes it easy to discover and reuse existing features across different models and teams, which can save time and effort.\n",
    "*   **Improving Model Governance**: A feature store provides a centralized place to track feature lineage and metadata, which can help with model explainability and compliance.\n",
    "\n",
    "In the following cells, we'll create a feature store, define our entity types and features, and ingest our batch features from BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform_v1 import (\n",
    "    FeatureOnlineStoreAdminServiceClient,\n",
    "    FeatureOnlineStoreServiceClient,\n",
    "    FeatureRegistryServiceClient,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import feature as feature_pb2\n",
    "from google.cloud.aiplatform_v1.types import feature_group as feature_group_pb2\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    feature_online_store as feature_online_store_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    feature_online_store_admin_service as feature_online_store_admin_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    feature_online_store_service as feature_online_store_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    feature_registry_service as feature_registry_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import feature_view as feature_view_pb2\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    featurestore_service as featurestore_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import io as io_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Admin Service Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "admin_client = FeatureOnlineStoreAdminServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")\n",
    "registry_client = FeatureRegistryServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create online store instance\n",
    "\n",
    "To create an online store instance.\n",
    "Create a `FeatureOnlineStore` instance with autoscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "online_store_config = feature_online_store_pb2.FeatureOnlineStore(\n",
    "    bigtable=feature_online_store_pb2.FeatureOnlineStore.Bigtable(\n",
    "        auto_scaling=feature_online_store_pb2.FeatureOnlineStore.Bigtable.AutoScaling(\n",
    "            min_node_count=1, max_node_count=1, cpu_utilization_target=50\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "create_store_lro = admin_client.create_feature_online_store(\n",
    "    feature_online_store_admin_service_pb2.CreateFeatureOnlineStoreRequest(\n",
    "        parent=f\"projects/{PROJECT_ID}/locations/{REGION}\",\n",
    "        feature_online_store_id=FEATURESTORE_ID,\n",
    "        feature_online_store=online_store_config,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify online store instance creation\n",
    "\n",
    "After the long-running operation (LRO) is complete, show the result.\n",
    "\n",
    "> **Note:** This operation might take up to 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait for the LRO to finish and get the LRO result.\n",
    "print(create_store_lro.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use list to verify the store is created.\n",
    "admin_client.list_feature_online_stores(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Registering Feature Groups and Features\n",
    "\n",
    "Before we can use our features for online serving, we need to register their metadata with the Vertex AI Feature Registry. This involves two main concepts:\n",
    "\n",
    "*   **`FeatureGroup`**: A `FeatureGroup` is a logical container that groups features defined on the same BigQuery data source. It tells the Feature Store where your feature data is located (the `input_uri`) and which column contains the unique entity IDs.\n",
    "\n",
    "*   **`Feature`**: A `Feature` represents a single column (a feature) within a `FeatureGroup`'s data source.\n",
    "\n",
    "In our project, we have four distinct sets of features based on the entity type (customer or terminal) and the calculation method (batch or streaming). Therefore, we will create four `FeatureGroup`s to organize them, one for each of our BigQuery feature tables:\n",
    "1.  **`fraudfinder_customers_batch`**: For batch-calculated customer features.\n",
    "2.  **`fraudfinder_customers_streaming`**: For streaming-calculated customer features (currently empty).\n",
    "3.  **`fraudfinder_terminals_batch`**: For batch-calculated terminal features.\n",
    "4.  **`fraudfinder_terminals_streaming`**: For streaming-calculated terminal features (currently empty).\n",
    "\n",
    "The following cells will use our helper function to create these four `FeatureGroup`s and register all the associated `Feature`s (columns) within each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define utility method for feature groups creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_fs_feature_group(\n",
    "    bq_source_uri, entity_id_column, feature_group_id, feature_ids_list\n",
    "):\n",
    "\n",
    "    # Now, create the featureGroup\n",
    "    feature_group_config = feature_group_pb2.FeatureGroup(\n",
    "        big_query=feature_group_pb2.FeatureGroup.BigQuery(\n",
    "            big_query_source=io_pb2.BigQuerySource(\n",
    "                input_uri=f\"bq://{bq_source_uri}\"\n",
    "            ),\n",
    "            # Add the entity_id_columns parameter here\n",
    "            entity_id_columns=[entity_id_column],\n",
    "        )\n",
    "    )\n",
    "    create_group_lro = registry_client.create_feature_group(\n",
    "        feature_registry_service_pb2.CreateFeatureGroupRequest(\n",
    "            parent=f\"projects/{PROJECT_ID}/locations/{REGION}\",\n",
    "            feature_group_id=feature_group_id,\n",
    "            feature_group=feature_group_config,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # After the long-running operation (LRO) is complete, show the result.\n",
    "    print(create_group_lro.result())\n",
    "\n",
    "    create_feature_lros = []\n",
    "    for id in feature_ids_list:\n",
    "        create_feature_lros.append(\n",
    "            registry_client.create_feature(\n",
    "                featurestore_service_pb2.CreateFeatureRequest(\n",
    "                    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureGroups/{feature_group_id}\",\n",
    "                    feature_id=id,\n",
    "                    feature=feature_pb2.Feature(),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Wait for FS Group creation\n",
    "    for lro in create_feature_lros:\n",
    "        # After the long-running operation (LRO) is complete, show the result.\n",
    "        print(lro.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUSTOMER_ID_COLUMN = \"entity_id\"  # entity_id\n",
    "\n",
    "CUSTOMER_BATCH_FEATURES_GROUP_ID = \"fraudfinder_customers_batch\"\n",
    "\n",
    "CUSTOMER_BATCH_FEATURE_IDS = [\n",
    "    \"customer_id_nb_tx_14day_window\",\n",
    "    \"customer_id_avg_amount_7day_window\",\n",
    "    \"customer_id_nb_tx_1day_window\",\n",
    "    \"customer_id_avg_amount_1day_window\",\n",
    "    \"customer_id_avg_amount_14day_window\",\n",
    "    \"customer_id_nb_tx_7day_window\",\n",
    "]\n",
    "\n",
    "# Creating feature Group for batch for customers\n",
    "create_fs_feature_group(\n",
    "    bq_source_uri=CUSTOMERS_FE_BQ_BATCH_TABLE_URI,\n",
    "    entity_id_column=CUSTOMER_ID_COLUMN,\n",
    "    feature_group_id=CUSTOMER_BATCH_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=CUSTOMER_BATCH_FEATURE_IDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUSTOMER_STREAMING_FEATURES_GROUP_ID = \"fraudfinder_customers_streaming\"\n",
    "CUSTOMER_STREAMING_FEATURE_IDS = [\n",
    "    \"customer_id_nb_tx_15min_window\",\n",
    "    \"customer_id_nb_tx_30min_window\",\n",
    "    \"customer_id_nb_tx_60min_window\",\n",
    "    \"customer_id_avg_amount_15min_window\",\n",
    "    \"customer_id_avg_amount_30min_window\",\n",
    "    \"customer_id_avg_amount_60min_window\",\n",
    "]\n",
    "\n",
    "# Creating feature Group for streaming for customers\n",
    "create_fs_feature_group(\n",
    "    bq_source_uri=CUSTOMERS_STREAMING_FE_TABLE_URI,\n",
    "    entity_id_column=CUSTOMER_ID_COLUMN,\n",
    "    feature_group_id=CUSTOMER_STREAMING_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=CUSTOMER_STREAMING_FEATURE_IDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, create the featureGroup for terminals\n",
    "TERMINAL_ID_COLUMN = \"entity_id\"\n",
    "\n",
    "TERMINAL_BATCH_FEATURES_GROUP_ID = \"fraudfinder_terminals_batch\"\n",
    "TERMINAL_BATCH_FEATURE_IDS = [\n",
    "    \"terminal_id_nb_tx_1day_window\",\n",
    "    \"terminal_id_nb_tx_7day_window\",\n",
    "    \"terminal_id_nb_tx_14day_window\",\n",
    "    \"terminal_id_risk_1day_window\",\n",
    "    \"terminal_id_risk_7day_window\",\n",
    "    \"terminal_id_risk_14day_window\",\n",
    "]\n",
    "\n",
    "# Creating feature Group for batch for customers\n",
    "create_fs_feature_group(\n",
    "    bq_source_uri=TERMINALS_FE_BQ_BATCH_TABLE_URI,\n",
    "    entity_id_column=TERMINAL_ID_COLUMN,\n",
    "    feature_group_id=TERMINAL_BATCH_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=TERMINAL_BATCH_FEATURE_IDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, create the featureGroup for terminals streaming features\n",
    "TERMINAL_STREAMING_FEATURES_GROUP_ID = \"fraudfinder_terminals_streaming\"\n",
    "TERMINAL_STREAMING_FEATURE_IDS = [\n",
    "    \"terminal_id_nb_tx_15min_window\",\n",
    "    \"terminal_id_nb_tx_30min_window\",\n",
    "    \"terminal_id_nb_tx_60min_window\",\n",
    "    \"terminal_id_avg_amount_15min_window\",\n",
    "    \"terminal_id_avg_amount_30min_window\",\n",
    "    \"terminal_id_avg_amount_60min_window\",\n",
    "]\n",
    "\n",
    "# Creating feature Group for batch for customers\n",
    "create_fs_feature_group(\n",
    "    bq_source_uri=TERMINALS_STREAMING_FE_TABLE_URI,\n",
    "    entity_id_column=TERMINAL_ID_COLUMN,\n",
    "    feature_group_id=TERMINAL_STREAMING_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=TERMINAL_STREAMING_FEATURE_IDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Feature Groups to the Online Store with `FeatureView`\n",
    "\n",
    "Now that we have registered our `FeatureGroup`s (our feature sources), we need a way to tell the Feature Store to actually serve these features for real-time lookups. This is the job of a **`FeatureView`**.\n",
    "\n",
    "A `FeatureView` acts as a bridge between the feature sources (`FeatureGroup`s) and the online serving cluster (the `FeatureOnlineStore` we created earlier). It defines which features from which feature groups should be made available for low-latency retrieval.\n",
    "\n",
    "Key responsibilities of a `FeatureView`:\n",
    "*   **Linking Sources:** It links one or more `FeatureGroup`s to a specific `FeatureOnlineStore`.\n",
    "*   **Syncing Data:** It manages the synchronization of data from the BigQuery sources (defined in the `FeatureGroup`s) to the high-performance online store (Bigtable). You can configure this sync to run on a schedule (cron) or continuously. For this lab, we'll use a continuous sync to keep the online data as fresh as possible.\n",
    "\n",
    "We will create two `FeatureView`s, one for each entity type:\n",
    "1.  **`fv_fraudfinder_customers`**: This view will combine the batch and streaming features for customers.\n",
    "2.  **`fv_fraudfinder_terminals`**: This view will combine the batch and streaming features for terminals.\n",
    "\n",
    "The helper function below will create these views and start the data sync process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_online_fs_view(\n",
    "    fs_view_id,\n",
    "    fs_online_store_id,\n",
    "    feature_group_ids,\n",
    "    feature_ids_list,\n",
    "    continuous,\n",
    "    cron_schedule=None,\n",
    "):\n",
    "\n",
    "    feature_groups = []\n",
    "\n",
    "    for feature_group_id, feature_ids in zip(\n",
    "        feature_group_ids, feature_ids_list\n",
    "    ):\n",
    "        feature_groups.append(\n",
    "            feature_view_pb2.FeatureView.FeatureRegistrySource.FeatureGroup(\n",
    "                feature_group_id=feature_group_id,\n",
    "                feature_ids=feature_ids,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    feature_registry_source = (\n",
    "        feature_view_pb2.FeatureView.FeatureRegistrySource(\n",
    "            feature_groups=feature_groups\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if continuous:\n",
    "        sync_config = feature_view_pb2.FeatureView.SyncConfig(continuous=True)\n",
    "    else:\n",
    "        sync_config = feature_view_pb2.FeatureView.SyncConfig(\n",
    "            cron=cron_schedule\n",
    "        )\n",
    "\n",
    "    create_view_lro = admin_client.create_feature_view(\n",
    "        feature_online_store_admin_service_pb2.CreateFeatureViewRequest(\n",
    "            parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{fs_online_store_id}\",\n",
    "            feature_view_id=fs_view_id,\n",
    "            run_sync_immediately=True,\n",
    "            feature_view=feature_view_pb2.FeatureView(\n",
    "                feature_registry_source=feature_registry_source,\n",
    "                sync_config=sync_config,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Wait for LRO to complete and show result\n",
    "    print(create_view_lro.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Check to ensure all resources provisioned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test cell to ensure that all resources provisioned:\n",
    "TEST_FEATURE_VIEW_ID = \"fv_fraudfinder_test_provisioned\"\n",
    "\n",
    "test_feature_view = f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURESTORE_ID}/featureViews/{TEST_FEATURE_VIEW_ID}\"\n",
    "\n",
    "admin_client.list_feature_views(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURESTORE_ID}\"\n",
    ")\n",
    "\n",
    "create_online_fs_view(\n",
    "    fs_view_id=TEST_FEATURE_VIEW_ID,\n",
    "    fs_online_store_id=FEATURESTORE_ID,\n",
    "    feature_group_ids=[\n",
    "        CUSTOMER_BATCH_FEATURES_GROUP_ID,\n",
    "        CUSTOMER_STREAMING_FEATURES_GROUP_ID,\n",
    "    ],\n",
    "    feature_ids_list=[\n",
    "        CUSTOMER_BATCH_FEATURE_IDS,\n",
    "        CUSTOMER_STREAMING_FEATURE_IDS,\n",
    "    ],\n",
    "    continuous=True,\n",
    ")\n",
    "\n",
    "# Delete TEST FeatureView\n",
    "delete_view_lro = admin_client.delete_feature_view(name=test_feature_view)\n",
    "\n",
    "print(delete_view_lro.result())\n",
    "\n",
    "admin_client.list_feature_views(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURESTORE_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating featurestore view for Customers Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUSTOMER_FEATURE_VIEW_ID = \"fv_fraudfinder_customers\"\n",
    "\n",
    "create_online_fs_view(\n",
    "    fs_view_id=CUSTOMER_FEATURE_VIEW_ID,\n",
    "    fs_online_store_id=FEATURESTORE_ID,\n",
    "    feature_group_ids=[\n",
    "        CUSTOMER_BATCH_FEATURES_GROUP_ID,\n",
    "        CUSTOMER_STREAMING_FEATURES_GROUP_ID,\n",
    "    ],\n",
    "    feature_ids_list=[\n",
    "        CUSTOMER_BATCH_FEATURE_IDS,\n",
    "        CUSTOMER_STREAMING_FEATURE_IDS,\n",
    "    ],\n",
    "    continuous=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating featurestore view for Terminals Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TERMINAL_FEATURE_VIEW_ID = \"fv_fraudfinder_terminals\"\n",
    "\n",
    "create_online_fs_view(\n",
    "    fs_view_id=TERMINAL_FEATURE_VIEW_ID,\n",
    "    fs_online_store_id=FEATURESTORE_ID,\n",
    "    feature_group_ids=[\n",
    "        TERMINAL_BATCH_FEATURES_GROUP_ID,\n",
    "        TERMINAL_STREAMING_FEATURES_GROUP_ID,\n",
    "    ],\n",
    "    feature_ids_list=[\n",
    "        TERMINAL_BATCH_FEATURE_IDS,\n",
    "        TERMINAL_STREAMING_FEATURE_IDS,\n",
    "    ],\n",
    "    continuous=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the `FeatureView` instance is created by listing all the feature views within the online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Again, list all feature view under the FEATURESTORE_ID to confirm\n",
    "admin_client.list_feature_views(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURESTORE_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "admin_client.list_feature_view_syncs(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURESTORE_ID}/featureViews/{CUSTOMER_FEATURE_VIEW_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "admin_client.list_feature_view_syncs(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURESTORE_ID}/featureViews/{TERMINAL_FEATURE_VIEW_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Serving for Real-Time Fraud Detection\n",
    "\n",
    "Now that we've ingested our features into the Vertex AI Feature Store, it's time to talk about **online serving**. In the context of fraud detection, online serving refers to the process of retrieving feature values for a single entity (e.g., a customer or a terminal) in real-time, with very low latency. This is a critical requirement for our use case, as we need to be able to make a fraud prediction in a matter of milliseconds, while a customer is waiting for their transaction to be approved.\n",
    "\n",
    "Vertex AI Feature Store provides a highly scalable and low-latency online serving solution that is optimized for real-time use cases. When you ingest features into a feature store, they are stored in both an offline storage (BigQuery) for batch use cases and an online store (Bigtable) for real-time serving. This dual-storage architecture allows you to use the same features for both training and inference, without having to worry about data skew.\n",
    "\n",
    "In the following cells, we'll show you how to use the Vertex AI SDK to fetch feature values from the online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_client = FeatureOnlineStoreServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `FeatureView` already defines the features needed for the model (via the BigQuery view in this demo). To fetch the data, submit a `fetch_feature_values` request specifying the `FeatureView` resource path and the ID of the entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets get a customer record from BigQuery View:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery customer_view_record --project {PROJECT_ID}\n",
    "SELECT * \n",
    "FROM tx.v_customers_features\n",
    "ORDER BY feature_timestamp DESC\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_view_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can check that this record availible using Vertex AI Feature Store online serving\n",
    "Note: it can take up to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Featurestore ID: {FEATURESTORE_ID}\")\n",
    "print(f\"Featurestore View ID: {CUSTOMER_FEATURE_VIEW_ID}\")\n",
    "\n",
    "customer_key = customer_view_record[\"entity_id\"][0]\n",
    "print(f\"entity_id={customer_key}\")\n",
    "\n",
    "FEATURE_VIEW_FULL_ID = f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURESTORE_ID}/featureViews/{CUSTOMER_FEATURE_VIEW_ID}\"\n",
    "\n",
    "try:\n",
    "    fe_data = data_client.fetch_feature_values(\n",
    "        request=feature_online_store_service_pb2.FetchFeatureValuesRequest(\n",
    "            feature_view=FEATURE_VIEW_FULL_ID,\n",
    "            data_key=feature_online_store_service_pb2.FeatureViewDataKey(\n",
    "                key=customer_key\n",
    "            ),\n",
    "            data_format=feature_online_store_service_pb2.FeatureViewDataFormat.PROTO_STRUCT,\n",
    "        )\n",
    "    )\n",
    "    customer_features = json.dumps(\n",
    "        {k: v for k, v in fe_data.proto_struct.items()}, indent=4\n",
    "    )\n",
    "    print(f\"Customer Features: {customer_features}\")\n",
    "except Exception as exp:\n",
    "    print(\"ERROR: \" + str(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery terminal_view_record --project {PROJECT_ID}\n",
    "SELECT * \n",
    "FROM tx.v_terminals_features\n",
    "ORDER BY feature_timestamp DESC\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminal_view_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Featurestore ID: {FEATURESTORE_ID}\")\n",
    "print(f\"Featurestore View ID: {TERMINAL_FEATURE_VIEW_ID}\")\n",
    "\n",
    "terminal_key = terminal_view_record[\"entity_id\"][0]\n",
    "print(f\"entity_id={terminal_key}\")\n",
    "\n",
    "FEATURE_VIEW_FULL_ID = f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURESTORE_ID}/featureViews/{TERMINAL_FEATURE_VIEW_ID}\"\n",
    "\n",
    "try:\n",
    "    fe_data = data_client.fetch_feature_values(\n",
    "        request=feature_online_store_service_pb2.FetchFeatureValuesRequest(\n",
    "            feature_view=FEATURE_VIEW_FULL_ID,\n",
    "            data_key=feature_online_store_service_pb2.FeatureViewDataKey(\n",
    "                key=customer_key\n",
    "            ),\n",
    "            data_format=feature_online_store_service_pb2.FeatureViewDataFormat.PROTO_STRUCT,\n",
    "        )\n",
    "    )\n",
    "    customer_features = json.dumps(\n",
    "        {k: v for k, v in fe_data.proto_struct.items()}, indent=4\n",
    "    )\n",
    "    print(f\"Customer Features: {customer_features}\")\n",
    "except Exception as exp:\n",
    "    print(\"ERROR: \" + str(exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1335c1b0761"
   },
   "source": [
    "### Inspect your feature store in the Vertex AI console\n",
    "\n",
    "You can also inspect your feature store in the [Vertex AI Feature Store console](https://console.cloud.google.com/vertex-ai/feature-store/online-stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END\n",
    "\n",
    "Now you can go to the next notebook `03_feature_engineering_streaming_new_fs.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c5b719f3dcd"
   },
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02_feature_engineering_batch.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

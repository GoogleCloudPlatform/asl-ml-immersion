{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting TFX metadata\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Use a GRPC server to access and analyze pipeline artifacts stored in the ML Metadata service of your AI Platform Pipelines instance.\n",
    "\n",
    "In this lab, you will explore TFX pipeline metadata including pipeline and run artifacts. An **AI Platform Pipelines** instance includes the [ML Metadata](https://github.com/google/ml-metadata) service. In **AI Platform Pipelines**, ML Metadata uses *MySQL* as a database backend and can be accessed using a GRPC server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ml_metadata\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "\n",
    "from ml_metadata.metadata_store import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.types import standard_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\"\n",
    "!python -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore metadata from existing TFX pipeline runs from AI Pipelines instance created in `lab-02` or `lab-03`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Kubernetes port forwarding\n",
    "\n",
    "To enable access to the ML Metadata GRPC server, configure Kubernetes port forwarding.\n",
    "\n",
    "From a JupyterLab terminal, execute the following commands:\n",
    "\n",
    "```\n",
    "gcloud container clusters get-credentials [YOUR CLUSTER] --zone [YOUR CLUSTER ZONE]  \n",
    "kubectl port-forward  service/metadata-grpc-service --namespace [YOUR NAMESPACE] 7000:8080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed to the next step, \"Connecting to ML Metadata\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to ML Metadata \n",
    "\n",
    "Configure ML Metadata GRPC client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpc_host = \"localhost\"\n",
    "grpc_port = 7000\n",
    "connection_config = metadata_store_pb2.MetadataStoreClientConfig()\n",
    "connection_config.host = grpc_host\n",
    "connection_config.port = grpc_port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to ML Metadata service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = metadata_store.MetadataStore(connection_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring ML Metadata \n",
    "\n",
    "The Metadata Store uses the following data model:\n",
    "\n",
    "- `ArtifactType` describes an artifact's type and its properties that are stored in the Metadata Store. These types can be registered on-the-fly with the Metadata Store in code, or they can be loaded in the store from a serialized format. Once a type is registered, its definition is available throughout the lifetime of the store.\n",
    "- `Artifact` describes a specific instances of an ArtifactType, and its properties that are written to the Metadata Store.\n",
    "- `ExecutionType` describes a type of component or step in a workflow, and its runtime parameters.\n",
    "- `Execution` is a record of a component run or a step in an ML workflow and the runtime parameters. An Execution can be thought of as an instance of an ExecutionType. Every time a developer runs an ML pipeline or step, executions are recorded for each step.\n",
    "- `Event` is a record of the relationship between an Artifact and Executions. When an Execution happens, Events record every Artifact that was used by the Execution, and every Artifact that was produced. These records allow for provenance tracking throughout a workflow. By looking at all Events MLMD knows what Executions happened, what Artifacts were created as a result, and can recurse back from any Artifact to all of its upstream inputs.\n",
    "- `ContextType` describes a type of conceptual group of Artifacts and Executions in a workflow, and its structural properties. For example: projects, pipeline runs, experiments, owners.\n",
    "- `Context` is an instances of a ContextType. It captures the shared information within the group. For example: project name, changelist commit id, experiment annotations. It has a user-defined unique name within its ContextType.\n",
    "- `Attribution` is a record of the relationship between Artifacts and Contexts.\n",
    "- `Association` is a record of the relationship between Executions and Contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the registered artifact types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for artifact_type in store.get_artifact_types():\n",
    "    print(artifact_type.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the registered execution types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for execution_type in store.get_execution_types():\n",
    "    print(execution_type.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the registered context types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context_type in store.get_context_types():\n",
    "    print(context_type.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing TFX artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data analysis and validation artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with metadata.Metadata(connection_config) as store:\n",
    "    stats_artifacts = store.get_artifacts_by_type(\n",
    "        standard_artifacts.ExampleStatistics.TYPE_NAME\n",
    "    )\n",
    "    schema_artifacts = store.get_artifacts_by_type(\n",
    "        standard_artifacts.Schema.TYPE_NAME\n",
    "    )\n",
    "    anomalies_artifacts = store.get_artifacts_by_type(\n",
    "        standard_artifacts.ExampleAnomalies.TYPE_NAME\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_path = stats_artifacts[-1].uri\n",
    "train_stats_file = os.path.join(stats_path, \"train\", \"stats_tfrecord\")\n",
    "eval_stats_file = os.path.join(stats_path, \"eval\", \"stats_tfrecord\")\n",
    "print(\n",
    "    \"Train stats file:{}, Eval stats file:{}\".format(\n",
    "        train_stats_file, eval_stats_file\n",
    "    )\n",
    ")\n",
    "\n",
    "schema_file = os.path.join(schema_artifacts[-1].uri, \"schema.pbtxt\")\n",
    "print(\"Generated schame file:{}\".format(schema_file))\n",
    "anomalies_file = os.path.join(anomalies_artifacts[-1].uri, \"anomalies.pbtxt\")\n",
    "print(\"Generated anomalies file:{}\".format(anomalies_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: looking at the features visualized below, answer the following questions:\n",
    "\n",
    "- Which feature transformations would you apply to each feature with TF Transform?\n",
    "- Are there data quality issues with certain features that may impact your model performance? How might you deal with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = tfdv.load_statistics(train_stats_file)\n",
    "eval_stats = tfdv.load_statistics(eval_stats_file)\n",
    "tfdv.visualize_statistics(\n",
    "    lhs_statistics=eval_stats,\n",
    "    rhs_statistics=train_stats,\n",
    "    lhs_name=\"EVAL_DATASET\",\n",
    "    rhs_name=\"TRAIN_DATASET\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.load_schema_text(schema_file)\n",
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = tfdv.load_anomalies_text(anomalies_file)\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve model evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with metadata.Metadata(connection_config) as store:\n",
    "    model_eval_artifacts = store.get_artifacts_by_type(\n",
    "        standard_artifacts.ModelEvaluation.TYPE_NAME\n",
    "    )\n",
    "\n",
    "model_eval_path = model_eval_artifacts[-1].uri\n",
    "print(\"Generated model evaluation result:{}\".format(model_eval_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize model evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: review the model evaluation results below and answer the following questions:\n",
    "\n",
    "- Which Wilderness Area had the highest accuracy?\n",
    "- Which Wilderness Area had the lowest performance? Why do you think that is? What are some steps you could take to improve your next model runs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = tfma.load_eval_result(model_eval_path)\n",
    "tfma.view.render_slicing_metrics(eval_result, slicing_column=\"Wilderness_Area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging tip: If the TFMA visualization of the Evaluator results do not render, try switching to view in a Classic Jupyter Notebook. You do so by clicking `Help > Launch Classic Notebook` and re-opening the notebook and running the above cell to see the interactive TFMA results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8a4i3PAFVQd",
    "tags": []
   },
   "source": [
    "# Dataflow NER Pipeline with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%pip install --quiet scikit-learn\n",
    "%pip install --quiet transformers[torch]\n",
    "%pip install --quiet seqeval\n",
    "%pip install --quiet tensorflow\n",
    "%pip install --quiet tf-keras\n",
    "%pip install --quiet torch --quiet\n",
    "%pip install --quiet datasets --quiet\n",
    "%pip install --quiet evaluate --quiet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "PROJECT_ID = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "%env GOOGLE_CLOUD_PROJECT={PROJECT_ID}\n",
    "#BUCKET_NAME=f'dataflow_demo_{PROJECT_ID}'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use the exact model you plan to train (bert-base, roberta, etc.)\n",
    "MODEL_NAME = \"google-bert/bert-base-multilingual-cased\"\n",
    "DATASET = \"./train_bert_ner_1k.txt\"\n",
    "OUTPUT_BUCKET_NAME=f\"bert-finetuning-ner-{PROJECT_ID}\"\n",
    "gcs_bucket = f\"gs://{OUTPUT_BUCKET_NAME}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!gsutil mb -l {REGION} gs://{OUTPUT_BUCKET_NAME}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference pipeline based on Apache Beam\n",
    "Can be orchestrated by using Cloud Composer or Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "DATAFLOW_BUCKET=f\"bert-ner-demo-io-storage-{PROJECT_ID}\" \n",
    "OUTPUT_GCS_BUCKET = f\"gs://{DATAFLOW_BUCKET}/output/\"\n",
    "TEMP_LOCATION=f\"gs://{DATAFLOW_BUCKET}/temp/\"\n",
    "STAGING_LOCATION=f\"gs://{DATAFLOW_BUCKET}/staging/\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!gsutil mb -l {REGION} gs://{DATAFLOW_BUCKET}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "ENDPOINT_ID=\"4897865805393297408\"\n",
    "PUBSUB_SUBSCRIPTION='projects/oleksandr-demo/subscriptions/input_messages-sub'\n",
    "BIGQUERY_NER_TABLE=f'{PROJECT_ID}:genai.ner_extract'\n",
    "DATAFLOW_JOB_NAME='bert-ner-inference'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import json\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "from apache_beam import Create\n",
    "from typing import Tuple, Iterable\n",
    "from google.cloud import storage\n",
    "from apache_beam.ml.inference import RunInference\n",
    "from apache_beam.ml.inference.base import PredictionResult, KeyedModelHandler, ModelHandler\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "import apache_beam as beam"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "def _get_core_type(pred):\n",
    "    \"\"\"\n",
    "    Helper to extract 'PHONE' from 'B-PHONE', 'I-PHONE', or just 'PHONE'\n",
    "    Handles keys 'entity', 'entity_group', or 'label'.\n",
    "    \"\"\"\n",
    "    label = pred.get('entity_group') or pred.get('entity') or pred.get('label')\n",
    "    if not label: return \"UNKNOWN\"\n",
    "\n",
    "    # Strip B- or I- prefixes\n",
    "    if \"-\" in label and (label.startswith(\"B-\") or label.startswith(\"I-\")):\n",
    "        return label.split(\"-\", 1)[1]\n",
    "    return label"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "def postprocess_predictions(predictions_result, merge_distance=1):\n",
    "    \"\"\"\n",
    "    Merges adjacent entities of the same type.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of dicts returned by the pipeline.\n",
    "        merge_distance: Max characters allowed between entities to merge them.\n",
    "                        0 = must be touching (e.g., \"123\" + \"-\")\n",
    "                        1 = allows spaces (e.g., \"John\" + \" \" + \"Doe\")\n",
    "    \"\"\"\n",
    "    predictions = predictions_result[1].inference\n",
    "    if not predictions:\n",
    "        return []\n",
    "\n",
    "    # 1. Sort by start index to ensure processing order\n",
    "    sorted_preds = sorted(predictions, key=lambda x: int(x['start']))\n",
    "    \n",
    "    merged = []\n",
    "    \n",
    "    # Initialize the first entity\n",
    "    # We strip 'B-' or 'I-' to compare the core type (e.g., \"PHONE\")\n",
    "    first_pred = sorted_preds[0]\n",
    "    current_group = {\n",
    "        \"entity_group\": _get_core_type(first_pred),\n",
    "        \"score\": first_pred['score'],\n",
    "        \"word\": first_pred['word'],\n",
    "        \"start\": int(first_pred['start']),\n",
    "        \"end\": int(first_pred['end'])\n",
    "    }\n",
    "\n",
    "    for next_pred in sorted_preds[1:]:\n",
    "        next_type = _get_core_type(next_pred)\n",
    "        \n",
    "        # Calculate gap between current end and next start\n",
    "        gap = int(next_pred['start'] - current_group['end'])\n",
    "        \n",
    "        # MERGE CONDITION:\n",
    "        # 1. Same Entity Type (e.g. PHONE == PHONE)\n",
    "        # 2. Adjacent or close enough (gap <= threshold)\n",
    "        if next_type == current_group['entity_group'] and gap <= merge_distance:\n",
    "            \n",
    "            # Update End Position\n",
    "            current_group['end'] = int(next_pred['end'])\n",
    "            \n",
    "            # Merge text safely\n",
    "            # If there is a space in the original text (gap > 0), add it back\n",
    "            # Also handle BERT subwords (remove '##' if present)\n",
    "            sep = \" \" * gap # Reconstruct space if gap exists\n",
    "            clean_word = next_pred['word'].replace(\"##\", \"\")\n",
    "            \n",
    "            # If the previous word ended with a subword marker (rare but possible), handle it\n",
    "            # But usually we just append\n",
    "            current_group['word'] += sep + clean_word\n",
    "            \n",
    "            # Update Score: You can take Max or Average\n",
    "            current_group['score'] = max(current_group['score'], next_pred['score'])\n",
    "            \n",
    "        else:\n",
    "            # NO MERGE: Push current and start new\n",
    "            merged.append(current_group)\n",
    "            \n",
    "            current_group = {\n",
    "                \"entity_group\": next_type,\n",
    "                \"score\": next_pred['score'],\n",
    "                \"word\": next_pred['word'],\n",
    "                \"start\": int(next_pred['start']),\n",
    "                \"end\": int(next_pred['end'])\n",
    "            }\n",
    "\n",
    "    # Append the final group\n",
    "    merged.append(current_group)\n",
    "    \n",
    "    # Final cleanup of words (in case the very first word had ##)\n",
    "    for m in merged:\n",
    "        m['word'] = m['word'].replace(\"##\", \"\")\n",
    "        \n",
    "    return {\"id\": predictions_result[0], \"message\": predictions_result[1].example, \"ner\": merged}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define BigQuery Schema\n",
    "Using Nested Repeated field for the list of extracted NER entities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "BIGQUERY_NER_SCHEMA = {\n",
    "    \"fields\": [\n",
    "        # Root level fields\n",
    "        {\"name\": \"id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"message\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        \n",
    "        # Nested Repeated field for the list of entities\n",
    "        {\n",
    "            \"name\": \"ner\",\n",
    "            \"type\": \"RECORD\", \n",
    "            \"mode\": \"REPEATED\",\n",
    "            \"fields\": [\n",
    "                {\"name\": \"entity_group\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                {\"name\": \"score\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n",
    "                {\"name\": \"word\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                {\"name\": \"start\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "                {\"name\": \"end\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a basic pipeline with NER Inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "elements=[\n",
    "    {\"id\": \"0e42fec7-71ce-40bd-ac83-e2ffc36c6fe0\", \"message\": \"Is kweaver@example.org the correct address for Christina Ramirez ?\"},\n",
    "    {\"id\": \"3590d10c-8c1d-4df0-817d-4184387d3a54\", \"message\": \"Is davidlucas@example.com the correct address for Patricia Parsons ?\"}\n",
    "]\n",
    "\n",
    "ner_model_handler = KeyedModelHandler(VertexAIModelHandlerJSON(\n",
    "    endpoint_id=ENDPOINT_ID, \n",
    "    project=PROJECT_ID, \n",
    "    location=REGION))\n",
    "\n",
    "# Set Interactive Runner for testing\n",
    "options = PipelineOptions(flags={})\n",
    "\n",
    "# Set Interactive Runner for testing\n",
    "p = beam.Pipeline(InteractiveRunner(), options=options)\n",
    "\n",
    "p_output = (p\n",
    "        | \"Create elements\" >> Create(elements)\n",
    "        | \"ToKeyValue\" >> beam.Map(lambda x: (x[\"id\"], x[\"message\"]))\n",
    "        | \"NER Model\" >> RunInference(ner_model_handler)\n",
    "        | \"Postprocess\" >> beam.Map(postprocess_predictions)\n",
    "        | \"Format Output\" >> beam.Map(json.dumps))\n",
    "\n",
    "ib.show(p_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Dataflow Streaming Job"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vfwfqzBnKeL2",
    "tags": []
   },
   "source": [
    "# Set up Beam PipelineOptions for Dataflow Runner\n",
    "pipeline_options = PipelineOptions(\n",
    "    runner=\"DataflowRunner\",\n",
    "    project=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    temp_location=TEMP_LOCATION,\n",
    "    staging_location=STAGING_LOCATION,\n",
    "    job_name=DATAFLOW_JOB_NAME,\n",
    "    save_main_session=True,\n",
    "    max_num_workers=1,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "pp = (p\n",
    "        | 'ReadPubSub' >> beam.io.ReadFromPubSub(subscription=PUBSUB_SUBSCRIPTION)\n",
    "        | 'DecodeMsg' >> beam.Map(lambda row: json.loads(row.decode('utf-8')))\n",
    "        | \"ToKeyValue\" >> beam.Map(lambda x: (x[\"id\"], x[\"message\"]))\n",
    "        | \"NER Model\" >> RunInference(ner_model_handler)\n",
    "        | \"Postprocess\" >> beam.Map(postprocess_predictions)\n",
    "        | \"WriteBigQuery\" >> beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "            table=BIGQUERY_NER_TABLE,\n",
    "            schema=BIGQUERY_NER_SCHEMA,\n",
    "            method=beam.io.gcp.bigquery.WriteToBigQuery.Method.STREAMING_INSERTS,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
    "        )\n",
    "\n",
    "p.run().wait_until_finish()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Apache Beam 2.69.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.69.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8a4i3PAFVQd",
    "tags": []
   },
   "source": [
    "# Dataflow NER Pipeline with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%pip install --quiet scikit-learn\n",
    "%pip install --quiet transformers[torch]\n",
    "%pip install --quiet seqeval\n",
    "%pip install --quiet tensorflow\n",
    "%pip install --quiet tf-keras\n",
    "%pip install --quiet torch --quiet\n",
    "%pip install --quiet datasets --quiet\n",
    "%pip install --quiet evaluate --quiet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "PROJECT_ID = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "%env GOOGLE_CLOUD_PROJECT={PROJECT_ID}\n",
    "#BUCKET_NAME=f'dataflow_demo_{PROJECT_ID}'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use the exact model you plan to train (bert-base, roberta, etc.)\n",
    "MODEL_NAME = \"google-bert/bert-base-multilingual-cased\"\n",
    "DATASET = \"./train_bert_ner_1k.txt\"\n",
    "OUTPUT_BUCKET_NAME=f\"bert-finetuning-ner-{PROJECT_ID}\"\n",
    "gcs_bucket = f\"gs://{OUTPUT_BUCKET_NAME}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!gsutil mb -l {REGION} gs://{OUTPUT_BUCKET_NAME}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=os.getenv(\"PROJECT_ID\"),\n",
    "    location=os.getenv(\"LOCATION\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference pipeline based on Apache Beam\n",
    "Can be orchestrated by using Cloud Composer or Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "DATAFLOW_BUCKET=f\"bert-ner-demo-io-storage-{PROJECT_ID}\" \n",
    "OUTPUT_GCS_BUCKET = f\"gs://{DATAFLOW_BUCKET}/output/\"\n",
    "TEMP_LOCATION=f\"gs://{DATAFLOW_BUCKET}/temp/\"\n",
    "STAGING_LOCATION=f\"gs://{DATAFLOW_BUCKET}/staging/\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!gsutil mb -l {REGION} gs://{DATAFLOW_BUCKET}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vfwfqzBnKeL2",
    "tags": []
   },
   "source": [
    "import json\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "from apache_beam import Create\n",
    "from typing import Tuple, Iterable\n",
    "from google.cloud import storage\n",
    "from apache_beam.ml.inference import RunInference\n",
    "from apache_beam.ml.inference.base import PredictionResult, KeyedModelHandler, ModelHandler\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFacePipelineModelHandler, PipelineTask\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam as beam\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, pipeline\n",
    "\n",
    "class BertNerPipelineHandler(ModelHandler):\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.pipeline = None\n",
    "        self.local_folder = \"./tmp_ner_finetuned\"\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the Transformer pipeline model.\"\"\"\n",
    "        # Load the fine-tuned model and tokenizer\n",
    "        if self.pipeline is None:\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(self.model_path)\n",
    "            blobs = bucket.list_blobs(prefix=\"\")\n",
    "            if not os.path.exists(self.local_folder):\n",
    "                os.makedirs(self.local_folder )\n",
    "            for blob in blobs:\n",
    "                file_path = os.path.join(self.local_folder, os.path.basename(blob.name))\n",
    "                blob.download_to_filename(file_path)\n",
    "            tokenizer = BertTokenizerFast.from_pretrained(self.local_folder)\n",
    "            model = BertForTokenClassification.from_pretrained(self.local_folder)\n",
    "            self.pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")\n",
    "        return self.pipeline\n",
    "\n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        \"\"\"Runs inference on a batch of inputs.\"\"\"\n",
    "        predictions = model(batch)\n",
    "        return [PredictionResult(x, y) for x, y in zip(batch, predictions)]\n",
    "\n",
    "    def get_num_bytes(self, batch):\n",
    "        \"\"\"Returns the estimated size of the batch.\"\"\"\n",
    "        return sum(len(str(x).encode('utf-8')) for x in batch)\n",
    "\n",
    "model_handler = BertNerPipelineHandler(OUTPUT_BUCKET_NAME)\n",
    "\n",
    "def mask_sensetive_data(predictions):\n",
    "    text = predictions.example\n",
    "    redacted_text = text\n",
    "\n",
    "    for mask in predictions.inference:\n",
    "        text_to_mask = text[int(mask['start']): int(mask['end'])]\n",
    "        if len(text_to_mask) > 0:\n",
    "            redacted_text = redacted_text.replace(text_to_mask, f\"[{mask['entity_group']}]\")\n",
    "    return {\"text\": text, \"redacted_text\": redacted_text}\n",
    "\n",
    "def extract_entity(predictions):\n",
    "    text = predictions.example\n",
    "    redacted_text = text\n",
    "    entity_dict = {}\n",
    "    for mask in predictions.inference:\n",
    "        text_to_mask = text[int(mask['start']): int(mask['end'])]\n",
    "        if len(text_to_mask) > 0:\n",
    "            if mask['entity_group'] not in entity_dict:\n",
    "                entity_dict[mask['entity_group']] = []\n",
    "            entity_dict[mask['entity_group']].append(text_to_mask)\n",
    "            \n",
    "    return {\"text\": text, \"extracted\": entity_dict}\n",
    "\n",
    "p = beam.Pipeline(options=PipelineOptions())\n",
    "\n",
    "elements=[\n",
    "    \"Send the invoice to 123 Main Street, New York. Contact John Connnor at johnconnor@example.com.\",\n",
    "    \"My name is Jessica Williams.\",\n",
    "]\n",
    "\n",
    "pp = (p\n",
    "      | \"Create elements\" >> Create(elements)\n",
    "      | \"RunInference\" >> RunInference(model_handler)\n",
    "      | \"Mask Data\" >> beam.Map(extract_entity) #extract_entity mask_sensetive_data\n",
    "      | \"Format Output\" >> beam.Map(json.dumps))\n",
    "\n",
    "ib.show(pp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import json\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "from apache_beam import Create\n",
    "from typing import Tuple, Iterable\n",
    "from google.cloud import storage\n",
    "from apache_beam.ml.inference import RunInference\n",
    "from apache_beam.ml.inference.base import PredictionResult, KeyedModelHandler, ModelHandler\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFacePipelineModelHandler, PipelineTask\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam as beam\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, pipeline\n",
    "\n",
    "class BertNerPipelineHandler(ModelHandler):\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.pipeline = None\n",
    "        self.local_folder = \"./tmp_ner_finetuned\"\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the Transformer pipeline model.\"\"\"\n",
    "        # Load the fine-tuned model and tokenizer\n",
    "        if self.pipeline is None:\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(self.model_path)\n",
    "            blobs = bucket.list_blobs(prefix=\"\")\n",
    "            if not os.path.exists(self.local_folder):\n",
    "                os.makedirs(self.local_folder )\n",
    "            for blob in blobs:\n",
    "                file_path = os.path.join(self.local_folder, os.path.basename(blob.name))\n",
    "                blob.download_to_filename(file_path)\n",
    "            tokenizer = BertTokenizerFast.from_pretrained(self.local_folder)\n",
    "            model = BertForTokenClassification.from_pretrained(self.local_folder)\n",
    "            self.pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")\n",
    "        return self.pipeline\n",
    "\n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        \"\"\"Runs inference on a batch of inputs.\"\"\"\n",
    "        predictions = model(batch)\n",
    "        return [PredictionResult(x, y) for x, y in zip(batch, predictions)]\n",
    "\n",
    "    def get_num_bytes(self, batch):\n",
    "        \"\"\"Returns the estimated size of the batch.\"\"\"\n",
    "        return sum(len(str(x).encode('utf-8')) for x in batch)\n",
    "\n",
    "model_handler = BertNerPipelineHandler(OUTPUT_BUCKET_NAME)\n",
    "\n",
    "def mask_sensetive_data(predictions):\n",
    "    text = predictions.example\n",
    "    redacted_text = text\n",
    "\n",
    "    for mask in predictions.inference:\n",
    "        text_to_mask = text[int(mask['start']): int(mask['end'])]\n",
    "        if len(text_to_mask) > 0:\n",
    "            redacted_text = redacted_text.replace(text_to_mask, f\"[{mask['entity_group']}]\")\n",
    "    return {\"text\": text, \"redacted_text\": redacted_text}\n",
    "\n",
    "\n",
    "def extract_entity(predictions):\n",
    "    text = predictions.example\n",
    "    redacted_text = text\n",
    "    entity_dict = {}\n",
    "    for mask in predictions.inference:\n",
    "        text_to_mask = text[int(mask['start']): int(mask['end'])]\n",
    "        if len(text_to_mask) > 0:\n",
    "            if mask['entity_group'] not in entity_dict:\n",
    "                entity_dict[mask['entity_group']] = []\n",
    "            entity_dict[mask['entity_group']].append(text_to_mask)\n",
    "            \n",
    "    return {\"text\": text} #, \"extracted\": entity_dict}\n",
    "\n",
    "# Set up Beam PipelineOptions for Dataflow\n",
    "# pipeline_options = PipelineOptions(\n",
    "#     runner=\"DirectRunner\",\n",
    "#     project=PROJECT_ID,\n",
    "#     region=LOCATION,\n",
    "#     temp_location=TEMP_LOCATION,\n",
    "#     staging_location=STAGING_LOCATION,\n",
    "#     job_name=\"bert-demo-ner-inference\",\n",
    "#     save_main_session=True,\n",
    "# )\n",
    "\n",
    "p = beam.Pipeline(options=PipelineOptions())\n",
    "\n",
    "output_table='genai.ner_pipeline'\n",
    "\n",
    "elements=[\n",
    "    \"Send the invoice to 123 Main Street, New York. Contact John Connnor at johnconnor@example.com.\",\n",
    "    \"My name is Jessica Williams.\",\n",
    "]\n",
    "\n",
    "pp = (p\n",
    "        # | 'Init' >> beam.Create([input_json_file])\n",
    "        # | \"Read File Content\" >> beam.io.fileio.ReadMatches()\n",
    "        # | \"Extract Content\" >> beam.Map(lambda file: file.read_utf8())\n",
    "        # | \"Parse JSON\" >> beam.Map(json.loads)\n",
    "        # | \"To Records\" >> beam.FlatMap()\n",
    "        # | \"Extract text\" >> beam.Map(lambda x: x[\"text\"])\n",
    "        | \"Create elements\" >> Create(elements)\n",
    "        | \"RunInference\" >> RunInference(model_handler)\n",
    "        | \"Postprocess\" >> beam.Map(extract_entity)\n",
    "        | \"Format Output\" >> beam.Map(json.dumps)\n",
    "        | \"Write BigQuery\" >> beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "            table=output_table,\n",
    "            schema='text:STRING',\n",
    "            method=beam.io.gcp.bigquery.WriteToBigQuery.Method.STREAMING_INSERTS,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
    "        )\n",
    "\n",
    "p.run().wait_until_finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Apache Beam 2.69.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.69.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

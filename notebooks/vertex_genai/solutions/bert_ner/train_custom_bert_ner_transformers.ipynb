{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8a4i3PAFVQd",
    "tags": []
   },
   "source": [
    "# Named Entity Recognition (NER) with Transformers (Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%pip install --quiet faker\n",
    "%pip install --quiet scikit-learn\n",
    "%pip install --quiet transformers[torch]\n",
    "%pip install --quiet seqeval\n",
    "%pip install --quiet tensorflow\n",
    "%pip install --quiet tf-keras\n",
    "%pip install --quiet torch --quiet\n",
    "%pip install --quiet datasets --quiet\n",
    "%pip install --quiet evaluate --quiet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "PROJECT_ID = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "%env GOOGLE_CLOUD_PROJECT={PROJECT_ID}\n",
    "BUCKET_NAME=f'dataflow_demo_{PROJECT_ID}'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use the exact model you plan to train (bert-base, roberta, etc.)\n",
    "MODEL_NAME = \"google-bert/bert-base-multilingual-cased\"\n",
    "FINETUNED_MODEL_PATH = \"./ner_finetuned_v2\"\n",
    "DATASET = \"./train_bert_ner_1k.txt\"\n",
    "gcs_bucket = \"gs://bert-finetuning-ner-demo\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=os.getenv(\"PROJECT_ID\"),\n",
    "    location=os.getenv(\"LOCATION\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "def upload_to_gcs(model_dir, bucket_name):\n",
    "    \"\"\"\n",
    "    Uploads a fine-tuned model directory to Google Cloud Storage (GCS).\n",
    "    \"\"\"\n",
    "    bucket_name = bucket_name.replace(\"gs://\", \"\")  # Remove gs:// prefix if present\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    for root, _, files in os.walk(model_dir):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            gcs_path = os.path.relpath(local_path, model_dir)\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "            print(f\"Uploaded {local_path} to gs://{bucket_name}/{gcs_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import random\n",
    "import re\n",
    "import json\n",
    "from faker import Faker\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Initialize Faker and YOUR specific Tokenizer\n",
    "fake = Faker()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ENTITY_TYPES = {\n",
    "    \"NAME\": fake.name,\n",
    "    \"ADDRESS\": lambda: fake.address().replace('\\n', ', '),\n",
    "    \"EMAIL\": fake.email,\n",
    "    \"PHONE_NUMBER\": fake.phone_number,\n",
    "}\n",
    "\n",
    "# Define templates using a simplified syntax\n",
    "# Define sentence templates with placeholders\n",
    "# The script will randomly select one of these structures\n",
    "#Loading file with generated templates containing placeholders for sensetive data\n",
    "templates_file = \"./generated_templates_v2.json\"\n",
    "with open(templates_file, 'r') as infile:\n",
    "    TEMPLATES = json.load(infile)\n",
    "\n",
    "def generate_sentence_with_spans():\n",
    "    \"\"\"\n",
    "    Generates a sentence and returns:\n",
    "    1. The full raw text.\n",
    "    2. A list of entities with their exact character start/end positions.\n",
    "    \"\"\"\n",
    "    template = random.choice(TEMPLATES)\n",
    "    \n",
    "    # Find all placeholders like {NAME}\n",
    "    placeholders = list(re.finditer(r'\\{(.*?)\\}', template))\n",
    "    \n",
    "    current_text = \"\"\n",
    "    last_pos = 0\n",
    "    entities = [] # Stores {'type': 'NAME', 'start': 5, 'end': 15}\n",
    "    \n",
    "    # Build the string piece by piece to track indices\n",
    "    for match in placeholders:\n",
    "        # Add the text BEFORE the entity\n",
    "        pre_text = template[last_pos:match.start()]\n",
    "        current_text += pre_text\n",
    "        \n",
    "        # Generate the entity value\n",
    "        entity_type = match.group(1)\n",
    "        entity_value = ENTITY_TYPES[entity_type]()\n",
    "        \n",
    "        # Record the start index of the entity\n",
    "        start_index = len(current_text)\n",
    "        \n",
    "        # Add the entity value\n",
    "        current_text += entity_value\n",
    "        \n",
    "        # Record the end index\n",
    "        end_index = len(current_text)\n",
    "        \n",
    "        # Store metadata\n",
    "        entities.append({\n",
    "            \"type\": entity_type,\n",
    "            \"start\": start_index,\n",
    "            \"end\": end_index,\n",
    "            \"value\": entity_value\n",
    "        })\n",
    "        \n",
    "        last_pos = match.end()\n",
    "        \n",
    "    # Add any remaining text after the last entity\n",
    "    current_text += template[last_pos:]\n",
    "    \n",
    "    return current_text, entities\n",
    "\n",
    "def generate_bert_train_data(num_sentences, output_file):\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "        for _ in range(num_sentences):\n",
    "            # 1. Generate Raw Text & Spans\n",
    "            text, entity_spans = generate_sentence_with_spans()\n",
    "            \n",
    "            # 2. Tokenize with Offset Mapping\n",
    "            # return_offsets_mapping=True gives us (start_char, end_char) for every token\n",
    "            encoding = tokenizer(text, return_offsets_mapping=True, truncation=True)\n",
    "            \n",
    "            tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "            offsets = encoding[\"offset_mapping\"]\n",
    "            \n",
    "            # 3. Align Labels\n",
    "            # We skip [CLS] (index 0) and [SEP] (last index) for the file output\n",
    "            # though usually we keep them for internal arrays. \n",
    "            # For CoNLL format, we usually want just the words.\n",
    "            \n",
    "            lines_to_write = []\n",
    "            \n",
    "            for i in range(1, len(tokens) - 1): # Skip [CLS] and [SEP]\n",
    "                token = tokens[i]\n",
    "                start_char, end_char = offsets[i]\n",
    "                \n",
    "                # Default Label\n",
    "                label = \"O\"\n",
    "                \n",
    "                # Check if this token falls inside any generated entity\n",
    "                for ent in entity_spans:\n",
    "                    # Overlap logic: \n",
    "                    # If the token's start/end lies within the entity's start/end\n",
    "                    if start_char >= ent[\"start\"] and end_char <= ent[\"end\"]:\n",
    "                        \n",
    "                        # Determine B or I\n",
    "                        # If this token starts at the exact beginning of the entity...\n",
    "                        if start_char == ent[\"start\"]:\n",
    "                            label = f\"B-{ent['type']}\"\n",
    "                        else:\n",
    "                            label = f\"I-{ent['type']}\"\n",
    "                        break # Found the entity, stop checking\n",
    "                \n",
    "                lines_to_write.append(f\"{token} {label}\")\n",
    "            \n",
    "            # 4. Write to file\n",
    "            for line in lines_to_write:\n",
    "                f.write(line + \"\\n\")\n",
    "                #print(line) # Preview\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# --- Execution ---\n",
    "print(\"Generating Training Data...\\n\")\n",
    "generate_bert_train_data(num_sentences=1000, output_file=DATASET)\n",
    "print(\"--- Generation Complete ---\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune BERT for NER"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pBs8zKpGFIMK",
    "tags": []
   },
   "source": [
    "import json\n",
    "#import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load generated train dataset\n",
    "with open(DATASET, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = f.readlines()\n",
    "\n",
    "# Parse dataset into sentences and labels\n",
    "def process_conll_data(raw_lines):\n",
    "    sentences, labels = [], []\n",
    "    current_sent, current_lab = [], []\n",
    "\n",
    "    for line in raw_lines:\n",
    "        parts = line.split() # Splits on any whitespace; returns [] if line is empty\n",
    "        \n",
    "        if not parts:\n",
    "            if current_sent: # Only append if we have accumulated data\n",
    "                sentences.append(current_sent)\n",
    "                labels.append(current_lab)\n",
    "                current_sent, current_lab = [], []\n",
    "        else:\n",
    "            current_sent.append(parts[0])\n",
    "            current_lab.append(parts[1])\n",
    "\n",
    "    # Flush the last buffer if the file didn't end with a newline\n",
    "    if current_sent:\n",
    "        sentences.append(current_sent)\n",
    "        labels.append(current_lab)\n",
    "        \n",
    "    return sentences, labels\n",
    "\n",
    "sentences, labels = process_conll_data(raw_data)\n",
    "\n",
    "# Create label mapping\n",
    "unique_labels = ['O',\n",
    "                 'B-ADDRESS',\n",
    "                 'I-ADDRESS',\n",
    "                 'B-PHONE_NUMBER',\n",
    "                 'I-PHONE_NUMBER',\n",
    "                 'B-NAME',\n",
    "                 'I-NAME',\n",
    "                 'B-EMAIL',\n",
    "                 'I-EMAIL']\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Base BERT model\n",
    "model_name = \"google-bert/bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_and_align_labels(sentences, labels):\n",
    "    tokenized_inputs = tokenizer(sentences, truncation=True, padding=True, is_split_into_words=True)\n",
    "    aligned_labels = []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignore special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])  # First subword gets the label\n",
    "            else:\n",
    "                label_ids.append(-100)  # Other subwords get -100\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        aligned_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert into Hugging Face dataset format\n",
    "train_data = Dataset.from_dict(tokenize_and_align_labels(train_texts, train_labels))\n",
    "test_data = Dataset.from_dict(tokenize_and_align_labels(test_texts, test_labels))\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_data, \"test\": test_data})\n",
    "\n",
    "# Count the number of unique labels from your dataset\n",
    "print(f\"labels:[{unique_labels}]\")\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "# Load the model with the correct label mappings\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",  # Disable all logging integrations\n",
    ")\n",
    "\n",
    "# Compute metrics\n",
    "# Load metric using evaluate library\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id2label[label] for label in label_seq if label != -100]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    pred_labels = [\n",
    "        [id2label[pred] for pred, lbl in zip(pred_seq, label_seq) if lbl != -100]\n",
    "        for pred_seq, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Compute metrics with zero_division handling\n",
    "    results = metric.compute(predictions=pred_labels, references=true_labels, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Saving finetuned model\n",
    "model_directory = FINETUNED_MODEL_PATH\n",
    "trainer.save_model(model_directory)\n",
    "tokenizer.save_pretrained(model_directory)\n",
    "\n",
    "#Upload finetuned model to Google Storage\n",
    "upload_to_gcs(model_directory, gcs_bucket)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NtwWY-WAO_Px"
   },
   "source": [
    "# Quick test for predictions\n",
    "#import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./ner_finetuned_v2\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "#print(model)\n",
    "\n",
    "# Create a Named Entity Recognition (NER, technically \"Token Classification\") pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")\n",
    "\n",
    "# Function to predict entities in text\n",
    "def predict_entities(text):\n",
    "    predictions = ner_pipeline(text)\n",
    "    return predictions\n",
    "\n",
    "# Example input text\n",
    "text = \"Send the invoice to 123 Main Street, New York. Contact John Connnor at johnconnor@example.com\"\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predict_entities(text)\n",
    "print(predictions)\n",
    "# Print the results\n",
    "print(\"Raw predictions:\")\n",
    "for entity in predictions:\n",
    "    print(\"-\"*80)\n",
    "    print(f\"\"\"Entity: {entity['word']}, \n",
    "          Type: {entity['entity_group']}, \n",
    "          Confidence: {entity['score']:.4f}, \n",
    "          Position: ({entity['start']}, {entity['end']})\"\"\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "def mask_sensetive_data(text, predictions):\n",
    "    prediction_sorted = sorted(predictions, key=lambda x: x['start'])\n",
    "    merged = []\n",
    "    redacted_text = text\n",
    "\n",
    "    for mask in predictions: #\n",
    "        text_to_mask = text[int(mask['start']): int(mask['end'])]\n",
    "        if len(text_to_mask)>0:\n",
    "            redacted_text = redacted_text.replace(text_to_mask, f\"[{mask['entity_group']}]\")\n",
    "    return {\"text\": text, \"redacted_text\": redacted_text}\n",
    "\n",
    "# Load test data\n",
    "input_texts = [\n",
    "    \"My phone number is 001-863-838-7300x0830.\",\n",
    "    \"My name is Jessica Williams.\",\n",
    "    \"My name is Gabriel Ryan.\",\n",
    "    \"My address is 150 Cortez Station Apt. 561, South Amberburgh, OH 44484.\",\n",
    "    \"The meeting with Maria Garcia from Google will be held at their office located at 48 Pirrama Road, Pyrmont NSW 2009.\"\n",
    "]\n",
    "\n",
    "# # Run inference\n",
    "results = [mask_sensetive_data(text, predict_entities(text)) for text in input_texts]\n",
    "\n",
    "print(\"\\n\\n output for test records:\")\n",
    "print(\"-------------------\")\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6oQl6ywSI4Ux"
   },
   "source": [
    "####\n",
    "# Inference pipeline V1 based on Vertex AI online Endpoint\n",
    "####\n",
    "#Setting Google Cloud env variables\n",
    "%env PROJECT_ID=oleksandr-demo\n",
    "%env LOCATION=us-central1\n",
    "%env CONTAINER_URI=us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-pytorch-inference-cu121.2-2.transformers.4-44.ubuntu2204.py311"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W-3qACl2I9gj"
   },
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_BepWtAAJPG3"
   },
   "source": [
    "#Uploading model to Vertex AI models\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"bert-ner-demo-v2\",\n",
    "    serving_container_image_uri=os.getenv(\"CONTAINER_URI\"),\n",
    "    artifact_uri=\"gs://bert-finetuning-ner-demo\",\n",
    "    serving_container_environment_variables={\n",
    "        \"HF_MODEL_DIR\": \"/tmp/models\",\n",
    "        \"HF_TASK\": \"token-classification\",\n",
    "    },\n",
    ")\n",
    "model.wait()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y5MMEmWVJVzh"
   },
   "source": [
    "#Creating Vertex AI Endpoint\n",
    "endpoint = aiplatform.Endpoint.create(display_name=\"bert-ner-demo-endpoint-v3\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qI1fK8c1JYs9"
   },
   "source": [
    "#Deploying model to Vertex AI Endpoint\n",
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    machine_type=\"n2-standard-4\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o6tpgnNaAT1q"
   },
   "source": [
    "print(endpoint.display_name)\n",
    "print(endpoint.name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "display_name=\"bert-ner-demo-endpoint-v3\"\n",
    "# List endpoints with a filter\n",
    "# We use the filter string to search specifically for the display_name\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f'display_name=\"{display_name}\"',\n",
    "    order_by=\"create_time desc\" # Optional: Get the most recently created one if duplicates exist\n",
    ")\n",
    "deployed_model = endpoints[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3Xo2IWfaJbQx"
   },
   "source": [
    "output = deployed_model.predict(instances=[\n",
    "    \"Send the invoice to 123 Main Street, New York. Contact John Connnor at johnconnor@example.com.\",\n",
    "    \"The meeting with Maria Garcia will be held at their office located at 48 Pirrama Road, Pyrmont NSW 2009.\"\n",
    "    ])\n",
    "\n",
    "print(\"RAW Predictions\")\n",
    "print(\"-\"*80)\n",
    "print(output)\n",
    "print(\"-\"*80)\n",
    "\n",
    "import re\n",
    "\n",
    "def _get_core_type(pred):\n",
    "    \"\"\"\n",
    "    Helper to extract 'PHONE' from 'B-PHONE', 'I-PHONE', or just 'PHONE'\n",
    "    Handles keys 'entity', 'entity_group', or 'label'.\n",
    "    \"\"\"\n",
    "    label = pred.get('entity_group') or pred.get('entity') or pred.get('label')\n",
    "    if not label: return \"UNKNOWN\"\n",
    "\n",
    "    # Strip B- or I- prefixes\n",
    "    if \"-\" in label and (label.startswith(\"B-\") or label.startswith(\"I-\")):\n",
    "        return label.split(\"-\", 1)[1]\n",
    "    return label\n",
    "\n",
    "def postprocess_predictions(predictions, merge_distance=1):\n",
    "    \"\"\"\n",
    "    Merges adjacent entities of the same type.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of dicts returned by the pipeline.\n",
    "        merge_distance: Max characters allowed between entities to merge them.\n",
    "                        0 = must be touching (e.g., \"123\" + \"-\")\n",
    "                        1 = allows spaces (e.g., \"John\" + \" \" + \"Doe\")\n",
    "    \"\"\"\n",
    "    if not predictions:\n",
    "        return []\n",
    "\n",
    "    # 1. Sort by start index to ensure processing order\n",
    "    sorted_preds = sorted(predictions, key=lambda x: x['start'])\n",
    "    \n",
    "    merged = []\n",
    "    \n",
    "    # Initialize the first entity\n",
    "    # We strip 'B-' or 'I-' to compare the core type (e.g., \"PHONE\")\n",
    "    first_pred = sorted_preds[0]\n",
    "    current_group = {\n",
    "        \"entity_group\": _get_core_type(first_pred),\n",
    "        \"score\": first_pred['score'],\n",
    "        \"word\": first_pred['word'],\n",
    "        \"start\": first_pred['start'],\n",
    "        \"end\": first_pred['end']\n",
    "    }\n",
    "\n",
    "    for next_pred in sorted_preds[1:]:\n",
    "        next_type = _get_core_type(next_pred)\n",
    "        \n",
    "        # Calculate gap between current end and next start\n",
    "        gap = int(next_pred['start'] - current_group['end'])\n",
    "        \n",
    "        # MERGE CONDITION:\n",
    "        # 1. Same Entity Type (e.g. PHONE == PHONE)\n",
    "        # 2. Adjacent or close enough (gap <= threshold)\n",
    "        if next_type == current_group['entity_group'] and gap <= merge_distance:\n",
    "            \n",
    "            # Update End Position\n",
    "            current_group['end'] = next_pred['end']\n",
    "            \n",
    "            # Merge text safely\n",
    "            # If there is a space in the original text (gap > 0), add it back\n",
    "            # Also handle BERT subwords (remove '##' if present)\n",
    "            sep = \" \" * gap # Reconstruct space if gap exists\n",
    "            clean_word = next_pred['word'].replace(\"##\", \"\")\n",
    "            \n",
    "            # If the previous word ended with a subword marker (rare but possible), handle it\n",
    "            # But usually we just append\n",
    "            current_group['word'] += sep + clean_word\n",
    "            \n",
    "            # Update Score: You can take Max or Average\n",
    "            current_group['score'] = max(current_group['score'], next_pred['score'])\n",
    "            \n",
    "        else:\n",
    "            # NO MERGE: Push current and start new\n",
    "            merged.append(current_group)\n",
    "            \n",
    "            current_group = {\n",
    "                \"entity_group\": next_type,\n",
    "                \"score\": next_pred['score'],\n",
    "                \"word\": next_pred['word'],\n",
    "                \"start\": next_pred['start'],\n",
    "                \"end\": next_pred['end']\n",
    "            }\n",
    "\n",
    "    # Append the final group\n",
    "    merged.append(current_group)\n",
    "    \n",
    "    # Final cleanup of words (in case the very first word had ##)\n",
    "    for m in merged:\n",
    "        m['word'] = m['word'].replace(\"##\", \"\")\n",
    "        \n",
    "    return merged\n",
    "print(\"Postprocessed Predictions\")\n",
    "print(\"-\"*80)\n",
    "for predictions in output.predictions:\n",
    "    print(postprocess_predictions(predictions))\n",
    "print(\"-\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s1qUj4teJd2I"
   },
   "source": [
    "# deployed_model.undeploy_all()\n",
    "# deployed_model.delete()\n",
    "# model.delete()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Apache Beam 2.69.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.69.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

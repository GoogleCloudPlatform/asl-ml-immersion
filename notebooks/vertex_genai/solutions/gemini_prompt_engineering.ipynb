{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02aa307c-a2b7-4927-98f5-51b27b908ba0",
   "metadata": {},
   "source": [
    "# Prompt Engineering with LLMs using Gemini\n",
    "\n",
    "**Learning Objective**\n",
    "\n",
    "1. Learn how query the Vertex Gemini API\n",
    "1. Learn how to setup the Gemini API parameters \n",
    "1. Learn prompt engineering for text generation\n",
    "1. Learn prompt engineering for chat applications\n",
    "\n",
    "\n",
    "The Vertex AI Gemini API lets you test, customize, and deploy instances of Google's Gemini large language models (LLM) so that you can leverage the capabilities of Gemini in your applications. The Gemini family of models supports text completion, multi-turn chat, and text embeddings generation.\n",
    "\n",
    "This notebook will provide examples of accessing pre-trained Gemini models with the API for use cases like text classification, summarization, extraction, and chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30622e-7ae5-4092-bebf-80ecd3b874f6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee081e-f7d1-44a8-8bee-57a3b6fbd750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da340d5-828a-48cf-a030-e0596a918050",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "The cell below implements the helper function `generate_content` to generate responses from the Gemini API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6954a-a9ca-41dd-aa61-e6538e514f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt,\n",
    "    model_name=\"gemini-1.0-pro\",\n",
    "):\n",
    "    model = GenerativeModel(model_name)\n",
    "    responses = model.generate_content(prompt, stream=True)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cff7e-8203-4ec8-8dbd-82dd14e3b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = generate(\n",
    "    \"What are five important things to understand about large language models?\"\n",
    ")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e32f10-3419-4f9b-9be2-334b4e1b0f01",
   "metadata": {},
   "source": [
    "### Text Classification \n",
    "\n",
    "Now that we've tested our wrapper function, let us explore prompting for classification. Text classification is a common machine learning use-case and is frequently used for tasks like spam detection, sentiment analysis, topic classification, and more. \n",
    "\n",
    "Both **zero-shot** and **few-shot** prompting are common with text classification use cases. Zero-shot prompting is where you do not provide examples with labels in the input, and few-shot prompting is where you provide (a few) examples in the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095984db-e3f1-431c-8464-6ed9c745b380",
   "metadata": {},
   "source": [
    "Let us start with zero-shot classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc027f3b-84ec-46ab-95b1-5d6afd6d1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Classify the following: \n",
    "text: \"Zero-shot prompting is really easy with Google's Gemini API.\"\n",
    "label: technology, politics, sports \n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d73f1-11ce-4581-90da-d69e76b57247",
   "metadata": {},
   "source": [
    "Here is now an example of a few-shot prompting for classification. Along with increasing the accuracy of your model, few-shot prompting gives you a certain control over the output format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9c371-dc6c-4ef7-98ad-dc8058b88bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What is the topic for a given text? \n",
    "- cats \n",
    "- dogs \n",
    "\n",
    "Text: They always sits on my lap and purr!\n",
    "The answer is: cats\n",
    "\n",
    "Text: They love to play fetch\n",
    "The answer is: dogs\n",
    "\n",
    "Text: I throw the frisbee in the water and they swim for hours! \n",
    "The answer is: dogs \n",
    "\n",
    "Text: They always knock things off my counter!\n",
    "The answer is:\n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff60e3-2e6d-4d70-988b-4940adb67c13",
   "metadata": {},
   "source": [
    "### Text Summarization\n",
    "\n",
    "LLMs can also be used for text summarization use cases. Text summarization produces a concise and fluent summary of a longer text document. The prompt inn the cell below simply instruct Gemini to summarize a given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04163aa1-b750-4033-93b3-c08e572872c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Provide a very short summary for the following:\n",
    "\n",
    "A transformer is a deep learning model. It is distinguished by its adoption of self-attention, \n",
    "differentially weighting the significance of each part of the input (which includes the \n",
    "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to \n",
    "process sequential input data, such as natural language, with applications towards tasks such \n",
    "as translation and text summarization. However, unlike RNNs, transformers process the \n",
    "entire input all at once. The attention mechanism provides context for any position in the \n",
    "input sequence. For example, if the input data is a natural language sentence, the transformer \n",
    "does not have to process one word at a time. This allows for more parallelization than RNNs \n",
    "and therefore reduces training times.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837c16a-67cc-4bdb-ae66-4882d61279de",
   "metadata": {},
   "source": [
    "If you need the summary to be in a certain way, as for instance a bullet point summary, you can instruct Gemini to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d03ec-ae6c-4d47-8d2e-09ebec182d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Provide four bullet points summarizing the following:\n",
    "\n",
    "A transformer is a deep learning model. It is distinguished by its adoption of self-attention, \n",
    "differentially weighting the significance of each part of the input (which includes the \n",
    "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to \n",
    "process sequential input data, such as natural language, with applications towards tasks such \n",
    "as translation and text summarization. However, unlike RNNs, transformers process the \n",
    "entire input all at once. The attention mechanism provides context for any position in the \n",
    "input sequence. For example, if the input data is a natural language sentence, the transformer \n",
    "does not have to process one word at a time. This allows for more parallelization than RNNs \n",
    "and therefore reduces training times.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df5150-b6ff-499e-adad-6ae6d5cc7b0c",
   "metadata": {},
   "source": [
    "Dialog summarization falls under the category of text summarization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594633f-ee1b-4a18-a11c-429b80e512e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Generate a one-liner summary of the following chat and at the end, summarize to-do's for the service rep: \n",
    "\n",
    "Customer: Hi! I'm reaching out to customer service because I am having issues.\n",
    "\n",
    "Service Rep: What seems to be the problem? \n",
    "\n",
    "Customer: I am trying to use the Gemini API but I keep getting an error. \n",
    "\n",
    "Service Rep: Can you share the error with me? \n",
    "\n",
    "Customer: Sure. The error says: \"ResourceExhausted: 429 Quota exceeded for \n",
    "      aiplatform.googleapis.com/online_prediction_requests_per_base_model \n",
    "      with base model: text-bison\"\n",
    "      \n",
    "Service Rep: It looks like you have exceeded the quota for usage. Please refer to \n",
    "             https://cloud.google.com/vertex-ai/docs/quotas for information about quotas\n",
    "             and limits. \n",
    "             \n",
    "Customer: Can you increase my quota?\n",
    "\n",
    "Service Rep: I cannot, but let me follow up with somebody who will be able to help.\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd094d3-3026-4ff3-9306-357c50bc921d",
   "metadata": {},
   "source": [
    "### Text Extraction \n",
    "Gemini can be used to extract and structure text. Text extraction can be used for a variety of purposes. One common purpose is to convert documents into a machine-readable format. This can be useful for storing documents in a database or for processing documents with software. Another common purpose is to extract information from documents. This can be useful for finding specific information in a document or for summarizing the content of a document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003d56b-67cc-4dc5-9a29-288b5043af71",
   "metadata": {},
   "source": [
    "Let us start with zero-shot extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b19b1-79cb-400a-87c7-6b13fe756c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Extract the ingredients from the following recipe. \n",
    "Return the ingredients in JSON format with keys: ingredient, quantity, type.\n",
    "\n",
    "Ingredients:\n",
    "* 1 tablespoon olive oil\n",
    "* 1 onion, chopped\n",
    "* 2 carrots, chopped\n",
    "* 2 celery stalks, chopped\n",
    "* 1 teaspoon ground cumin\n",
    "* 1/2 teaspoon ground coriander\n",
    "* 1/4 teaspoon turmeric powder\n",
    "* 1/4 teaspoon cayenne pepper (optional)\n",
    "* Salt and pepper to taste\n",
    "* 1 (15 ounce) can black beans, rinsed and drained\n",
    "* 1 (15 ounce) can kidney beans, rinsed and drained\n",
    "* 1 (14.5 ounce) can diced tomatoes, undrained\n",
    "* 1 (10 ounce) can diced tomatoes with green chilies, undrained\n",
    "* 4 cups vegetable broth\n",
    "* 1 cup chopped fresh cilantro\n",
    "\"\"\"\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669e27e-3bfe-4c95-8c11-dd424cc8a735",
   "metadata": {},
   "source": [
    "As for classification, few-shot prompting gives you more control on the format of what is extracted: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc370e-b071-4377-b9c0-10e53c76c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Extract the technical specifications from the text below in JSON format.\n",
    "\n",
    "Text: Google Nest WiFi, network speed up to 1200Mpbs, 2.4GHz and 5GHz frequencies, WP3 protocol\n",
    "JSON: {\n",
    "  \"product\":\"Google Nest WiFi\",\n",
    "  \"speed\":\"1200Mpbs\",\n",
    "  \"frequencies\": [\"2.4GHz\", \"5GHz\"],\n",
    "  \"protocol\":\"WP3\"\n",
    "}\n",
    "\n",
    "Text: Google Pixel 7, 5G network, 8GB RAM, Tensor G2 processor, 128GB of storage, Lemongrass\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdb835-a706-4f00-8f19-972f3fd447e4",
   "metadata": {},
   "source": [
    "## Prompt engineering for chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba2c36-d280-433f-b124-0106c8ce2c9c",
   "metadata": {},
   "source": [
    "The Vertex AI Gemini API for chat is optimized for multi-turn chat. Multi-turn chat is when a model tracks the history of a chat conversation and then uses that history as the context for responses.\n",
    "\n",
    "Gemini enables you to have freeform conversations across multiple turns. The ChatSession class simplifies the process by managing the state of the conversation, so unlike with generate_content, you do not have to store the conversation history as a list.\n",
    "\n",
    "Let's initialize the chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fb4f8-df82-43c6-acc6-d93854d18c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "chat = model.start_chat(history=[])\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239950b-cb28-4cb5-aedf-0278ed356e06",
   "metadata": {},
   "source": [
    "The ChatSession.send_message method returns the same GenerateContentResponse type as GenerativeModel.generate_content. It also appends your message and the response to the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f03267-2469-4b9a-9f37-eef780a2feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    \"In one sentence, explain how a computer works to a young child.\"\n",
    ")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825730b6-168f-455b-9ac3-f98a04e5dee2",
   "metadata": {},
   "source": [
    "Recall that within a chat session, history is preserved. This enables the model to remember things within a given chat session for context. You can see this history in the `history` attribute of the chat session object. Notice that the history is simply a list of previous input/output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526265fd-85ff-452e-94ed-906d690361b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d279ee-4ac7-4b7e-901e-261d4b88191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"What question did I ask you last?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86a0c0-5668-4efa-8049-1eb7b4d56cdc",
   "metadata": {},
   "source": [
    "### Adding Context\n",
    "\n",
    "While the `ChatSession` class shown earlier can handle many use cases, it does make some assumptions. If your use case doesn't fit into this chat implementation it's good to remember that `ChatSession` is just a wrapper around `GenerativeModel.generate_content`. In addition to single requests, it can handle multi-turn conversations.\n",
    "\n",
    "The individual messages are `protos.Content objects` or compatible dictionaries, as seen in previous sections. As a dictionary, the message requires role and parts keys. The role in a conversation can either be the user, which provides the prompts, or model, which provides the responses.\n",
    "\n",
    "Pass a list of `protos.Content` objects and it will be treated as multi-turn chat:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b641bdf-fca4-46a6-aad5-e310456f6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-1.5-flash\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"Briefly explain how a computer works to a young child.\"],\n",
    "    }\n",
    "]\n",
    "responses = model.generate_content(str(messages), stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da257d4-e31e-45ed-884b-fac663862e30",
   "metadata": {},
   "source": [
    "To continue the conversation, add the response and another message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a4cc8-412b-4f4e-83ce-2c0654ded06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\"role\": \"model\", \"parts\": [response.text]})\n",
    "\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\n",
    "            \"Okay, how about a more detailed explanation to a high school student?\"\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "responses = model.generate_content(str(messages), stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062fa76-69b4-461c-b2ff-37ace5427fd8",
   "metadata": {},
   "source": [
    "### Adding chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0a97a-fe2c-498c-81bb-2ca4164a7d8e",
   "metadata": {},
   "source": [
    "You can add chat history to a chat by adding messages from role user and model alternately. System messages can be set in the first part for the first message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b9209-300b-4107-87bc-36a7c195ae32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat2 = model.start_chat(\n",
    "    history=[\n",
    "        Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                Part.from_text(\n",
    "                    \"\"\"\n",
    "    My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\n",
    "    Who do you work for?\n",
    "    \"\"\"\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "        Content(role=\"model\", parts=[Part.from_text(\"I work for Ned.\")]),\n",
    "        Content(role=\"user\", parts=[Part.from_text(\"What do I like?\")]),\n",
    "        Content(\n",
    "            role=\"model\", parts=[Part.from_text(\"Ned likes watching movies.\")]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = chat2.send_message(\"Are my favorite movies based on a book series?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55819d-862e-41ef-be09-0fbc6df3f0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat2.send_message(\"When were these books published?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ff364-3db0-49e7-a87e-4a2681595aa5",
   "metadata": {},
   "source": [
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

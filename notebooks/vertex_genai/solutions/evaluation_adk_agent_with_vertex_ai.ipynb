{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5646d634-70b1-4a6b-b53a-8702bcd57894",
   "metadata": {},
   "source": [
    "# Evaluate your AI Agent using Vertex AI Gen AI Evaluation service\n",
    "## Overview\n",
    "\n",
    "This notebook guides you on how to evaluate an ADK (Agent Development Kit) agent using Vertex AI Gen AI Evaluation for agent evaluation.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "* Setup local ADK agent for evaluation with Vertex AI Gen AI Evaluation service\n",
    "* Prepare Agent Evaluation dataset\n",
    "* Set up and use single-tool usage evaluation\n",
    "* Use the Trajectory evaluation\n",
    "* Use the Response evaluation\n",
    "* Bring-Your-Own-Dataset (BYOD) section shows you how to evaluate agent by providing an evaluation dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Vertex Generative AI Evaluation Service\n",
    "The Vertex Gen AI evaluation service provides enterprise-grade tools for objective, data-driven assessment of generative AI models and AI Agents. \n",
    "It supports and informs a number of development tasks like model migrations, prompt editing, fine-tuning and AI agent evaluation.\n",
    "While ADK provides its own built-in evaluation module, this notebook demonstrates how to use the Vertex AI Generative AI Evaluation Service to assess the performance of an ADK-based agent.\n",
    "This approach offers a broader, explainable, and quality-controlled toolkit to evaluate generative models or applications using custom metrics and human-aligned benchmarks.\n",
    "\n",
    "For more information, see the [Vertex AI Gen AI Evaluation service](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5ecd2-09e3-4801-9918-bbf861b62d3d",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "09d0fe85-302a-4ac6-afcb-360bb7764c3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "import asyncio\n",
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm  # For multi-model support\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from google.genai import types  # For creating message Content/Parts\n",
    "from IPython.display import HTML, Markdown, display\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e7b1b3e-6b91-42f4-9550-fe027efb1988",
   "metadata": {
    "tags": []
   },
   "source": [
    "LOCATION = \"us-central1\"\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\"  # Use Vertex AI API"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec87bb48-e10d-49f7-97da-4ffc52edd4a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%bash\n",
    "echo > adk_agents/.env \"GOOGLE_CLOUD_LOCATION=$GOOGLE_CLOUD_LOCATION\n",
    "GOOGLE_GENAI_USE_VERTEXAI=$GOOGLE_GENAI_USE_VERTEXAI\n",
    "\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9593b46-00f1-4af6-926d-e58017535000",
   "metadata": {
    "tags": []
   },
   "source": [
    "MODEL = \"gemini-2.0-flash\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12630b81-bbe3-4e54-8747-15ce5b74076d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define helper functions\n",
    "\n",
    "Initiate a set of plotting helper functions to visualize our evaluation results:\n",
    " - Grouped bar chart with specified evaluation metrics,\n",
    " - Displays a subset of rows including a drill-down view,\n",
    " - Plot the radar chart."
   ]
  },
  {
   "cell_type": "code",
   "id": "d76610c1-73e4-464f-99b2-8176f64a0ed9",
   "metadata": {
    "tags": []
   },
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "\n",
    "def plot_bar_plot(\n",
    "    eval_result: pd.DataFrame, title: str, metrics: list[str] = None\n",
    ") -> None:\n",
    "    \"\"\"Displays a grouped bar chart with specified evaluation metrics from a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "\n",
    "    summary_metrics = eval_result.summary_metrics\n",
    "    if metrics:\n",
    "        summary_metrics = {\n",
    "            k: summary_metrics[k]\n",
    "            for k, v in summary_metrics.items()\n",
    "            if any(selected_metric in k for selected_metric in metrics)\n",
    "        }\n",
    "\n",
    "    data.append(\n",
    "        go.Bar(\n",
    "            x=list(summary_metrics.keys()),\n",
    "            y=list(summary_metrics.values()),\n",
    "            name=title,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data)\n",
    "\n",
    "    # Change the bar mode\n",
    "    fig.update_layout(barmode=\"group\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def format_output_as_markdown(output: dict) -> str:\n",
    "    \"\"\"Convert the output dictionary to a formatted markdown string.\"\"\"\n",
    "    markdown = \"### AI Response\\n\"\n",
    "    markdown += f\"{output['response']}\\n\\n\"\n",
    "\n",
    "    if output[\"predicted_trajectory\"]:\n",
    "        output[\"predicted_trajectory\"] = json.loads(\n",
    "            output[\"predicted_trajectory\"]\n",
    "        )\n",
    "        markdown += \"### Function Calls\\n\"\n",
    "        for call in output[\"predicted_trajectory\"]:\n",
    "            markdown += f\"- **Function**: `{call['tool_name']}`\\n\"\n",
    "            markdown += \"  - **Arguments**:\\n\"\n",
    "            for key, value in call[\"tool_input\"].items():\n",
    "                markdown += f\"    - `{key}`: `{value}`\\n\"\n",
    "\n",
    "    return markdown\n",
    "\n",
    "\n",
    "def display_dataframe_rows(\n",
    "    df: pd.DataFrame,\n",
    "    columns: list[str] | None = None,\n",
    "    num_rows: int = 3,\n",
    "    display_drilldown: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Displays a subset of rows from a DataFrame, optionally including a drill-down view.\"\"\"\n",
    "\n",
    "    if columns:\n",
    "        df = df[columns]\n",
    "\n",
    "    base_style = \"font-family: monospace; font-size: 14px; white-space: pre-wrap; width: auto; overflow-x: auto;\"\n",
    "    header_style = base_style + \"font-weight: bold;\"\n",
    "\n",
    "    for _, row in df.head(num_rows).iterrows():\n",
    "        for column in df.columns:\n",
    "            display(\n",
    "                HTML(\n",
    "                    f\"<span style='{header_style}'>{column.replace('_', ' ').title()}: </span>\"\n",
    "                )\n",
    "            )\n",
    "            display(\n",
    "                HTML(f\"<span style='{base_style}'>{row[column]}</span><br>\")\n",
    "            )\n",
    "\n",
    "        display(HTML(\"<hr>\"))\n",
    "\n",
    "        if (\n",
    "            display_drilldown\n",
    "            and \"predicted_trajectory\" in df.columns\n",
    "            and \"reference_trajectory\" in df.columns\n",
    "        ):\n",
    "            display_drilldown(row)\n",
    "\n",
    "\n",
    "def display_radar_plot(eval_results, title: str, metrics=None):\n",
    "    \"\"\"Plot the radar plot.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    summary_metrics = eval_results.summary_metrics\n",
    "    if metrics:\n",
    "        summary_metrics = {\n",
    "            k: summary_metrics[k]\n",
    "            for k, v in summary_metrics.items()\n",
    "            if any(selected_metric in k for selected_metric in metrics)\n",
    "        }\n",
    "\n",
    "    min_val = 0  # = min(summary_metrics.values())\n",
    "    max_val = max(summary_metrics.values())\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(\n",
    "            r=list(summary_metrics.values()),\n",
    "            theta=list(summary_metrics.keys()),\n",
    "            fill=\"toself\",\n",
    "            name=title,\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[min_val, max_val])),\n",
    "        showlegend=True,\n",
    "    )\n",
    "    fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2d754cc-3499-4b81-81ec-4ba1f4da7a61",
   "metadata": {},
   "source": [
    "## Prepare an Agent files\n",
    "\n",
    "Here, let's reuse the basic agent files created in [building_agent_with_adk.ipynb](../../../../../../Downloads/building_agent_with_adk.ipynb) notebook.\n",
    "\n",
    "If you haven't run the notebook, executed the cells below to create files."
   ]
  },
  {
   "cell_type": "code",
   "id": "7a289a32-cfd2-48cf-abd5-edbc60c12b2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%writefile ./adk_agents/agent1_weather_lookup/tools.py\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Retrieves the current weather report for a specified city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the weather information.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'report' key with weather details.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
    "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
    "\n",
    "    # Mock weather data\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25째C.\"},\n",
    "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15째C.\"},\n",
    "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18째C.\"},\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        return mock_weather_db[city_normalized]\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd0d941f-7aee-4bb7-90d6-d6dbae11181d",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%writefile ./adk_agents/agent1_weather_lookup/agent.py\n",
    "from google.adk.agents import Agent\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "from .tools import get_weather\n",
    "\n",
    "root_agent = Agent(\n",
    "    name=\"weather_agent_v1\",\n",
    "    model=MODEL, # Can be a string for Gemini or a LiteLlm object\n",
    "    description=\"Provides weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use the 'get_weather' tool to find the information. \"\n",
    "                \"If the tool returns an error, inform the user politely. \"\n",
    "                \"If the tool is successful, present the weather report clearly.\",\n",
    "    tools=[get_weather], # Pass the function directly\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca24f126-5532-4331-ada0-a91a9b71fee9",
   "metadata": {
    "tags": []
   },
   "source": [
    "from adk_agents.agent1_weather_lookup import agent\n",
    "\n",
    "importlib.reload(agent)  # Force reload\n",
    "\n",
    "# Example tool usage (optional test)\n",
    "print(agent.get_weather(\"New York\"))\n",
    "print(agent.get_weather(\"Paris\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0110289e-fc75-4e42-8060-84e956f5efb7",
   "metadata": {},
   "source": [
    "### Setup Runner and Session Service\n",
    "\n",
    "To manage conversations and execute the agent, we need two more components:\n",
    "\n",
    "* `SessionService`: Responsible for managing conversation history and state for different users and sessions. The `InMemorySessionService` is a simple implementation that stores everything in memory, suitable for testing and simple applications. It keeps track of the messages exchanged. We'll explore state persistence more in Step 4\\.  \n",
    "* `Runner`: The engine that orchestrates the interaction flow. It takes user input, routes it to the appropriate agent, manages calls to the LLM and tools based on the agent's logic, handles session updates via the `SessionService`, and yields events representing the progress of the interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0f555-4559-400a-91d9-3b6b58786c7d",
   "metadata": {},
   "source": [
    "Let's define some constants first."
   ]
  },
  {
   "cell_type": "code",
   "id": "a52d1dc7-c6e7-45e5-be52-358934dfbcd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "APP_NAME = \"weather_info_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\"  # Using a fixed ID for simplicity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cad8e20d-a891-4b31-abe0-3e0860c22971",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define a function for Session and Runner"
   ]
  },
  {
   "cell_type": "code",
   "id": "9091b05f-19db-4027-9865-b876f9139043",
   "metadata": {
    "tags": []
   },
   "source": [
    "async def setup_session_and_runner():\n",
    "    session_service = InMemorySessionService()\n",
    "    example_session = await session_service.create_session(\n",
    "        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
    "    )\n",
    "\n",
    "    print(f\"--- Examining Session Properties ---\")\n",
    "    print(f\"ID (`id`):                {example_session.id}\")\n",
    "    print(f\"Application Name (`app_name`): {example_session.app_name}\")\n",
    "    print(f\"User ID (`user_id`):         {example_session.user_id}\")\n",
    "    print(\n",
    "        f\"State (`state`):           {example_session.state}\"\n",
    "    )  # Note: Only shows initial state here\n",
    "    print(\n",
    "        f\"Events (`events`):         {example_session.events}\"\n",
    "    )  # Initially empty\n",
    "    print(\n",
    "        f\"Last Update (`last_update_time`): {example_session.last_update_time:.2f}\"\n",
    "    )\n",
    "    print(f\"---------------------------------\")\n",
    "\n",
    "    runner = Runner(\n",
    "        agent=agent.root_agent,\n",
    "        app_name=APP_NAME,\n",
    "        session_service=session_service,\n",
    "    )\n",
    "    return example_session, runner"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d972f81-18ec-40a1-9137-392747cc1b70",
   "metadata": {},
   "source": [
    "### Interact with the Agent\n",
    "\n",
    "We need a way to send messages to our agent and receive its responses. Since LLM calls and tool executions can take time, ADK's `Runner` operates asynchronously.\n",
    "\n",
    "We'll define an `async` helper function (`call_agent_async`) that:\n",
    "\n",
    "1. Takes a user query string.  \n",
    "2. Packages it into the ADK `Content` format.  \n",
    "3. Calls `runner.run_async`, providing the user/session context and the new message.  \n",
    "4. Iterates through the **Events** yielded by the runner. Events represent steps in the agent's execution (e.g., tool call requested, tool result received, intermediate LLM thought, final response).  \n",
    "5. Identifies and prints the **final response** event using `event.is_final_response()`.\n",
    "\n",
    "**Why `async`?** Interactions with LLMs and potentially tools (like external APIs) are I/O-bound operations. Using `asyncio` allows the program to handle these operations efficiently without blocking execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a8bc70-bc5e-4b3b-8241-052d7456fdbb",
   "metadata": {},
   "source": [
    "### Run the Conversation\n",
    "\n",
    "Finally, let's test our setup by sending a few queries to the agent. We wrap our `async` calls in a main `async` function and run it using `await`.\n",
    "\n",
    "Watch the output:\n",
    "\n",
    "* See the user queries.  \n",
    "* Notice the `--- Tool: get_weather called... ---` logs when the agent uses the tool.  \n",
    "* Observe the agent's final responses, including how it handles the case where weather data isn't available (for Paris)."
   ]
  },
  {
   "cell_type": "code",
   "id": "e3845dd2-40f3-4f78-9689-f9d4118592d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "async def call_agent_async(query):\n",
    "\n",
    "    print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "    session, runner = await setup_session_and_runner()\n",
    "    events = runner.run_async(\n",
    "        user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
    "    )\n",
    "\n",
    "    final_response = \"\"\n",
    "    predicted_trajectory_list = []\n",
    "\n",
    "    async for event in events:\n",
    "        # Ensure content and parts exist before accessing them\n",
    "        if not event.content or not event.content.parts:\n",
    "            continue\n",
    "\n",
    "        # Iterate through ALL parts in the event's content\n",
    "        for part in event.content.parts:\n",
    "            if part.function_call:\n",
    "                tool_info = {\n",
    "                    \"tool_name\": part.function_call.name,\n",
    "                    \"tool_input\": dict(part.function_call.args),\n",
    "                }\n",
    "                # Ensure we don't add duplicates if the same call appears somehow\n",
    "                if tool_info not in predicted_trajectory_list:\n",
    "                    predicted_trajectory_list.append(tool_info)\n",
    "\n",
    "            # The final text response is usually in the last event from the model\n",
    "            if event.content.role == \"model\" and part.text:\n",
    "                # Overwrite response; the last text response found is likely the final one\n",
    "                final_response = part.text.strip()\n",
    "\n",
    "        if event.is_final_response():\n",
    "            final_response = event.content.parts[0].text\n",
    "            print(\"Agent Response: \", final_response)\n",
    "\n",
    "    # Dump the collected trajectory list into a JSON string\n",
    "    final_output = {\n",
    "        \"response\": str(final_response),\n",
    "        \"predicted_trajectory\": json.dumps(predicted_trajectory_list),\n",
    "    }\n",
    "    return final_output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28a45e38-154a-44f2-a16c-cf5db21c8d7b",
   "metadata": {},
   "source": [
    "## Evaluating a ADK agent with Vertex AI Gen AI Evaluation\n",
    "\n",
    "When working with AI agents, it's important to keep track of their performance and how well they're working. You can look at this in two main ways: **monitoring** and **observability**.\n",
    "\n",
    "Monitoring focuses on how well your agent is performing specific tasks:\n",
    "\n",
    "* **Single Tool Selection**: Is the agent choosing the right tools for the job?\n",
    "\n",
    "* **Multiple Tool Selection (or Trajectory)**: Is the agent making logical choices in the order it uses tools?\n",
    "\n",
    "* **Response generation**: Is the agent's output good, and does it make sense based on the tools it used?\n",
    "\n",
    "Observability is about understanding the overall health of the agent:\n",
    "\n",
    "* **Latency**: How long does it take the agent to respond?\n",
    "\n",
    "* **Failure Rate**: How often does the agent fail to produce a response?\n",
    "\n",
    "Vertex AI Gen AI Evaluation service helps you to assess all of these aspects both while you are prototyping the agent or after you deploy it in production. It provides [pre-built evaluation criteria and metrics](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval) so you can see exactly how your agents are doing and identify areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee0a18-8f55-4108-9af4-092c22ea3f1d",
   "metadata": {},
   "source": [
    "### Prepare Agent Evaluation dataset\n",
    "\n",
    "To evaluate your AI agent using the Vertex AI Gen AI Evaluation service, you need a specific dataset depending on what aspects you want to evaluate of your agent.  \n",
    "\n",
    "This dataset should include the prompts given to the agent. It can also contain the ideal or expected response (ground truth) and the intended sequence of tool calls the agent should take (reference trajectory) representing the sequence of tools you expect agent calls for each given prompt.\n",
    "\n",
    "Below you have an example of dataset you might have with a customer support agent with user prompt and the reference trajectory."
   ]
  },
  {
   "cell_type": "code",
   "id": "199305fa-5379-42bb-befa-35292bba22f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "eval_data = {\n",
    "    \"prompt\": [\n",
    "        \"Tell me the weather in New York\",\n",
    "        \"What is the weather like in London?\",\n",
    "        \"How about Paris?\",\n",
    "        \"Tell me the weather in New York and London?\",\n",
    "        \"What is the weather like in New York and London?\",\n",
    "        \"Tell me the weather in New York, London and Paris?\",\n",
    "    ],\n",
    "    \"reference_trajectory\": [\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"Paris\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            },\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            },\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"Paris\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            },\n",
    "        ],\n",
    "    ],\n",
    "}\n",
    "\n",
    "eval_sample_dataset = pd.DataFrame(eval_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f459d6a4-9531-423e-8709-6a2be8d4304d",
   "metadata": {},
   "source": [
    "Print some samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "e55ec8cc-b165-4d50-af55-d96b3c99788f",
   "metadata": {
    "tags": []
   },
   "source": [
    "display(eval_sample_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6cd47287-1138-4ac8-8a3d-37aa0e0c43e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "display_dataframe_rows(eval_sample_dataset, num_rows=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea76bb46-026a-4bae-ba83-2904c0a3e1d4",
   "metadata": {},
   "source": [
    "### Single tool usage evaluation\n",
    "\n",
    "After you've set your AI agent and the evaluation dataset, you start evaluating if the agent is choosing the correct single tool for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e38c5-26da-4ff7-a3e2-d5e636708847",
   "metadata": {},
   "source": [
    "#### Set single tool usage metrics\n",
    "\n",
    "The `trajectory_single_tool_use` metric in Vertex AI Gen AI Evaluation gives you a quick way to evaluate whether your agent is using the tool you expect it to use, regardless of any specific tool order. It's a basic but useful way to start evaluating if the right tool was used at some point during the agent's process.\n",
    "\n",
    "To use the `trajectory_single_tool_use` metric, you need to set what tool should have been used for a particular user's request. For example, if a user asks to \"send an email\", you might expect the agent to use an \"send_email\" tool, and you'd specify that tool's name when using this metric."
   ]
  },
  {
   "cell_type": "code",
   "id": "2cb095dc-3b31-479d-8d79-d6ed3ea2726e",
   "metadata": {
    "tags": []
   },
   "source": [
    "from vertexai.preview.evaluation import EvalTask\n",
    "from vertexai.preview.evaluation.metrics import (\n",
    "    PointwiseMetric,\n",
    "    PointwiseMetricPromptTemplate,\n",
    "    TrajectorySingleToolUse,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b1a3787-cc46-43bd-b77f-1aae7dddc39b",
   "metadata": {
    "tags": []
   },
   "source": [
    "single_tool_usage_metrics = [TrajectorySingleToolUse(tool_name=\"get_weather\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c899a092-2a8a-4900-af78-0a55eeb6547b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run an evaluation task\n",
    "\n",
    "To run the evaluation, you initiate an `EvalTask` using the pre-defined dataset (`eval_sample_dataset`) and metrics (`single_tool_usage_metrics` in this case) within an experiment. Then, you run the evaluation using agent_parsed_outcome function and assigns a unique identifier to this specific evaluation run, storing and visualizing the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "id": "983f4093-a008-484f-8a26-abc79025221f",
   "metadata": {
    "tags": []
   },
   "source": [
    "EXPERIMENT_NAME = \"evaluate-adk-agent-v1\"\n",
    "PROJECT = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET_NAME = f\"agent-evaluation-{PROJECT}-bucket\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c87e7c6107343da",
   "metadata": {},
   "source": [
    "**Checking for the existence of BUCKET. Creating it if it doesn't exist:**"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf2e4276d2ec2202",
   "metadata": {
    "tags": []
   },
   "source": [
    "!gsutil ls $BUCKET_URI || gsutil mb -l $LOCATION $BUCKET_URI"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "944b7f7f-4111-48a2-b9f0-e46a17b20eca",
   "metadata": {
    "tags": []
   },
   "source": [
    "import random\n",
    "import string\n",
    "import uuid\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def get_id() -> str:\n",
    "    \"\"\"Generate a uuid\"\"\"\n",
    "    return str(uuid.uuid4())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f3ad749-75f3-410c-be75-b490fe3ddc41",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we wrap the `call_agent_async` in a synchronous function so that we can pass it to the evaluation service."
   ]
  },
  {
   "cell_type": "code",
   "id": "616e239a-d0d2-42ec-9547-41aded136536",
   "metadata": {
    "tags": []
   },
   "source": [
    "def agent_parsed_outcome(query):\n",
    "    return asyncio.run(call_agent_async(query))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b67fb61-108a-48a3-97e1-e0b1a167e227",
   "metadata": {
    "tags": []
   },
   "source": [
    "EXPERIMENT_RUN = f\"single-metric-eval-{get_id()}\"\n",
    "\n",
    "single_tool_call_eval_task = EvalTask(\n",
    "    dataset=eval_sample_dataset,\n",
    "    metrics=single_tool_usage_metrics,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    output_uri_prefix=BUCKET_URI + \"/single-metric-eval\",\n",
    ")\n",
    "\n",
    "single_tool_call_eval_result = single_tool_call_eval_task.evaluate(\n",
    "    runnable=agent_parsed_outcome, experiment_run_name=EXPERIMENT_RUN\n",
    ")\n",
    "\n",
    "print(single_tool_call_eval_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "037071fd-3b2a-485e-84a1-5944e9947c3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "def display_eval_report(eval_result: pd.DataFrame) -> None:\n",
    "    \"\"\"Display the evaluation results.\"\"\"\n",
    "    metrics_df = pd.DataFrame.from_dict(\n",
    "        eval_result.summary_metrics, orient=\"index\"\n",
    "    ).T\n",
    "    display(Markdown(\"### Summary Metrics\"))\n",
    "    display(metrics_df)\n",
    "\n",
    "    display(Markdown(\"### Row-wise Metrics\"))\n",
    "    display(eval_result.metrics_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8fa0de6-48a7-4f39-ab0f-16aff59a8933",
   "metadata": {
    "tags": []
   },
   "source": [
    "display_eval_report(single_tool_call_eval_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "068b8c31-3707-4a4b-8f8e-127fcd0d89d4",
   "metadata": {},
   "source": [
    "### Trajectory Evaluation\n",
    "\n",
    "After evaluating the agent's ability to select the single most appropriate tool for a given task, you generalize the evaluation by analyzing the tool sequence choices with respect to the user input (trajectory). This assesses whether the agent not only chooses the right tools but also utilizes them in a rational and effective order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb18591-0265-4288-853b-0a6eeb760f73",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set trajectory metrics\n",
    "\n",
    "To evaluate agent's trajectory, Vertex AI Gen AI Evaluation provides several ground-truth based metrics:\n",
    "\n",
    "* `trajectory_exact_match`: identical trajectories (same actions, same order)\n",
    "\n",
    "* `trajectory_in_order_match`: reference actions present in predicted trajectory, in order (extras allowed)\n",
    "\n",
    "* `trajectory_any_order_match`: all reference actions present in predicted trajectory (order, extras don't matter).\n",
    "\n",
    "* `trajectory_precision`: proportion of predicted actions present in reference\n",
    "\n",
    "* `trajectory_recall`: proportion of reference actions present in predicted.  \n",
    "\n",
    "All metrics score 0 or 1, except `trajectory_precision` and `trajectory_recall` which range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "id": "4da027ea-94ee-40bc-bf98-950a304f718c",
   "metadata": {
    "tags": []
   },
   "source": [
    "trajectory_metrics = [\n",
    "    \"trajectory_exact_match\",\n",
    "    \"trajectory_in_order_match\",\n",
    "    \"trajectory_any_order_match\",\n",
    "    \"trajectory_precision\",\n",
    "    \"trajectory_recall\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a3ddd652-5c2e-4d01-8bd4-bd457e377121",
   "metadata": {},
   "source": [
    "#### Run an evaluation task\n",
    "\n",
    "Submit an evaluation by running `evaluate` method of the new `EvalTask`."
   ]
  },
  {
   "cell_type": "code",
   "id": "217d1c35-469e-40a5-aa60-52bd9d4f304f",
   "metadata": {
    "tags": []
   },
   "source": [
    "EXPERIMENT_RUN = f\"trajectory-{get_id()}\"\n",
    "\n",
    "trajectory_eval_task = EvalTask(\n",
    "    dataset=eval_sample_dataset,\n",
    "    metrics=trajectory_metrics,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    output_uri_prefix=BUCKET_URI + \"/multiple-metric-eval\",\n",
    ")\n",
    "\n",
    "trajectory_eval_result = trajectory_eval_task.evaluate(\n",
    "    runnable=agent_parsed_outcome, experiment_run_name=EXPERIMENT_RUN\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ff0c2fa1-00b3-4925-ac38-87ce6b325b80",
   "metadata": {
    "tags": []
   },
   "source": [
    "display_eval_report(trajectory_eval_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ffd74f53-0d32-4e50-8706-c1a490790bc7",
   "metadata": {},
   "source": [
    "#### Visualize evaluation results\n",
    "\n",
    "Print and visualize a sample of evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "id": "541b79d3-7a73-4865-a032-b96eb6ea6298",
   "metadata": {
    "tags": []
   },
   "source": [
    "display_dataframe_rows(trajectory_eval_result.metrics_table, num_rows=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6af6823-24fe-43e1-b034-af010405dba7",
   "metadata": {
    "tags": []
   },
   "source": [
    "plot_bar_plot(\n",
    "    trajectory_eval_result,\n",
    "    title=\"Trajectory Metrics\",\n",
    "    metrics=[f\"{metric}/mean\" for metric in trajectory_metrics],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba34733d-6641-48ab-bb56-b39fd65f860e",
   "metadata": {},
   "source": [
    "### Evaluate final response\n",
    "\n",
    "Similar to model evaluation, you can evaluate the final response of the agent using Vertex AI Gen AI Evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff1748-20a4-4ea2-a512-3781ce2aac73",
   "metadata": {},
   "source": [
    "#### Set response metrics\n",
    "\n",
    "After agent inference, Vertex AI Gen AI Evaluation provides several metrics to evaluate generated responses. You can use computation-based metrics to compare the response to a reference (if needed) and using existing or custom model-based metrics to determine the quality of the final response.\n",
    "\n",
    "Check out the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval) to learn more.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f8c4cca-9ac8-4a15-bb2c-6b99effac541",
   "metadata": {
    "tags": []
   },
   "source": [
    "response_metrics = [\"safety\", \"coherence\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "333adf41-1255-4257-a203-a4755d632bb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run an evaluation task\n",
    "\n",
    "To evaluate agent's generated responses, use the `evaluate` method of the EvalTask class."
   ]
  },
  {
   "cell_type": "code",
   "id": "4ba941c6-828c-477f-99b5-5862033730ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "EXPERIMENT_RUN = f\"response-{get_id()}\"\n",
    "\n",
    "response_eval_task = EvalTask(\n",
    "    dataset=eval_sample_dataset,\n",
    "    metrics=response_metrics,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    output_uri_prefix=BUCKET_URI + \"/response-metric-eval\",\n",
    ")\n",
    "\n",
    "response_eval_result = response_eval_task.evaluate(\n",
    "    runnable=agent_parsed_outcome, experiment_run_name=EXPERIMENT_RUN\n",
    ")\n",
    "\n",
    "display_eval_report(response_eval_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2adaeb0c-c802-4802-a353-ad9257fe18d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualize evaluation results\n",
    "\n",
    "\n",
    "Print new evaluation result sample."
   ]
  },
  {
   "cell_type": "code",
   "id": "339cc5e2-f7d2-40da-b598-b143a0fb0b4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "display_dataframe_rows(response_eval_result.metrics_table, num_rows=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0c0e9031-8fd1-4e79-bfc0-0777f700b9d5",
   "metadata": {},
   "source": [
    "### Evaluate generated response conditioned by tool choosing\n",
    "\n",
    "When evaluating AI agents that interact with environments, standard text generation metrics like coherence may not be sufficient. This is because these metrics primarily focus on text structure, while agent responses should be assessed based on their effectiveness within the environment.\n",
    "\n",
    "Instead, use custom metrics that assess whether the agent's response logically follows from its tools choices like the one you have in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a447d03-b950-41fc-83c9-1aaa9c11302e",
   "metadata": {},
   "source": [
    "#### Define a custom metric\n",
    "\n",
    "According to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#model-based-metrics), you can define a prompt template for evaluating whether an AI agent's response follows logically from its actions by setting up criteria and a rating system for this evaluation.\n",
    "\n",
    "Define a `criteria` to set the evaluation guidelines and a `pointwise_rating_rubric` to provide a scoring system (1 or 0). Then use a `PointwiseMetricPromptTemplate` to create the template using these components.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5aaeeb38-20dc-4579-a3e4-9efb52fbe9a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "criteria = {\n",
    "    \"Follows trajectory\": (\n",
    "        \"Evaluate whether the agent's response logically follows from the \"\n",
    "        \"sequence of actions it took. Consider these sub-points:\\n\"\n",
    "        \"  - Does the response reflect the information gathered during the trajectory?\\n\"\n",
    "        \"  - Is the response consistent with the goals and constraints of the task?\\n\"\n",
    "        \"  - Are there any unexpected or illogical jumps in reasoning?\\n\"\n",
    "        \"Provide specific examples from the trajectory and response to support your evaluation.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "pointwise_rating_rubric = {\n",
    "    \"1\": \"Follows trajectory\",\n",
    "    \"0\": \"Does not follow trajectory\",\n",
    "}\n",
    "\n",
    "response_follows_trajectory_prompt_template = PointwiseMetricPromptTemplate(\n",
    "    criteria=criteria,\n",
    "    rating_rubric=pointwise_rating_rubric,\n",
    "    input_variables=[\"prompt\", \"predicted_trajectory\"],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0b50b813-c1fc-40dd-9cf2-d73004d1a7e2",
   "metadata": {},
   "source": [
    "Print the prompt_data of this template containing the combined criteria and rubric information ready for use in an evaluation."
   ]
  },
  {
   "cell_type": "code",
   "id": "cd9c551b-e1f9-405b-bdc4-8072d4c0a1a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(response_follows_trajectory_prompt_template.prompt_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0332923b-e46c-4dfe-b06f-d47696119d1a",
   "metadata": {},
   "source": [
    "After you define the evaluation prompt template, set up the associated metric to evaluate how well a response follows a specific trajectory. The `PointwiseMetric` creates a metric where `response_follows_trajectory` is the metric's name and `response_follows_trajectory_prompt_template` provides instructions or context for evaluation you set up before."
   ]
  },
  {
   "cell_type": "code",
   "id": "6f00ced8-113a-44a3-98d6-46d4c4c7b882",
   "metadata": {
    "tags": []
   },
   "source": [
    "response_follows_trajectory_metric = PointwiseMetric(\n",
    "    metric=\"response_follows_trajectory\",\n",
    "    metric_prompt_template=response_follows_trajectory_prompt_template,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ac68ad3-11b1-46ac-a67a-a345efd4d4f5",
   "metadata": {},
   "source": [
    "#### Set response metrics\n",
    "\n",
    "Set new generated response evaluation metrics by including the custom metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "958f63fc-6d2b-4b89-93cb-5c5b3b071d15",
   "metadata": {
    "tags": []
   },
   "source": [
    "response_tool_metrics = [\n",
    "    \"trajectory_exact_match\",\n",
    "    \"trajectory_in_order_match\",\n",
    "    \"safety\",\n",
    "    response_follows_trajectory_metric,\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b8d7c45c-02cf-4ebf-ad6b-1e7037d65ff6",
   "metadata": {},
   "source": [
    "#### Run an evaluation task\n",
    "\n",
    "Run a new agent's evaluation."
   ]
  },
  {
   "cell_type": "code",
   "id": "88453a2a-a98c-47ce-a964-36e4b472cc19",
   "metadata": {
    "tags": []
   },
   "source": [
    "EXPERIMENT_RUN = f\"response-over-tools-{get_id()}\"\n",
    "\n",
    "response_eval_tool_task = EvalTask(\n",
    "    dataset=eval_sample_dataset,\n",
    "    metrics=response_tool_metrics,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    output_uri_prefix=BUCKET_URI + \"/reasoning-metric-eval\",\n",
    ")\n",
    "\n",
    "response_eval_tool_result = response_eval_tool_task.evaluate(\n",
    "    runnable=agent_parsed_outcome, experiment_run_name=EXPERIMENT_RUN\n",
    ")\n",
    "\n",
    "display_eval_report(response_eval_tool_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "06f69c4f-5fe7-49cc-a462-6a5a6679456e",
   "metadata": {},
   "source": [
    "#### Visualize evaluation results\n",
    "\n",
    "Visualize evaluation result sample."
   ]
  },
  {
   "cell_type": "code",
   "id": "a7cb9526-09eb-4019-becf-5a0d133b07c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "display_dataframe_rows(response_eval_tool_result.metrics_table, num_rows=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "914f6796-9aa0-4d7f-9d2a-3f86fc5fd456",
   "metadata": {
    "tags": []
   },
   "source": [
    "plot_bar_plot(\n",
    "    response_eval_tool_result,\n",
    "    title=\"Response Metrics\",\n",
    "    metrics=[f\"{metric}/mean\" for metric in response_tool_metrics],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30da5dd8-58d5-43e2-9e01-5608c91d181d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bonus: Bring-Your-Own-Dataset (BYOD) and evaluate a ADK agent using Vertex AI Gen AI Evaluation\n",
    "\n",
    "In Bring Your Own Dataset (BYOD) [scenarios](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-dataset), you provide both the predicted trajectory and the generated response from the agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b3a8bd-77e6-40bb-871b-cd958496548a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bring your own evaluation dataset\n",
    "\n",
    "Define the evaluation dataset with the predicted trajectory and the generated response."
   ]
  },
  {
   "cell_type": "code",
   "id": "2f5f9cb6-6653-4d25-92a5-c5b7a2fa72b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "byod_eval_data = {\n",
    "    \"prompt\": [\n",
    "        \"Tell me the weather in New York\",\n",
    "        \"What is the weather like in London?\",\n",
    "        \"How about Paris?\",\n",
    "        \"Tell me the weather in New York and London?\",\n",
    "        \"What is the weather like in New York and London?\",\n",
    "        \"Tell me the weather in New York, London and Paris?\",\n",
    "    ],\n",
    "    \"reference_trajectory\": [\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"Paris\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            },\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            },\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"Paris\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            },\n",
    "        ],\n",
    "    ],\n",
    "    \"predicted_trajectory\": [\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"Paris\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            },\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            },\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"London\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"Paris\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_weather\",\n",
    "                \"tool_input\": {\"city\": \"New York\"},\n",
    "            },\n",
    "        ],\n",
    "    ],\n",
    "    \"response\": [\n",
    "        \"The weather in New York is sunny with a temperature of 25째C.\",\n",
    "        \"The weather in London is sunny with a temperature of 25째C.\",\n",
    "        \"The weather in New York is sunny with a temperature of 25째C.\",\n",
    "        \"The weather in New York is sunny with a temperature of 25째C.\",\n",
    "        \"The weather in New York is sunny with a temperature of 25째C.\",\n",
    "        \"The weather in New York is sunny with a temperature of 25째C.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "byod_eval_sample_dataset = pd.DataFrame(byod_eval_data)\n",
    "byod_eval_sample_dataset[\"predicted_trajectory\"] = byod_eval_sample_dataset[\n",
    "    \"predicted_trajectory\"\n",
    "].apply(json.dumps)\n",
    "byod_eval_sample_dataset[\"reference_trajectory\"] = byod_eval_sample_dataset[\n",
    "    \"reference_trajectory\"\n",
    "].apply(json.dumps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76a30c86-de73-4bb3-8824-37e49aa06e68",
   "metadata": {},
   "source": [
    "### Run an evaluation task\n",
    "\n",
    "Run a new agent's evaluation using your own dataset and the same setting of the latest evaluation."
   ]
  },
  {
   "cell_type": "code",
   "id": "cf8b1cfc-b4ad-42f4-8c4c-fdb47504fe8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "EXPERIMENT_RUN_NAME = f\"response-over-tools-byod-{get_id()}\"\n",
    "\n",
    "byod_response_eval_tool_task = EvalTask(\n",
    "    dataset=byod_eval_sample_dataset,\n",
    "    metrics=response_tool_metrics,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    output_uri_prefix=BUCKET_URI + \"/byod-eval\",\n",
    ")\n",
    "\n",
    "byod_response_eval_tool_result = byod_response_eval_tool_task.evaluate(\n",
    "    experiment_run_name=EXPERIMENT_RUN_NAME\n",
    ")\n",
    "\n",
    "display_eval_report(byod_response_eval_tool_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e90fea3c-f4b9-4f6c-b59b-1e15654b27bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize evaluation results\n",
    "\n",
    "Visualize evaluation result sample."
   ]
  },
  {
   "cell_type": "code",
   "id": "e838e093-0553-4093-bc4f-7c93fb704bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "display_dataframe_rows(byod_response_eval_tool_result.metrics_table, num_rows=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6addae7e-c340-4483-8ddf-8d38b62eeda9",
   "metadata": {
    "tags": []
   },
   "source": [
    "display_radar_plot(\n",
    "    byod_response_eval_tool_result,\n",
    "    title=\"ADK agent evaluation\",\n",
    "    metrics=[f\"{metric}/mean\" for metric in response_tool_metrics],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "562b6138-4d26-40ca-a306-3470b1136367",
   "metadata": {},
   "source": [
    "Copyright 2026 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "id": "61681970-a984-427c-9043-0a3cb37b6c00",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m138",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m138"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

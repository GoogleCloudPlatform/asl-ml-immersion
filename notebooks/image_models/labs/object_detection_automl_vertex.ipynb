{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Object Detection with Vertex AI AutoML</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Learning Objectives</b> ##\n",
    "\n",
    "1. Learn how to create a managed dataset on Vertex AI for object detection\n",
    "1. Learn how to train an AutoML object detection model on Vertex\n",
    "1. Learn how to evaluate a model trained with AutoML on Vertex\n",
    "1. Learn how to deploy a model trained with AutoML to a Vertex Endpoint\n",
    "1. Learn how to predict on new data with deployed model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use Vertex AI to train an AutoML model capable of detecting multiple objects in a given image and provide information about the objects and their location within the image.\n",
    "\n",
    "We will start by creating a Managed Dataset on Vertex AI and importing a publicly available set of images into it. After that we will train, evaluate and deploy the AutoML model to a Vertex Endpoint. Finally we will send prediction requests to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT  # defaults to PROJECT\n",
    "REGION = \"us-central1\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare and Format Training Data\n",
    "\n",
    "The first step in creating a Managed Datset on Vertex AI is to prepare the training data. In this case the training dataset is composed of images along with information identifying the location (through bounding boxes coordinates) and type of objects (through labels) in the images. \n",
    "Here are some constraints some general rules for preparing an Managed Datset for object detection:\n",
    "\n",
    "* The following image formats are supported: JPEG, PNG, GIF, BMP, or ICO. Maximum file size is 30MB per image.\n",
    "\n",
    "* It is recommended to have about 1000 training images per label (i.e. object type you want to detect in the images). For each label you must have at least 10 images, each with at least one annotation (bounding box and the label). In general, the more images per label you have the better your model will perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a CSV file with image URIs and labels\n",
    "\n",
    "Once your image files have been uploaded to a Cloud Storage bucket, you must create a CSV (or JSONL) file that lists the image URIs, along with bounding box information and the object labels. The CSV file will contain one row per bounding box in the image, so an image that has two bounding boxes will have two corresponding rows in the CSV file sharing the same image URI. \n",
    "\n",
    "In the example below, rows 1 and 2 reference the same image that has 2 annotations \n",
    "`(car,0.1,0.1,,,0.3,0.3,,)` and  `(bike,.7,.6,,,.8,.9,,)`. The first element of the annotation\n",
    "is the object label in the bounding box, while the rest are the coordinates of the bounding box\n",
    "within the image (see below for details).\n",
    "\n",
    "\n",
    "Row 3 refers to an image that has only 1 annotation `(car,0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.3)`, while row 4 references an image with no annotations.\n",
    "\n",
    "The first column corresponds to the data split (this is optional - if not provided autoML will do a 80/10/10 split), the second column to the image URI, and the last columns hold the annotations. \n",
    "\n",
    "Bounding boxes can be in one of two formats:\n",
    "- Containing 2 vertices (x,y coordinates of diagonally opposite points of the rectangle): `(x_min, y_min, , ,x_max,y_max,,)` OR\n",
    "- Containing all 4 vertices`(x_min,y_min,x_max,y_min,x_max,y_max,x_min,y_max)`\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "TRAIN,gs://folder/image1.png,car,0.1,0.1,,,0.3,0.3,,\n",
    "TRAIN,gs://folder/image1.png,bike,.7,.6,,,.8,.9,,\n",
    "UNASSIGNED,gs://folder/im2.png,car,0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.3\n",
    "TEST,gs://folder/im3.png,,,,,,,,,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row above has these columns:\n",
    "\n",
    "1. <b>Which dataset is the content in the row being assigned to.</b> - `TRAIN`, `VALIDATE`, `TEST` or `UNASSIGNED`\n",
    "1. <b>What content is being annotated.</b> - It contains the GCS URI for the image\n",
    "1. <b>A label that identifies how the object is categorized.\n",
    "1. <b>A bounding box for an object in the image.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will use a CSV that has already been created for us. The image URIs in the CSV point to publically avaliable images in GCS and contain annotations related to the foods/ingredients in each image.\n",
    "\n",
    "Make sure the GCS bucket exists (if not then create it), then copy the CSV to your GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo -e \"Bucket gs://${BUCKET} already exists.\"\n",
    "    \n",
    "else\n",
    "   echo \"Creating a new GCS bucket.\"\n",
    "   gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "   echo -e \"\\nHere are your current buckets:\"\n",
    "   gsutil ls\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first few rows of the CSV file, then copy it to your GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../dataset_import_files/salads.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp ../dataset_import_files/salads.csv gs://{BUCKET}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the Managed Dataset\n",
    "\n",
    "Next step is to create a Managed Dataset on Vertex AI for object detection.\n",
    "<br/>\n",
    "**NOTE**: This can take between 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"salad_dataset\"\n",
    "\n",
    "ds = aiplatform.ImageDataset.create(\n",
    "    display_name=DATASET_NAME,\n",
    "    gcs_source=f\"gs://{BUCKET}/salads.csv\",\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.bounding_box,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train AutoML Model\n",
    "Launch the training job. We will set the following parameters in `job.run()`\n",
    "- `dataset`: The managed dataset we just created\n",
    "- `model_display_name`: The display name of the model. The trained model will become a Vertex Model object after training is finished.\n",
    "- `training_fraction_split`: Percentage of data to train on. Explicitly set to 0.8 (80%).\n",
    "- `validation_fraction_split`: Percentage of data to validate on. Explicitly set to 0.1 (10%).\n",
    "- `test_fraction_split`: Percentage of data to test on. Explicitly set to 0.1 (10%).\n",
    "- `budget_milli_node_hours`: Milli-node hours for training job (e.g. 1000 means 1 node hour). Minimum for AutoML Object Detection training jobs is 20000 (20 node hours). \n",
    "- `disable_early_stopping`: Boolean. Explicitly set to False (i.e. Enable early stopping).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DISPLAY_NAME = \"salad\"\n",
    "\n",
    "job = aiplatform.AutoMLImageTrainingJob(\n",
    "    display_name=\"salad_\" + TIMESTAMP, prediction_type=\"object_detection\"\n",
    ")\n",
    "\n",
    "model = job.run(\n",
    "    # TODO: Launch the training job\n",
    "    # Set the train split to 80% with 10% for both validation and testing\n",
    "    # Set the budget to 20 node hours\n",
    "    # Enable early stopping\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Model training can take between 1.5-2.5 hours to complete. You must wait for model to finish training before moving forward. \n",
    "\n",
    "Retrieve the model resource and get the evaluation metrics. We will look at the [Mean Average Precision](https://blog.paperspace.com/mean-average-precision/). [Here](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) is a good resource to learn more about evaluation metrics for object detection models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model resources by display name\n",
    "models = aiplatform.Model.list(filter=\"display_name=salad\")\n",
    "salad_model = models[0]\n",
    "\n",
    "# Get model evaluation\n",
    "client_options = {\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    "model_service_client = aiplatform.gapic.ModelServiceClient(\n",
    "    client_options=client_options\n",
    ")\n",
    "\n",
    "model_evaluations = model_service_client.list_model_evaluations(\n",
    "    parent=salad_model.resource_name\n",
    ")\n",
    "evals = list(model_evaluations)[0]\n",
    "\n",
    "for metric in list(evals.metrics[\"boundingBoxMetrics\"]):\n",
    "    try:\n",
    "        print(f\"Mean Average Precision (mAP): {metric['meanAveragePrecision']}\")\n",
    "    except KeyError:\n",
    "        print(\"IOU threshold is too high to detect positive cases.\")\n",
    "\n",
    "    print(f\"Intersection over union (IoU): {metric['iouThreshold']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deploy the model\n",
    "\n",
    "Once we are happy with the performance of our trained model, we can deploy it so that it will be\n",
    "available for predictions through an API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_DISPLAY_NAME='salad_model_endpoint'\n",
    "\n",
    "# TODO: Deploy the model to an endpoint \n",
    "endpoint ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Send prediction request\n",
    "\n",
    "In this example we will invoke an individual prediction from an image that is stored in our project's Cloud storage bucket.\n",
    "Object detection models output many bounding boxes for an input image. For the output we are expecting that each box comes with:\n",
    "1. a label and \n",
    "1. a score of confidence.\n",
    "\n",
    "Preparing the data to get a prediction entails the following:\n",
    "1. Encode the image with `base64.b64encode` which returns the encoded bytes. Then apply `.decode(\"utf-8\")` to return the decoded string.\n",
    "1. Create the `ImageObjectDetectionPredictionInstance`\n",
    "1. Create the `ImageObjectDetectionPredictionParams`. This is namely used to set a confidence threshold.\n",
    "1. Make the prediction request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "\n",
    "TEST_IMAGE = \"../test_images/salad.jpg\"  # Replace with a Cloud storage bucket uploaded image of your choice\n",
    "\n",
    "# Read the file.\n",
    "with open(TEST_IMAGE, \"rb\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "encoded_content = base64.b64encode(file_content).decode(\"utf-8\")\n",
    "instance = predict.instance.ImageObjectDetectionPredictionInstance(\n",
    "    content=encoded_content\n",
    ").to_value()\n",
    "\n",
    "parameters = predict.params.ImageObjectDetectionPredictionParams(\n",
    "    confidence_threshold=0.7\n",
    ").to_value()\n",
    "\n",
    "response = endpoint.predict(instances=[instance], parameters=parameters)\n",
    "preds = response.predictions[0]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the response object from the deployed model, we can inspect its predictions (i.e., the\n",
    "bounding boxes and objects that the model has detected from the images we sent to it in the cell above):\n",
    "\n",
    "`preds` is a Python dictionary with keys:\n",
    "* `bboxes`: A list of predicted bounding box coordinates\n",
    "* `displayNames`: A list of predicted labels for the bounding boxes\n",
    "* `confidences`: A list of the prediction confidences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# To generate a different colors for each label\n",
    "cycol = cycle(\"bgrcmk\")\n",
    "\n",
    "img = Image.open(TEST_IMAGE)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img)\n",
    "\n",
    "for i in range(len(preds[\"displayNames\"])):\n",
    "    bbox = preds[\"bboxes\"][i]\n",
    "    display_name = preds[\"displayNames\"][i]\n",
    "    confidence = preds[\"confidences\"][i]\n",
    "    xy = (int(bbox[0] * img.width), int(bbox[2] * img.height))\n",
    "    w = int(img.width * (bbox[1] - bbox[0]))\n",
    "    h = int(img.height * (bbox[3] - bbox[2]))\n",
    "    r = patches.Rectangle(\n",
    "        xy,\n",
    "        w,\n",
    "        h,\n",
    "        linewidth=1,\n",
    "        facecolor=\"none\",\n",
    "        label=f\"{display_name}: {confidence:.3f}\",\n",
    "        edgecolor=next(cycol),\n",
    "    )\n",
    "    ax.add_patch(r)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! In this lab, you created a Vertex AI Managed Dataset for object detection. You then trained an AutoML model and deployed that trained model to an endpoint to serve predictions with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "asl_kernel",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "ASL Kernel (Local)",
   "language": "python",
   "name": "asl_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

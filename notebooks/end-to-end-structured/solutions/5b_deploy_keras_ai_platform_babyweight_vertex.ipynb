{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 5b:  Deploy and predict with Keras model on Vertex AI\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Setup up the environment\n",
    "1. Deploy trained Keras model to an endpoint for online prediction on Vertex AI\n",
    "1. Online predict from model on Vertex AI\n",
    "1. Batch predict from model on Vertex AI\n",
    "\n",
    "## Introduction \n",
    "In this notebook, we'll be deploying our Keras model to Vertex AI and creating predictions.\n",
    "\n",
    "We will set up the environment, deploy a trained Keras model to Vertex AI for online prediction, online predict from deployed model on Vertex AI, and batch predict on Vertex AI.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in this student lab notebook -- try to complete this notebook first and then review the [solution notebook](../solutions/5b_deploy_keras_ai_platform_babyweight.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJ7ByvoXzpVI"
   },
   "source": [
    "## Set up environment variables and load necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "except ImportError:\n",
    "    !pip3 install -U google-cloud-aiplatform --user\n",
    "\n",
    "    print(\"Please restart the kernel and re-run the notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables.\n",
    "\n",
    "Set environment variables so that we can use them throughout the entire lab. We will be using our project name for our bucket, so you only need to change your project and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"$PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these to try this notebook out\n",
    "PROJECT = \"asl-ml-immersion\"  # Replace with your PROJECT\n",
    "BUCKET = PROJECT  # defaults to PROJECT\n",
    "REGION = \"us-central1\"  # Replace with your REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set ai/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check our trained model files\n",
    "\n",
    "Let's check the directory structure of our outputs of our trained model in folder we exported the model to in our last [lab](../solutions/10_train_keras_ai_platform_babyweight.ipynb). We'll want to deploy the saved_model.pb within the directory of the tuned model as well as the variable values in the variables folder. Therefore, we need the path of the latest tuned directory so that everything within it can be found by Vertex AI's model deployment service. Note that the `2*` substrings are there to match timestamp strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/tuned_2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_LOCATION=$(gsutil ls -d -- gs://${BUCKET}/babyweight/tuned_2*/2* \\\n",
    "                 | tail -1)\n",
    "gsutil ls ${MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model, create endpoint and deploy trained model\n",
    "\n",
    "Uploading our SavedModel from the above `MODEL_LOCATION`, creating and endpoint and deploying the trained model to act as a REST web service are three simple gcloud calls. We also run a command to list the endpoints, to fetch the fully qualified resource name `ENDPOINT_RESOURCENAME` for the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "MODEL_DISPLAYNAME=babyweight_model_$TIMESTAMP\n",
    "ENDPOINT_DISPLAYNAME=babyweight_endpoint_$TIMESTAMP\n",
    "IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\"\n",
    "MODEL_LOCATION=$(gsutil ls -d -- gs://${BUCKET}/babyweight/tuned_2*/2* \\\n",
    "                 | tail -1)\n",
    "echo \"MODEL_LOCATION=${MODEL_LOCATION}\"\n",
    "\n",
    "# Model\n",
    "MODEL_RESOURCENAME=$(gcloud ai models upload \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$MODEL_DISPLAYNAME \\\n",
    "    --container-image-uri=$IMAGE_URI \\\n",
    "    --artifact-uri=$MODEL_LOCATION \\\n",
    "    --format=\"value(model)\")\n",
    "\n",
    "MODEL_ID=$(echo $MODEL_RESOURCENAME | cut -d\"/\" -f6)\n",
    "\n",
    "echo \"MODEL_DISPLAYNAME=${MODEL_DISPLAYNAME}\"\n",
    "echo \"MODEL_RESOURCENAME=${MODEL_RESOURCENAME}\"\n",
    "echo \"MODEL_ID=${MODEL_ID}\"\n",
    "\n",
    "# Endpoint\n",
    "ENDPOINT_RESOURCENAME=$(gcloud ai endpoints create \\\n",
    "  --region=$REGION \\\n",
    "  --display-name=$ENDPOINT_DISPLAYNAME \\\n",
    "  --format=\"value(name)\")\n",
    "\n",
    "ENDPOINT_ID=$(echo $ENDPOINT_RESOURCENAME | cut -d\"/\" -f6)\n",
    "\n",
    "echo \"ENDPOINT_DISPLAYNAME=${ENDPOINT_DISPLAYNAME}\"\n",
    "echo \"ENDPOINT_RESOURCENAME=${ENDPOINT_RESOURCENAME}\"\n",
    "echo \"ENDPOINT_ID=${ENDPOINT_ID}\"\n",
    "\n",
    "# Deployment\n",
    "DEPLOYEDMODEL_DISPLAYNAME=${MODEL_DISPLAYNAME}_deployment\n",
    "MACHINE_TYPE=n1-standard-2\n",
    "MIN_REPLICA_COUNT=1\n",
    "MAX_REPLICA_COUNT=3\n",
    "\n",
    "gcloud ai endpoints deploy-model $ENDPOINT_RESOURCENAME \\\n",
    "  --region=$REGION \\\n",
    "  --model=$MODEL_RESOURCENAME \\\n",
    "  --display-name=$DEPLOYEDMODEL_DISPLAYNAME \\\n",
    "  --machine-type=$MACHINE_TYPE \\\n",
    "  --min-replica-count=$MIN_REPLICA_COUNT \\\n",
    "  --max-replica-count=$MAX_REPLICA_COUNT \\\n",
    "  --traffic-split=0=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model to make online prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python API\n",
    "\n",
    "We can use the Python API to send a JSON request to the endpoint of the service to make it predict a baby's weight. The order of the responses are the order of the instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_RESOURCENAME = (\n",
    "    \"\"  # TODO: Copy your `ENDPOINT_RESOURCENAME` from above.\n",
    ")\n",
    "os.environ[\"ENDPOINT_RESOURCENAME\"] = ENDPOINT_RESOURCENAME\n",
    "\n",
    "api_endpoint = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "    client_options=client_options\n",
    ")\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"is_male\": \"True\",\n",
    "        \"mother_age\": 26.0,\n",
    "        \"plurality\": \"Single(1)\",\n",
    "        \"gestation_weeks\": 39,\n",
    "    },\n",
    "    {\n",
    "        \"is_male\": \"False\",\n",
    "        \"mother_age\": 29.0,\n",
    "        \"plurality\": \"Single(1)\",\n",
    "        \"gestation_weeks\": 38,\n",
    "    },\n",
    "    {\n",
    "        \"is_male\": \"True\",\n",
    "        \"mother_age\": 26.0,\n",
    "        \"plurality\": \"Triplets(3)\",\n",
    "        \"gestation_weeks\": 39,\n",
    "    },\n",
    "    {\n",
    "        \"is_male\": \"Unknown\",\n",
    "        \"mother_age\": 29.0,\n",
    "        \"plurality\": \"Multiple(2+)\",\n",
    "        \"gestation_weeks\": 38,\n",
    "    },\n",
    "]\n",
    "\n",
    "instances = [\n",
    "    json_format.ParseDict(instance, Value()) for instance in instances\n",
    "]\n",
    "response = client.predict(endpoint=ENDPOINT_RESOURCENAME, instances=instances)\n",
    "\n",
    "# The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "print(\" prediction:\", response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions for the four instances were: 5.33, 6.09, 2.50, and 5.86 pounds respectively when I ran it (your results might be different)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gcloud shell API\n",
    "\n",
    "Instead we could use the gcloud shell API. Create a newline delimited JSON file with one instance per line and submit using gcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inputs.json\n",
    "{\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"is_male\": \"True\",\n",
    "            \"mother_age\": 26.0,\n",
    "            \"plurality\": \"Single(1)\",\n",
    "            \"gestation_weeks\": 39,\n",
    "        },\n",
    "        {\n",
    "            \"is_male\": \"False\",\n",
    "            \"mother_age\": 26.0,\n",
    "            \"plurality\": \"Single(1)\",\n",
    "            \"gestation_weeks\": 39,\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call `gcloud ai endpoint predict` using the JSON we just created and point to our deployed `ENDPOINT_RESOURCENAME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ai endpoints predict $ENDPOINT_RESOURCENAME \\\n",
    "    --region=$REGION \\\n",
    "    --json-request=inputs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model to make batch prediction.\n",
    "\n",
    "Batch prediction is commonly used when you have thousands to millions of predictions. It will create a Vertex AI batch prediction job. We will put our prediction request JSONL file (multiple lines of JSON records) to GCS, and use the Python API to request the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inputs.jsonl\n",
    "{\n",
    "    \"is_male\": \"True\",\n",
    "    \"mother_age\": 26.0,\n",
    "    \"plurality\": \"Single(1)\",\n",
    "    \"gestation_weeks\": 39,\n",
    "}\n",
    "{\n",
    "    \"is_male\": \"False\",\n",
    "    \"mother_age\": 26.0,\n",
    "    \"plurality\": \"Single(1)\",\n",
    "    \"gestation_weeks\": 39,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp inputs.jsonl gs://$BUCKET/babyweight/batchpred/inputs.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESOURCENAME = (\n",
    "    \"\"  # TODO: replace with your MODEL_RESOURCENAME from above\n",
    ")\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "my_model = aiplatform.Model(MODEL_RESOURCENAME)\n",
    "\n",
    "batch_prediction_job = my_model.batch_predict(\n",
    "    job_display_name=\"babyweight_batch\",\n",
    "    gcs_source=f\"gs://{BUCKET}/babyweight/batchpred/inputs.jsonl\",\n",
    "    gcs_destination_prefix=f\"gs://{BUCKET}/babyweight/batchpred/outputs\",\n",
    "    machine_type=\"n1-standard-2\",\n",
    "    accelerator_count=0,\n",
    "    starting_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")\n",
    "\n",
    "batch_prediction_job.wait()\n",
    "\n",
    "print(batch_prediction_job.display_name)\n",
    "print(batch_prediction_job.resource_name)\n",
    "print(batch_prediction_job.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat $(gsutil ls gs://$BUCKET/babyweight/batchpred/outputs | tail -n1)prediction.errors_stats-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat $(gsutil ls gs://$BUCKET/babyweight/batchpred/outputs | tail -n1)prediction.results-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary:\n",
    "In this lab, we set up the environment, deployed a trained Keras model to Vertex AI, online predicted from deployed model, and batch predicted from deployed model on Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN Encode-Decoder for Translation\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Learn how to create a tf.data.Dataset for seq2seq problems\n",
    "1. Learn how to train an encoder-decoder model in Keras\n",
    "1. Learn how to save the encoder and the decoder as separate models \n",
    "1. Learn how to piece together the trained encoder and decoder into a translation function\n",
    "1. Learn how to use the BLUE score to evaluate a translation model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab we'll build a translation model from Spanish to English using a RNN encoder-decoder model architecture.\n",
    "We will start by creating train and eval datasets (using the `tf.data.Dataset` API) that are typical for seq2seq problems. Then we will use the Keras functional API to train an RNN encoder-decoder model, which will save as two separate models, the encoder and decoder model. Using these two separate pieces we will implement the translation function.\n",
    "At last, we'll benchmark our results using the industry standard BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "import evaluate\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import utils_preproc\n",
    "from keras.layers import GRU, Dense, Embedding, Input\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "MODEL_PATH = \"translate_models/baseline\"\n",
    "DATA_URL = (\n",
    "    \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    ")\n",
    "LOAD_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a language dataset provided by http://www.manythings.org/anki/. The dataset contains Spanish-English  translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "The dataset is a curated list of 120K translation pairs from http://tatoeba.org/, a platform for community contributed translations by native speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation data stored at: /home/jupyter/.keras/datasets/spa-eng_extracted/spa-eng/spa.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = keras.utils.get_file(\"spa-eng.zip\", origin=DATA_URL, extract=True)\n",
    "\n",
    "path_to_file = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"spa-eng_extracted/spa-eng/spa.txt\"\n",
    ")\n",
    "print(\"Translation data stored at:\", path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mspa-eng\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /home/jupyter/.keras/datasets/spa-eng_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /home/jupyter/.keras/datasets/spa-eng/spa.txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat /home/jupyter/.keras/datasets/spa-eng/spa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    path_to_file, sep=\"\\t\", header=None, names=[\"english\", \"spanish\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8807</th>\n",
       "      <td>She adores cats.</td>\n",
       "      <td>Ella adora los gatos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>Tom saw Mary leave.</td>\n",
       "      <td>Tom vio a Mary irse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26585</th>\n",
       "      <td>You've grown so tall.</td>\n",
       "      <td>Te has puesto tan alto.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     english                  spanish\n",
       "8807        She adores cats.    Ella adora los gatos.\n",
       "18282    Tom saw Mary leave.     Tom vio a Mary irse.\n",
       "26585  You've grown so tall.  Te has puesto tan alto."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `utils_preproc` package we have written for you,\n",
    "we will use the following functions to pre-process our dataset of sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame with 'english' and 'spanish' columns\n",
    "# and NUM_EXAMPLES is defined\n",
    "# data = data.sample(NUM_EXAMPLES, random_state=SEED)\n",
    "\n",
    "# Create a single dataset from the DataFrame columns\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data[\"spanish\"], data[\"english\"])\n",
    ")\n",
    "\n",
    "# Define constants\n",
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "TEST_PROP = 0.2\n",
    "NUM_EXAMPLES = 30000\n",
    "TRAIN_SIZE = int(NUM_EXAMPLES * (1 - TEST_PROP))\n",
    "\n",
    "# Create the training and validation raw text datasets\n",
    "train_raw = full_dataset.take(TRAIN_SIZE)\n",
    "val_raw = full_dataset.skip(TRAIN_SIZE)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"Custom standardization function for the TextVectorization layer.\"\"\"\n",
    "    text = tf.strings.lower(input_string)\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.regex_replace(text, r\"([?.!,¿])\", r\" \\1 \")\n",
    "    text = tf.strings.regex_replace(text, r'[\" \"]+', \" \")\n",
    "    text = tf.strings.regex_replace(text, r\"[^a-zA-Z?.!,¿]+\", \" \")\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join([\"<start> \", text, \" <end>\"])\n",
    "    return text\n",
    "\n",
    "\n",
    "# Create TextVectorization layers\n",
    "source_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=INPUT_VOCAB_SIZE,\n",
    "    output_sequence_length=max_length_inp,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "target_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=TARGET_VOCAB_SIZE,\n",
    "    output_sequence_length=max_length_targ,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "# Create temporary datasets for adaptation, containing only the text\n",
    "train_text_source = train_raw.map(lambda source, target: source)\n",
    "train_text_target = train_raw.map(lambda source, target: target)\n",
    "\n",
    "# Adapt the layers on the training text\n",
    "source_vectorization.adapt(train_text_source)\n",
    "target_vectorization.adapt(train_text_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize_text(source, target):\n",
    "    source = source_vectorization(source)\n",
    "    target = target_vectorization(target)\n",
    "    return source, target\n",
    "\n",
    "\n",
    "def make_batches(ds):\n",
    "    return (\n",
    "        ds.shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(lambda x, y: ((x, y), tf.roll(y, -1, 1)))\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "# Create the final datasets for model training\n",
    "train_batches = make_batches(train_raw)\n",
    "eval_batches = make_batches(val_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `utils_preproc.preprocess_sentence()` method does the following:\n",
    "1. Converts sentence to lower case\n",
    "2. Adds a space between punctuation and words\n",
    "3. Replaces tokens that aren't a-z or punctuation with space\n",
    "4. Adds `<start>` and `<end>` tokens\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "    \"El invierno se acerca.\",\n",
    "    \"Tom no comio nada.\",\n",
    "    \"Su pierna mala le impidió ganar la carrera.\",\n",
    "    \"Su respuesta es erronea.\",\n",
    "    \"¿Qué tal si damos un paseo después del almuerzo?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> no estamos comiendo . <end>',\n",
       " '<start> esta llegando el invierno . <end>',\n",
       " '<start> el invierno se acerca . <end>',\n",
       " '<start> tom no comio nada . <end>',\n",
       " '<start> su pierna mala le impidio ganar la carrera . <end>',\n",
       " '<start> su respuesta es erronea . <end>',\n",
       " '<start> ¿ que tal si damos un paseo despues del almuerzo ? <end>']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = [utils_preproc.preprocess_sentence(s) for s in raw]\n",
    "processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Integerizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `utils_preproc.tokenize()` method does the following:\n",
    "    \n",
    "1. Splits each sentence into a token list\n",
    "1. Maps each token to an integer\n",
    "1. Pads to length of longest sentence \n",
    "\n",
    "It returns an instance of a [Keras Tokenizer](https://keras.io/preprocessing/text/)\n",
    "containing the token-integer mapping along with the integerized sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  8,  9,  3,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 10, 11,  5,  6,  3,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  5,  6, 12, 13,  3,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 14,  4, 15, 16,  3,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  7, 17, 18, 19, 20, 21, 22, 23,  3,  2,  0,  0],\n",
       "       [ 1,  7, 24, 25, 26,  3,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,  2]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integerized, tokenizer = utils_preproc.tokenize(processed)\n",
    "integerized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputted tokenizer can be used to get back the actual works\n",
    "from the integers representing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> no estamos comiendo . <end>',\n",
       " '<start> esta llegando el invierno . <end>',\n",
       " '<start> el invierno se acerca . <end>',\n",
       " '<start> tom no comio nada . <end>',\n",
       " '<start> su pierna mala le impidio ganar la carrera . <end>',\n",
       " '<start> su respuesta es erronea . <end>',\n",
       " '<start> ¿ que tal si damos un paseo despues del almuerzo ? <end>']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(integerized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the tf.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `load_and_preprocess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first implement a function that will read the raw sentence-pair file\n",
    "and preprocess the sentences with `utils_preproc.preprocess_sentence`.\n",
    "\n",
    "The `load_and_preprocess` function takes as input\n",
    "- the path where the sentence-pair file is located\n",
    "- the number of examples one wants to read in\n",
    "\n",
    "It returns a tuple whose first component contains the english\n",
    "preprocessed sentences, while the second component contains the\n",
    "spanish ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(path, num_examples):\n",
    "    with open(path_to_file) as fp:\n",
    "        lines = fp.read().strip().split(\"\\n\")\n",
    "\n",
    "    # TODO 1a\n",
    "    sentence_pairs = [\n",
    "        [utils_preproc.preprocess_sentence(sent) for sent in line.split(\"\\t\")]\n",
    "        for line in lines[:num_examples]\n",
    "    ]\n",
    "\n",
    "    return zip(*sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> fire ! <end>\n",
      "<start> incendio ! <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = load_and_preprocess(path_to_file, num_examples=10)\n",
    "\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `load_and_integerize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `utils_preproc.tokenize`, let us now implement the function `load_and_integerize` that takes as input the data path along with the number of examples we want to read in and returns the following tuple:\n",
    "\n",
    "```python\n",
    "  (input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer)\n",
    "```\n",
    "\n",
    "where \n",
    "\n",
    "\n",
    "* `input_tensor` is an integer tensor of shape `(num_examples, max_length_inp)` containing the integerized versions of the source language sentences\n",
    "* `target_tensor` is an integer tensor of shape `(num_examples, max_length_targ)` containing the integerized versions of the target language sentences\n",
    "* `inp_lang_tokenizer` is the source language tokenizer\n",
    "* `targ_lang_tokenizer` is the target language tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_integerize(path, num_examples=None):\n",
    "    targ_lang, inp_lang = load_and_preprocess(path, num_examples)\n",
    "\n",
    "    # TODO 1b\n",
    "    input_tensor, inp_lang_tokenizer = utils_preproc.tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = utils_preproc.tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and eval splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split this data 80/20 into train and validation, and we'll use only the first 30K examples, since we'll be training on a single GPU. \n",
    " \n",
    "Let us set variable for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_PROP = 0.2\n",
    "NUM_EXAMPLES = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load and integerize the sentence paris and store the tokenizer for the source and the target language into the `int_lang` and `targ_lang` variable respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_and_integerize(\n",
    "    path_to_file, NUM_EXAMPLES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us store the maximal sentence length of both languages into two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length_targ = target_tensor.shape[1]\n",
    "max_length_inp = input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now using scikit-learn `train_test_split` to create our splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splits = train_test_split(\n",
    "    input_tensor, target_tensor, test_size=TEST_PROP, random_state=SEED\n",
    ")\n",
    "\n",
    "input_tensor_train = splits[0]\n",
    "input_tensor_val = splits[1]\n",
    "\n",
    "target_tensor_train = splits[2]\n",
    "target_tensor_val = splits[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the number of example in each split looks good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 24000, 6000, 6000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    len(input_tensor_train),\n",
    "    len(target_tensor_train),\n",
    "    len(input_tensor_val),\n",
    "    len(target_tensor_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `utils_preproc.int2word` function allows you to transform back the integerized sentences into words. Note that the `<start>` token is alwasy encoded as `1`, while the `<end>` token is always encoded as `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; int to word mapping\n",
      "[  1 133  14 316   3   2   0   0   0   0   0   0   0   0   0   0]\n",
      "['<start>', 'deja', 'de', 'leer', '.', '<end>', '', '', '', '', '', '', '', '', '', ''] \n",
      "\n",
      "Target Language; int to word mapping\n",
      "[  1  86 341   3   2   0   0   0   0   0   0]\n",
      "['<start>', 'stop', 'reading', '.', '<end>', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Language; int to word mapping\")\n",
    "print(input_tensor_train[0])\n",
    "print(utils_preproc.int2word(inp_lang, input_tensor_train[0]), \"\\n\")\n",
    "\n",
    "print(\"Target Language; int to word mapping\")\n",
    "print(target_tensor_train[0])\n",
    "print(utils_preproc.int2word(targ_lang, target_tensor_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf.data dataset for train and eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implement the `create_dataset` function that takes as input\n",
    "* `encoder_input` which is an integer tensor of shape `(num_examples, max_length_inp)` containing the integerized versions of the source language sentences\n",
    "* `decoder_input` which is an integer tensor of shape `(num_examples, max_length_targ)`containing the integerized versions of the target language sentences\n",
    "\n",
    "It returns a `tf.data.Dataset` containing examples for the form\n",
    "\n",
    "```python\n",
    "        ((source_sentence, target_sentence), shifted_target_sentence)\n",
    "```\n",
    "\n",
    "where `source_sentence` and `target_setence` are the integer version of source-target language pairs and `shifted_target` is the same as `target_sentence` but with indices shifted by 1. \n",
    "\n",
    "**Remark:** In the training code, `source_sentence`  (resp. `target_sentence`) will be fed as the encoder (resp. decoder) input, while `shifted_target` will be used to compute the cross-entropy loss by comparing the decoder output with the shifted target sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(encoder_input, decoder_input):\n",
    "    # TODO 1c\n",
    "\n",
    "    # shift ahead by 1\n",
    "    target = tf.roll(decoder_input, -1, 1)\n",
    "\n",
    "    # replace last column with 0s\n",
    "    zeros = tf.zeros([target.shape[0], 1], dtype=tf.int32)\n",
    "    target = tf.concat((target[:, :-1], zeros), axis=-1)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ((encoder_input, decoder_input), target)\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the actual train and eval dataset using the function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    create_dataset(input_tensor_train, target_tensor_train)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .repeat()\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataset = create_dataset(input_tensor_val, target_tensor_val).batch(\n",
    "    BATCH_SIZE, drop_remainder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RNN encoder-decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an encoder-decoder architecture, however we embed our words into a latent space prior to feeding them into the RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_UNITS = 1024\n",
    "\n",
    "INPUT_VOCAB_SIZE = len(inp_lang.word_index) + 1\n",
    "TARGET_VOCAB_SIZE = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the encoder network with Keras functional API. It will\n",
    "* start with an `Input` layer that will consume the source language integerized sentences\n",
    "* then feed them to an `Embedding` layer of `EMBEDDING_DIM` dimensions\n",
    "* which in turn will pass the embeddings to a `GRU` recurrent layer with `HIDDEN_UNITS`\n",
    "\n",
    "The output of the encoder will be the `encoder_outputs` and the `encoder_state`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), name=\"encoder_input\")\n",
    "\n",
    "# TODO 2a\n",
    "encoder_inputs_embedded = Embedding(\n",
    "    input_dim=INPUT_VOCAB_SIZE,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    input_length=max_length_inp,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_rnn = GRU(\n",
    "    units=HIDDEN_UNITS,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer=\"glorot_uniform\",\n",
    ")\n",
    "\n",
    "encoder_outputs, encoder_state = encoder_rnn(encoder_inputs_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the decoder network, which is very similar to the encoder network.\n",
    "\n",
    "It will\n",
    "* start with an `Input` layer that will consume the source language integerized sentences\n",
    "* then feed that input to an `Embedding` layer of `EMBEDDING_DIM` dimensions\n",
    "* which in turn will pass the embeddings to a `GRU` recurrent layer with `HIDDEN_UNITS`\n",
    "\n",
    "**Important:** The main difference with the encoder, is that the recurrent `GRU` layer will take as input not only the decoder input embeddings, but also the `encoder_state` as outputted by the encoder above. This is where the two networks are linked!\n",
    "\n",
    "The output of the encoder will be the `decoder_outputs` and the `decoder_state`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,), name=\"decoder_input\")\n",
    "\n",
    "# TODO 2b\n",
    "decoder_inputs_embedded = Embedding(\n",
    "    input_dim=TARGET_VOCAB_SIZE,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    input_length=max_length_targ,\n",
    ")(decoder_inputs)\n",
    "\n",
    "decoder_rnn = GRU(\n",
    "    units=HIDDEN_UNITS,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer=\"glorot_uniform\",\n",
    ")\n",
    "\n",
    "decoder_outputs, decoder_state = decoder_rnn(\n",
    "    decoder_inputs_embedded, initial_state=encoder_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the encoder-decoder architecture is a softmax `Dense` layer that will create the next word probability vector or next word `predictions` from the `decoder_output`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder_dense = Dense(TARGET_VOCAB_SIZE, activation=\"softmax\")\n",
    "\n",
    "predictions = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to train the encoder-decoder network defined above, we now need to create a trainable Keras `Model` by specifying which are the `inputs` and the `outputs` of our problem. They should correspond exactly to what the type of input/output in our train and eval `tf.data.Dataset` since that's what will be fed to the `inputs` and `outputs` we declare while instantiating the Keras `Model`.\n",
    "\n",
    "While compiling our model, we should make sure that the loss is the `sparse_categorical_crossentropy` so that we can compare the true word indices for the target language as outputted by our train `tf.data.Dataset` with the next word `predictions` vector as outputted by the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,409,984</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,263,360</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)]            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │            │ gru_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)]            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,058,375</span> │ gru_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">4935</span>)             │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m2,409,984\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m1,263,360\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m3,938,304\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │            │                   │\n",
       "│                     │ \u001b[38;5;34m1024\u001b[0m)]            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m3,938,304\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │            │ gru_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]       │\n",
       "│                     │ \u001b[38;5;34m1024\u001b[0m)]            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m5,058,375\u001b[0m │ gru_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│                     │ \u001b[38;5;34m4935\u001b[0m)             │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,608,327</span> (63.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,608,327\u001b[0m (63.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,608,327</span> (63.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,608,327\u001b[0m (63.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO 2c\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=predictions)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1242s\u001b[0m 3s/step - loss: 2.6807 - val_loss: 4.6698\n"
     ]
    }
   ],
   "source": [
    "STEPS_PER_EPOCH = len(input_tensor_train) // BATCH_SIZE\n",
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=eval_batches,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the translation (or decoding) function\n",
    "\n",
    "We can't just use model(), because we don't know all the inputs we used during training. We only know the encoder_input (source language) but not the decoder_input (target language), which is what we want to predict (i.e., the translation of the source language)!\n",
    "\n",
    "We do however know the first token of the decoder input, which is the `<start>` token. So using this plus the state of the encoder RNN, we can predict the next token. We will then use that token to be the second token of decoder input, and continue like this until we predict the `<end>` token, or we reach some defined max length.\n",
    "\n",
    "So, the strategy now is to split our trained network into two independent Keras models:\n",
    "\n",
    "* an **encoder model** with signature `encoder_inputs -> encoder_state`\n",
    "* a **decoder model** with signature `[decoder_inputs, decoder_state_input] -> [predictions, decoder_state]`\n",
    "\n",
    "This way, we will be able to encode the source language sentence into the vector `encoder_state` using the encoder and feed it to the decoder model along with the `<start>` token at step 1. \n",
    "\n",
    "Given that input, the decoder will produce the first word of the translation, by sampling from the `predictions` vector (for simplicity, our sampling strategy here will be to take the next word to be the one whose index has the maximum probability in the `predictions` vector) along with a new state vector, the `decoder_state`. \n",
    "\n",
    "At this point, we can feed again to the decoder the predicted first word and as well as the new `decoder_state` to predict the translation second word. \n",
    "\n",
    "This process can be continued until the decoder produces the token `<stop>`. \n",
    "\n",
    "This is how we will implement our translation (or decoding) function, but let us first extract a separate encoder and a separate decoder from our trained encoder-decoder model. \n",
    "\n",
    "\n",
    "**Remark:** If we have already trained and saved the models (i.e, `LOAD_CHECKPOINT` is `True`) we will just load the models, otherwise, we extract them from the trained network above by explicitly creating the encoder and decoder Keras `Model`s with the signature we want.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CHECKPOINT:\n",
    "    encoder_model = load_model(os.path.join(MODEL_PATH, \"encoder_model\"))\n",
    "    decoder_model = load_model(os.path.join(MODEL_PATH, \"decoder_model\"))\n",
    "\n",
    "else:\n",
    "    # TODO 3a\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_state)\n",
    "\n",
    "    decoder_state_input = Input(\n",
    "        shape=(HIDDEN_UNITS,), name=\"decoder_state_input\"\n",
    "    )\n",
    "\n",
    "    # Reuses weights from the decoder_rnn layer\n",
    "    decoder_outputs, decoder_state = decoder_rnn(\n",
    "        decoder_inputs_embedded, initial_state=decoder_state_input\n",
    "    )\n",
    "\n",
    "    # Reuses weights from the decoder_dense layer\n",
    "    predictions = decoder_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model(\n",
    "        inputs=[decoder_inputs, decoder_state_input],\n",
    "        outputs=[predictions, decoder_state],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Create Inference Encoder Model ---\n",
    "# This model will convert raw Spanish sentences into the encoder's final state.\n",
    "\n",
    "# Input layer for raw strings\n",
    "encoder_string_input = keras.Input(\n",
    "    shape=(1,), dtype=\"string\", name=\"encoder_string_input\"\n",
    ")\n",
    "\n",
    "# Vectorize the strings using the adapted layer\n",
    "encoder_vectorized = source_vectorization(encoder_string_input)\n",
    "\n",
    "# --- FIX: Reuse the trained Embedding layer by its correct name ---\n",
    "encoder_embedding_layer = model.get_layer(\"embedding_2\")\n",
    "encoder_embedded = encoder_embedding_layer(encoder_vectorized)\n",
    "\n",
    "# --- FIX: Reuse the trained GRU layer by its correct name ---\n",
    "encoder_gru_layer = model.get_layer(\"gru_2\")\n",
    "_, encoder_state_output = encoder_gru_layer(encoder_embedded)\n",
    "\n",
    "# Create the final encoder model for inference\n",
    "encoder_model = keras.Model(\n",
    "    inputs=encoder_string_input,\n",
    "    outputs=encoder_state_output,\n",
    "    name=\"inference_encoder\",\n",
    ")\n",
    "\n",
    "\n",
    "# --- Create Inference Decoder Model ---\n",
    "# This model's structure is similar to the original, as it works with token IDs.\n",
    "\n",
    "# Input layers for the decoder\n",
    "decoder_input = keras.Input(shape=(None,), name=\"decoder_input\")\n",
    "decoder_state_input = keras.Input(\n",
    "    shape=(HIDDEN_UNITS,), name=\"decoder_state_input\"\n",
    ")\n",
    "\n",
    "# --- FIX: Reuse trained layers from the main model by their correct names ---\n",
    "decoder_embedding_layer = model.get_layer(\"embedding_3\")\n",
    "decoder_gru_layer = model.get_layer(\"gru_3\")\n",
    "decoder_dense_layer = model.get_layer(\"dense_1\")\n",
    "\n",
    "# Build the decoder graph\n",
    "decoder_embedded = decoder_embedding_layer(decoder_input)\n",
    "decoder_outputs, decoder_state_output = decoder_gru_layer(\n",
    "    decoder_embedded, initial_state=decoder_state_input\n",
    ")\n",
    "decoder_predictions = decoder_dense_layer(decoder_outputs)\n",
    "\n",
    "# Create the final decoder model for inference\n",
    "decoder_model = keras.Model(\n",
    "    inputs=[decoder_input, decoder_state_input],\n",
    "    outputs=[decoder_predictions, decoder_state_output],\n",
    "    name=\"inference_decoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a separate encoder and a separate decoder, let's implement a translation function, to which we will give the generic name of `decode_sequences` (to stress that this procedure is general to all seq2seq problems). \n",
    "\n",
    "`decode_sequences` will take as input\n",
    "* `input_seqs` which is the integerized source language sentence tensor that the encoder can consume\n",
    "* `output_tokenizer` which is the target languague tokenizer we will need to extract back words from predicted word integers\n",
    "* `max_decode_length` which is the length after which we stop decoding if the `<stop>` token has not been predicted\n",
    "\n",
    "\n",
    "**Note**: Now that the encoder and decoder have been turned into Keras models, to feed them their input, we need to use the `.predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the vocabulary and start/end token IDs from the adapted TextVectorization layer\n",
    "targ_vocab = target_vectorization.get_vocabulary()\n",
    "targ_index_lookup = dict(zip(range(len(targ_vocab)), targ_vocab))\n",
    "start_token_id = target_vectorization.get_vocabulary().index(\"<start>\")\n",
    "end_token_id = target_vectorization.get_vocabulary().index(\"<end>\")\n",
    "\n",
    "\n",
    "def decode_sequences(input_sentences, max_decode_length=50):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        input_sentences: A list of raw strings in the source language.\n",
    "    Returns:\n",
    "        A list of translated sentences.\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    # Encode the input strings to get the initial state.\n",
    "    states_value = encoder_model(input_sentences)\n",
    "\n",
    "    # Initialize the target sequence with the <start> token ID.\n",
    "    target_seq = tf.fill([batch_size, 1], start_token_id)\n",
    "    decoded_sentences = [\"\" for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(max_decode_length):\n",
    "        output_tokens, decoder_state = decoder_model([target_seq, states_value])\n",
    "\n",
    "        # Sample a token (we use argmax for simplicity)\n",
    "        sampled_token_index = tf.argmax(output_tokens[:, -1, :], axis=-1)\n",
    "\n",
    "        for j in range(batch_size.numpy()):\n",
    "            token = targ_index_lookup[sampled_token_index[j].numpy()]\n",
    "            if token == \"<end>\":\n",
    "                continue  # Don't add the end token to the output\n",
    "            decoded_sentences[j] += token + \" \"\n",
    "\n",
    "        # Update the target sequence for the next iteration\n",
    "        target_seq = tf.expand_dims(sampled_token_index, axis=-1)\n",
    "\n",
    "        # Update the decoder state\n",
    "        states_value = decoder_state\n",
    "\n",
    "    return [s.strip() for s in decoded_sentences]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sentences = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "]\n",
    "\n",
    "machine_translations = decode_sequences(tf.constant(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i m not a lot .    <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start>',\n",
       " 'he s a doctor .     <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start>']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_seqs, output_tokenizer, max_decode_length=50):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input_seqs: int tensor of shape (BATCH_SIZE, SEQ_LEN)\n",
    "    output_tokenizer: Tokenizer used to conver from int to words\n",
    "\n",
    "    Returns translated sentences\n",
    "    \"\"\"\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model(input_seqs)\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    batch_size = input_seqs.shape[0]\n",
    "    target_seq = tf.ones([batch_size, 1])\n",
    "\n",
    "    decoded_sentences = [[] for _ in range(batch_size)]\n",
    "\n",
    "    # TODO 4: Sampling loop\n",
    "    for i in range(max_decode_length):\n",
    "        output_tokens, decoder_state = decoder_model([target_seq, states_value])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[:, -1, :], axis=-1)\n",
    "\n",
    "        tokens = utils_preproc.int2word(output_tokenizer, sampled_token_index)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            decoded_sentences[j].append(tokens[j])\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = tf.expand_dims(tf.constant(sampled_token_index), axis=-1)\n",
    "\n",
    "        # Update states\n",
    "        states_value = decoder_state\n",
    "\n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "INPUT:\n",
      "No estamos comiendo.\n",
      "REFERENCE TRANSLATION:\n",
      "We're not eating.\n",
      "MACHINE TRANSLATION:\n",
      "i m not a lot .    <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start>\n",
      "-\n",
      "INPUT:\n",
      "Está llegando el invierno.\n",
      "REFERENCE TRANSLATION:\n",
      "Winter is coming.\n",
      "MACHINE TRANSLATION:\n",
      "he s a doctor .     <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start>\n",
      "-\n",
      "INPUT:\n",
      "El invierno se acerca.\n",
      "REFERENCE TRANSLATION:\n",
      "Winter is coming.\n",
      "MACHINE TRANSLATION:\n",
      "he s a doctor .     <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start>\n",
      "-\n",
      "INPUT:\n",
      "Tom no comio nada.\n",
      "REFERENCE TRANSLATION:\n",
      "Tom ate nothing.\n",
      "MACHINE TRANSLATION:\n",
      "tom is a lot .     <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start>\n",
      "-\n",
      "INPUT:\n",
      "Su pierna mala le impidió ganar la carrera.\n",
      "REFERENCE TRANSLATION:\n",
      "His bad leg prevented him from winning the race.\n",
      "MACHINE TRANSLATION:\n",
      "he s a little man .    <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start>\n",
      "-\n",
      "INPUT:\n",
      "Su respuesta es erronea.\n",
      "REFERENCE TRANSLATION:\n",
      "Your answer is wrong.\n",
      "MACHINE TRANSLATION:\n",
      "it s a good .     <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start>\n",
      "-\n",
      "INPUT:\n",
      "¿Qué tal si damos un paseo después del almuerzo?\n",
      "REFERENCE TRANSLATION:\n",
      "How about going for a walk after lunch?\n",
      "MACHINE TRANSLATION:\n",
      "why is you ?      <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start>\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "    \"El invierno se acerca.\",\n",
    "    \"Tom no comio nada.\",\n",
    "    \"Su pierna mala le impidió ganar la carrera.\",\n",
    "    \"Su respuesta es erronea.\",\n",
    "    \"¿Qué tal si damos un paseo después del almuerzo?\",\n",
    "]\n",
    "\n",
    "reference_translations = [\n",
    "    \"We're not eating.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Tom ate nothing.\",\n",
    "    \"His bad leg prevented him from winning the race.\",\n",
    "    \"Your answer is wrong.\",\n",
    "    \"How about going for a walk after lunch?\",\n",
    "]\n",
    "\n",
    "machine_translations = decode_sequences(tf.constant(sentences))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(\"-\")\n",
    "    print(\"INPUT:\")\n",
    "    print(sentences[i])\n",
    "    print(\"REFERENCE TRANSLATION:\")\n",
    "    print(reference_translations[i])\n",
    "    print(\"MACHINE TRANSLATION:\")\n",
    "    print(machine_translations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's us save the full training encoder-decoder model, as well as the separate encoder and decoder model to disk for latter reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_CHECKPOINT:\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "    # TODO 3b\n",
    "    model.save(os.path.join(MODEL_PATH, \"model\"))\n",
    "    encoder_model.save(os.path.join(MODEL_PATH, \"encoder_model\"))\n",
    "    decoder_model.save(os.path.join(MODEL_PATH, \"decoder_model\"))\n",
    "\n",
    "    with open(os.path.join(MODEL_PATH, \"encoder_tokenizer.pkl\"), \"wb\") as fp:\n",
    "        pickle.dump(inp_lang, fp)\n",
    "\n",
    "    with open(os.path.join(MODEL_PATH, \"decoder_tokenizer.pkl\"), \"wb\") as fp:\n",
    "        pickle.dump(targ_lang, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric (BLEU)\n",
    "\n",
    "Unlike say, image classification, there is no one right answer for a machine translation. However our current loss metric, cross entropy, only gives credit when the machine translation matches the exact same word in the same order as the reference translation. \n",
    "\n",
    "Many attempts have been made to develop a better metric for natural language evaluation. The most popular currently is Bilingual Evaluation Understudy (BLEU).\n",
    "\n",
    "- It is quick and inexpensive to calculate.\n",
    "- It allows flexibility for the ordering of words and phrases.\n",
    "- It is easy to understand.\n",
    "- It is language independent.\n",
    "- It correlates highly with human evaluation.\n",
    "- It has been widely adopted.\n",
    "\n",
    "The score is from 0 to 1, where 1 is an exact match.\n",
    "\n",
    "It works by counting matching n-grams between the machine and reference texts, regardless of order. BLUE-4 counts matching n grams from 1-4 (1-gram, 2-gram, 3-gram and 4-gram). It is common to report both BLUE-1 and BLUE-4\n",
    "\n",
    "It still is imperfect, since it gives no credit to synonyms and so human evaluation is still best when feasible. However BLEU is commonly considered the best among bad options for an automated metric.\n",
    "\n",
    "The Hugging Face evaluate framework has an implementation that we will use.\n",
    "\n",
    "We can't run calculate BLEU during training, because at that time the correct decoder input is used. Instead we'll calculate it now.\n",
    "\n",
    "For more info: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess(sentence):\n",
    "    filtered = list(filter(lambda x: x != \"\" and x != \"<end>\", sentence))\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now average the `bleu_1` and `bleu_4` scores for all the sentence pairs in the eval set. The next cell takes around 1 minute (8 minutes for full dataset eval) to run, the bulk of which is decoding the sentences in the validation set. Please wait until completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EVALUATE = 1000  # `len(input_tensor_val)` for full eval.\n",
    "\n",
    "reference = []\n",
    "candidate = []\n",
    "\n",
    "for idx in tqdm(range(NUM_EVALUATE)):\n",
    "    reference_sentence = utils_preproc.int2word(\n",
    "        targ_lang, target_tensor_val[idx][1:]\n",
    "    )\n",
    "\n",
    "    decoded_sentence = decode_sequences(\n",
    "        input_tensor_val[idx : idx + 1], targ_lang, max_length_targ\n",
    "    )[0]\n",
    "\n",
    "    candidate.append(postprocess(decoded_sentence))\n",
    "    reference.append([postprocess(reference_sentence)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_1 = bleu.compute(predictions=candidate, references=reference, max_order=1)\n",
    "bleu_4 = bleu.compute(predictions=candidate, references=reference, max_order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bleu_1[\"bleu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bleu_4[\"bleu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Francois Chollet: https://github.com/keras-team/keras-io/blob/master/examples/nlp/lstm_seq2seq.py\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

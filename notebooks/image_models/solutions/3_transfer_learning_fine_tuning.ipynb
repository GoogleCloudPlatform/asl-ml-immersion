{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning and Fine-tuning with Keras Hub\n",
    "This notebook demonstrates how to use **transfer learning** and **fine-tuning** with pre-trained models from [Keras Hub](https://keras.io/keras_hub/) to achieve better image classification accuracy than training a model from scratch. This is a common and effective strategy when you have a small dataset, limited computational resources, or insufficient time to train a large model from the beginning.\n",
    "\n",
    "**Learning Objectives**\n",
    "- **Transfer Learning**: Learn how to leverage a pre-trained model as a feature extractor.\n",
    "- **Fine-tuning**: Understand how to fine-tune a pre-trained model's weights for a specific task.\n",
    "- **Learning Rate Scheduling**: Discover how a learning rate schedule can be used to achieve stable and effective fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import IPython.display as display\n",
    "import keras\n",
    "import keras_hub\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPooling2D,\n",
    "    Softmax,\n",
    ")\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "\n",
    "As usual, let's take a look at the data before we start building our model. We'll be using a creative-commons licensed flower photo dataset of 3670 images falling into 5 categories: 'daisy', 'roses', 'dandelion', 'sunflowers', and 'tulips'.\n",
    "\n",
    "The below [keras.utils.get_file](https://keras.io/api/utils/python_utils/#getfile-function) command downloads a dataset to the local Keras cache. To see the files through a terminal, copy the output of the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = keras.utils.get_file(\n",
    "    \"flower_photos\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\",\n",
    "    untar=True,\n",
    ")\n",
    "\n",
    "# Print data path\n",
    "print(\"cd\", data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use python's built in [pathlib](https://docs.python.org/3/library/pathlib.html) tool to get a sense of this unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir_path = pathlib.Path(data_dir)\n",
    "\n",
    "image_count = len(list(data_dir_path.glob(\"*/*/*.jpg\")))\n",
    "print(\"There are\", image_count, \"images.\")\n",
    "\n",
    "CLASS_NAMES = np.array(\n",
    "    [\n",
    "        item.name\n",
    "        for item in data_dir_path.glob(\"*/*\")\n",
    "        if item.name != \"LICENSE.txt\"\n",
    "    ]\n",
    ")\n",
    "print(\"These are the available classes:\", CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display some images so we can see what our model will be trying to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roses = list(data_dir_path.glob(\"*/roses/*\"))\n",
    "\n",
    "for image_path in roses[:3]:\n",
    "    display.display(Image.open(str(image_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dataset\n",
    "For training a machine learning model, the data must be organized and loaded efficiently. Here we use [keras.utils.image_dataset_from_directory](https://keras.io/api/data_loading/image/#imagedatasetfromdirectory-function) to create a tf.data.Dataset object. \n",
    "\n",
    "Ths method assumes the data is stored in this directory structure:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "main_directory/\n",
    "...class_a/\n",
    "......a_image_1.jpg\n",
    "......a_image_2.jpg\n",
    "...class_b/\n",
    "......b_image_1.jpg\n",
    "......b_image_2.jpg\n",
    "```\n",
    "\n",
    "The utility automatically handles:\n",
    "- File Discovery: It scans the directory structure to find all image files.\n",
    "- Labeling: It infers the class labels from the subdirectory names (e.g., roses, tulips).\n",
    "- Splitting: It divides the data into training (80%) and validation (20%) subsets as specified by `validation_split` and return both subsets (`subset=\"both\"`).\n",
    "- Resizing: It resizes all images to a consistent size (224x224 pixels) to be compatible with the model architecture we'll use later.\n",
    "- Batching: It groups the images into batches for efficient training, with a batch size of 32.\n",
    "\n",
    "This process ensures the data is ready for model consumption without the need for manual loops or complex file handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "# 10 is a magic number tuned for local training of this dataset.\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "SEED = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds, eval_ds = keras.utils.image_dataset_from_directory(\n",
    "    f\"{data_dir}/flower_photos\",\n",
    "    seed=SEED,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"both\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    label_mode=\"categorical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN Model\n",
    "\n",
    "These flower photos are much larger than handwritting recognition images in MNIST. They are about 10 times as many pixels per axis **and** there are three color channels, making the information here over 200 times larger!\n",
    "\n",
    "We've also added a `Rescaling` layer to rescale the pixel range from [0,255] to [0,1]. We can apply this operation in a data pipeline, but we can avoid training-serving schew by incorporating it in the model itself.\n",
    "\n",
    "How do our current techniques stand up? Copy your best model architecture over from the <a href=\"2_mnist_cnn.ipynb\">MNIST models lab</a> and see how well it does after training for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nclasses = len(CLASS_NAMES)\n",
    "hidden_layer_1_neurons = 400\n",
    "hidden_layer_2_neurons = 100\n",
    "dropout_rate = 0.25\n",
    "num_filters_1 = 64\n",
    "kernel_size_1 = 3\n",
    "pooling_size_1 = 2\n",
    "num_filters_2 = 32\n",
    "kernel_size_2 = 3\n",
    "pooling_size_2 = 2\n",
    "\n",
    "layers = [\n",
    "    keras.layers.Rescaling(scale=1.0 / 255),\n",
    "    Conv2D(\n",
    "        num_filters_1,\n",
    "        kernel_size=kernel_size_1,\n",
    "        activation=\"relu\",\n",
    "        input_shape=(IMG_WIDTH, IMG_HEIGHT, 3),\n",
    "    ),\n",
    "    MaxPooling2D(pooling_size_1),\n",
    "    Conv2D(num_filters_2, kernel_size=kernel_size_2, activation=\"relu\"),\n",
    "    MaxPooling2D(pooling_size_2),\n",
    "    Flatten(),\n",
    "    Dense(hidden_layer_1_neurons, activation=\"relu\"),\n",
    "    Dense(hidden_layer_2_neurons, activation=\"relu\"),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(nclasses),\n",
    "    Softmax(),\n",
    "]\n",
    "\n",
    "cnn_model = Sequential(layers)\n",
    "cnn_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = cnn_model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=eval_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACCURACY_COLS = [\"accuracy\", \"val_accuracy\"]\n",
    "_ = pd.DataFrame(history.history)[ACCURACY_COLS].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with Keras Hub\n",
    "\n",
    "If your model is like mine, it learns a little bit, slightly better then random, but *ugh*, it's overfitting! Since our model is too shallow for this dataset, it may be using very low-level features (like colors) and failing to capture the overall semantics.\n",
    "\n",
    "To overcome the limitations of the simple CNN, we introduce transfer learning. \n",
    "\n",
    "We're leveraging a pre-trained image model—MobileNetV3 from Keras Hub—and repurposing it for our task. This model is ideal because it's lightweight and built for mobile inference.\n",
    "\n",
    "We can retrieve a pre-trained feature extractor object by specifying a model identifier in the `keras_hub.models.Backbone.from_preset()` function. Please refer to [the document](https://keras.io/keras_hub/presets/) for available models.\n",
    "\n",
    "Here we set `backbone.trainable` to `False` for transfer learning, so the millions of weights in the pre-trained model are not updated during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone = keras_hub.models.Backbone.from_preset(\n",
    "    \"mobilenet_v3_large_100_imagenet_21k\",\n",
    ")\n",
    "backbone.trainable = False\n",
    "\n",
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the \"Trainable params\" is 0 in the backbone.\n",
    "\n",
    "Instead, a new classification head (a simple Dense layer) is added on top, and only this new head is trained to classify the flower images. \n",
    "\n",
    "This approach leverages the powerful, pre-learned features of the backbone to achieve a strong performance baseline quickly, without the risk of overfitting the small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transfer_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        keras.layers.Rescaling(scale=1.0 / 255),\n",
    "        backbone,\n",
    "        keras.layers.GlobalMaxPooling2D(),\n",
    "        keras.layers.Dropout(rate=0.2),\n",
    "        keras.layers.Dense(\n",
    "            nclasses,\n",
    "            activation=\"softmax\",\n",
    "            kernel_regularizer=keras.regularizers.l2(0.0001),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# transfer_model.build((None,) + (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transfer_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = transfer_model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=eval_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACCURACY_COLS = [\"accuracy\", \"val_accuracy\"]\n",
    "_ = pd.DataFrame(history.history)[ACCURACY_COLS].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, looking better! Still, there's room to improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning MobileNet\n",
    "Following the transfer learning step, let's see how fine-tuning works. Here we unfreeze the backbone and train with our classification head. \n",
    "\n",
    "Note that we **don't** set `backbone.trainable` to `False` to leave it trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone = keras_hub.models.Backbone.from_preset(\n",
    "    \"mobilenet_v3_large_100_imagenet_21k\",\n",
    ")\n",
    "\n",
    "# Leave trainable True for fine-tuning\n",
    "# backbone.trainable = False\n",
    "\n",
    "finetune_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        keras.layers.Rescaling(scale=1.0 / 255),\n",
    "        backbone,\n",
    "        keras.layers.GlobalMaxPooling2D(),\n",
    "        keras.layers.Dropout(rate=0.2),\n",
    "        keras.layers.Dense(\n",
    "            nclasses,\n",
    "            activation=\"softmax\",\n",
    "            kernel_regularizer=keras.regularizers.l2(0.0001),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "finetune_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = finetune_model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=eval_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACCURACY_COLS = [\"accuracy\", \"val_accuracy\"]\n",
    "_ = pd.DataFrame(history.history)[ACCURACY_COLS].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the training accuracy looks better than the previous case, the validation accuracy remains the same.\n",
    "\n",
    "Because now we are tuning a lot of parameters in Mobilenet, it easily overfits to this small dataset.\n",
    "\n",
    "Furthermore, the default learning rate might have corrupted the knowledge MobileNet acquired during pre-training. A careful selection of the learning rate is crucial for effective fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Schedule for Fine-tuning\n",
    "\n",
    "Now, let's add an additional technique for fine-tuning.\n",
    "Since fine-tuning is very sensitive to the learning rate, it is important to use a carefully designed **learning rate schedule**.\n",
    "\n",
    "\n",
    "A typical learning rate schedule involves two phases: \n",
    "- **Warmup Phase**: The learning rate starts small and increases slowly to prevent breaking pre-trained model patterns and to ease into the new task\n",
    "- **Decay Phase**: The learning rate is gradually reduced for finer tuning and improved convergence\n",
    "\n",
    "Here we design this schedule using `keras.optimizers.schedules.CosineDecay`, where we can specify the warmup period flexibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "warmup_tgt = 0.001\n",
    "scheduler = keras.optimizers.schedules.CosineDecay(\n",
    "    0.0,\n",
    "    num_train_steps,\n",
    "    warmup_target=warmup_tgt,\n",
    "    warmup_steps=num_warmup_steps,\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot([scheduler(lr) for lr in range(num_train_steps)])\n",
    "plt.title(\"Learning Rate Schedule\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone = keras_hub.models.Backbone.from_preset(\n",
    "    \"mobilenet_v3_large_100_imagenet_21k\",\n",
    ")\n",
    "\n",
    "ft_lr_schedule_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        keras.layers.Rescaling(scale=1.0 / 255),\n",
    "        backbone,\n",
    "        keras.layers.GlobalMaxPooling2D(),\n",
    "        keras.layers.Dropout(rate=0.2),\n",
    "        keras.layers.Dense(\n",
    "            nclasses,\n",
    "            activation=\"softmax\",\n",
    "            kernel_regularizer=keras.regularizers.l2(0.0001),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the same model using the new learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ft_lr_schedule_model.compile(\n",
    "    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = ft_lr_schedule_model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=eval_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACCURACY_COLS = [\"accuracy\", \"val_accuracy\"]\n",
    "_ = pd.DataFrame(history.history)[ACCURACY_COLS].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the new learning rate schedule, we're now seeing better training and validation performance, as well as a more stable learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

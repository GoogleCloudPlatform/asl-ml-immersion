{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Using Cloud Composer to orchestrate Kubeflow pipeline on Vertex AI"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E**Learning Objectives:**\n",
    "1. Learn how to create a custom DAG for Cloud Composer\n",
    "2. Learn how to use Airflow operators for Vertex AI Pipelines\n",
    "3. Learn how to orchestrate Vertex AI Pipelines with existing ETL (Extract, Transform, Load) pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the Python code for an Airflow DAG creation and shows how you *would* programmatically interact with Google Cloud Storage to upload DAG files.\n",
    "**Important Notes:**\n",
    "1.  **DAG Execution:** Airflow DAGs are typically uploaded directly to your Cloud Composer environment's GCS DAGs folder. Airflow workers then discover and parse these files. You do *not* run the DAG code directly from this notebook to execute the Airflow workflow.\n",
    "2.  **Authentication:** To run the GCS upload code, ensure your Vertex AI Workbench environment (e.g., Colab, a local Jupyter server with `gcloud` authenticated) has the necessary Google Cloud permissions to write to your Composer DAGs bucket.\n",
    "3.  **Cloud Composer DAGs Folder:** The target GCS path for DAGs in Cloud Composer is usually `gs://YOUR_COMPOSER_BUCKET/dags/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring environment settings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-06-04T16:33:16.668395Z",
     "start_time": "2025-06-04T16:33:13.531901Z"
    }
   },
   "source": [
    "import os\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\"\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"ARTIFACT_STORE\"] = ARTIFACT_STORE\n",
    "VERTEX_AI_PIPELINE_YAML = \"gs://your-bucket-name/path/to/covertype_kfp_pipeline.yaml\" # TODO: Update path to your compiled KFP YAML\n",
    "GCS_SOURCE_DATASET_PATH = \"data/covertype/dataset.csv\"\n",
    "GCS_TRAIN_DATASET_PATH=\"gs://your-bucket-name/data/train_export.csv\"\n",
    "GCS_BUCKET_NAME=\"asl-public\"\n",
    "BIGQUERY_DATASET_ID=\"airflow_demo_dataset\"\n",
    "TABLE_ID=\"covertype\"\n",
    "\n",
    "BIGQUERY_TABLE_SCHEMA = (\n",
    "    [\n",
    "        {\"name\": \"Elevation\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Aspect\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Slope\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\n",
    "            \"name\": \"Horizontal_Distance_To_Hydrology\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Vertical_Distance_To_Hydrology\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Horizontal_Distance_To_Roadways\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\"name\": \"Hillshade_9am\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Hillshade_Noon\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Hillshade_3pm\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\n",
    "            \"name\": \"Horizontal_Distance_To_Fire_Points\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\"name\": \"Wilderness_Area\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Soil_Type\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Cover_Type\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "    ],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAG Code\n",
    "Below is the Airflow DAG code (`demo_vertex_ai_pipeline_integration.py`) that you intend to upload to your Cloud Composer environment."
   ]
  },
  {
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-06-04T16:38:39.978726Z",
     "start_time": "2025-06-04T16:38:39.901502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "airflow_dag_code = f\"\"\"\n",
    "import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.google.cloud.operators.vertex_ai.pipeline_job import (\n",
    "    DeletePipelineJobOperator,\n",
    "    GetPipelineJobOperator,\n",
    "    RunPipelineJobOperator,\n",
    ")\n",
    "from airflow.providers.google.cloud.transfers.bigquery_to_gcs import (\n",
    "    BigQueryToGCSOperator,\n",
    ")\n",
    "from airflow.providers.google.cloud.transfers.gcs_to_bigquery import (\n",
    "    GCSToBigqueryOperator,\n",
    ")\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "# Replace with your actual project and region\n",
    "PROJECT_ID = \"{PROJECT_ID}\"  # Update with your Project ID\n",
    "REGION = \"{REGION}\" # Update with your GCP Region\n",
    "\n",
    "# Path to a compiled kubeflow pipeline yaml\n",
    "VERTEX_AI_PIPELINE_YAML = \"{VERTEX_AI_PIPELINE_YAML}\" # Update path to your compiled KFP YAML\n",
    "\n",
    "GCS_SOURCE_DATASET_PATH = \"{GCS_SOURCE_DATASET_PATH}\"\n",
    "GCS_BUCKET_NAME = {GCS_BUCKET_NAME} # This is a public bucket, if you use your own data, use your own bucket name\n",
    "\n",
    "GCS_TRAIN_DATASET_PATH = \"{GCS_TRAIN_DATASET_PATH}\" # Path for exported training data\n",
    "\n",
    "# Put your BigQuery dataset id here:\n",
    "BIGQUERY_DATASET_ID = \"{BIGQUERY_DATASET_ID}\"\n",
    "TABLE_ID = \"{TABLE_ID}\"\n",
    "\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"demo_vertex_ai_pipeline_integration\",\n",
    "    start_date=datetime.datetime(2025, 1, 1),\n",
    "    schedule=None,\n",
    "    catchup=False,\n",
    "    tags=[\"vertex_ai\", \"pipeline\", \"ml\"],\n",
    "    params={\n",
    "        \"gcs_train_dataset_path\": GCS_TRAIN_DATASET_PATH,\n",
    "    },\n",
    ") as dag:\n",
    "\n",
    "    # Loading dataset from GCS to BigQuery (Emulating basic ETL process)\n",
    "    load_gcs_to_bigquery = GCSToBigqueryOperator(\n",
    "        task_id=\"load_csv_to_bigquery\",\n",
    "        bucket=GCS_BUCKET_NAME,\n",
    "        source_objects=[GCS_SOURCE_DATASET_PATH],\n",
    "        destination_project_dataset_table=f'{BIGQUERY_DATASET_ID}.{TABLE_ID}',\n",
    "        # Optional: Define schema, remove if auto-detect works for you\n",
    "        schema_fields=BIGQUERY_TABLE_SCHEMA,\n",
    "        # Or \"NEWLINE_DELIMITED_JSON\", \"PARQUET\", \"AVRO\", etc.\n",
    "        source_format=\"CSV\",\n",
    "        # Creates the table if it doesn't exist\n",
    "        create_disposition=\"CREATE_IF_NEEDED\",\n",
    "        # Overwrites the table if it exists. Use \"WRITE_APPEND\" to append.\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        skip_leading_rows=1,  # For CSVs with a header row\n",
    "        field_delimiter=\",\",  # For CSVs\n",
    "    )\n",
    "\n",
    "    # exporting dataset from BigQuery to GCS\n",
    "    bigquery_to_gcs = BigQueryToGCSOperator(\n",
    "        task_id=\"bigquery_to_gcs_export\",\n",
    "        source_project_dataset_table=f\"{BIGQUERY_DATASET_ID}.{TABLE_ID}\",\n",
    "        destination_cloud_storage_uris=GCS_TRAIN_DATASET_PATH,\n",
    "        export_format=\"CSV\",\n",
    "        print_header=True,\n",
    "    )\n",
    "\n",
    "    # Triggering a pipeline from a GCS compiled yaml file\n",
    "    run_vertex_ai_pipeline = RunPipelineJobOperator(\n",
    "        task_id=\"start_vertex_ai_pipeline\",\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        template_path=VERTEX_AI_PIPELINE_YAML,\n",
    "        # example of passing params to kubeflow pipeline\n",
    "        parameter_values={\n",
    "            \"training_file_path\": \"{{ params.gcs_train_dataset_path }}\",\n",
    "        },\n",
    "        # Unique display name\n",
    "        display_name=\"triggered-demo-pipeline-{{ ts_nodash }}\",\n",
    "    )\n",
    "\n",
    "    # Fetching VertexAI pipeline job information\n",
    "    get_vertexai_ai_pipline_status = GetPipelineJobOperator(\n",
    "        task_id=\"vertex_ai_pipline_status\",\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        pipeline_job_id=\"{{ task_instance.xcom_pull(\"\n",
    "        \"task_ids='start_vertex_ai_pipeline', \"\n",
    "        \"key='pipeline_job_id') }}\",\n",
    "    )\n",
    "\n",
    "    # Deleting VertexAI pipeline job\n",
    "    delete_pipeline_job = DeletePipelineJobOperator(\n",
    "        task_id=\"delete_vertex_ai_pipeline_job\",\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        pipeline_job_id=\"{{ task_instance.xcom_pull(\"\n",
    "        \"task_ids='start_vertex_ai_pipeline', \"\n",
    "        \"key='pipeline_job_id') }}\",\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "    )\n",
    "\n",
    "    # Combine all steps into a DAG\n",
    "    (\n",
    "        load_gcs_to_bigquery\n",
    "        >> bigquery_to_gcs\n",
    "        >> run_vertex_ai_pipeline\n",
    "        >> get_vertexai_ai_pipline_status\n",
    "        >> delete_pipeline_job\n",
    "    )\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid format specifier ' \"Elevation\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"' for object of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m airflow_dag_code = \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[33mimport datetime\u001B[39m\n\u001B[32m      3\u001B[39m \n\u001B[32m      4\u001B[39m \u001B[33mfrom airflow import DAG\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[33mfrom airflow.providers.google.cloud.operators.vertex_ai.pipeline_job import (\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[33m    DeletePipelineJobOperator,\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[33m    GetPipelineJobOperator,\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[33m    RunPipelineJobOperator,\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[33m)\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[33mfrom airflow.providers.google.cloud.transfers.bigquery_to_gcs import (\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[33m    BigQueryToGCSOperator,\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[33m)\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[33mfrom airflow.providers.google.cloud.transfers.gcs_to_bigquery import (\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[33m    GCSToBigqueryOperator,\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[33m)\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[33mfrom airflow.utils.trigger_rule import TriggerRule\u001B[39m\n\u001B[32m     17\u001B[39m \n\u001B[32m     18\u001B[39m \u001B[33m# Replace with your actual project and region\u001B[39m\n\u001B[32m     19\u001B[39m \u001B[33mPROJECT_ID = \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mPROJECT_ID\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m  # Update with your Project ID\u001B[39m\n\u001B[32m     20\u001B[39m \u001B[33mREGION = \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mREGION\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m # Update with your GCP Region\u001B[39m\n\u001B[32m     21\u001B[39m \n\u001B[32m     22\u001B[39m \u001B[33m# Path to a compiled kubeflow pipeline yaml\u001B[39m\n\u001B[32m     23\u001B[39m \u001B[33mVERTEX_AI_PIPELINE_YAML = \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mVERTEX_AI_PIPELINE_YAML\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m # Update path to your compiled KFP YAML\u001B[39m\n\u001B[32m     24\u001B[39m \n\u001B[32m     25\u001B[39m \u001B[33mGCS_SOURCE_DATASET_PATH = \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mGCS_SOURCE_DATASET_PATH\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     26\u001B[39m \u001B[33mGCS_BUCKET_NAME = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mGCS_BUCKET_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m # This is a public bucket, if you use your own data, use your own bucket name\u001B[39m\n\u001B[32m     27\u001B[39m \n\u001B[32m     28\u001B[39m \u001B[33mGCS_TRAIN_DATASET_PATH = \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mGCS_TRAIN_DATASET_PATH\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m # Path for exported training data\u001B[39m\n\u001B[32m     29\u001B[39m \n\u001B[32m     30\u001B[39m \u001B[33m# Put your BigQuery dataset id here:\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[33mBIGQUERY_DATASET_ID = \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mBIGQUERY_DATASET_ID\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[33mTABLE_ID = \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTABLE_ID\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     33\u001B[39m \n\u001B[32m     34\u001B[39m \u001B[33mBIGQUERY_TABLE_SCHEMA = (\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[33m    [\u001B[39m\n\u001B[32m     36\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mElevation\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAspect\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSlope\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\n\u001B[32m     40\u001B[39m \u001B[38;5;250m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mHorizontal_Distance_To_Hydrology\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[33m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     42\u001B[39m \u001B[33m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     44\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\n\u001B[32m     45\u001B[39m \u001B[38;5;250m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mVertical_Distance_To_Hydrology\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     46\u001B[39m \u001B[33m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[33m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     49\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\n\u001B[32m     50\u001B[39m \u001B[38;5;250m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mHorizontal_Distance_To_Roadways\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[33m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     52\u001B[39m \u001B[33m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     53\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     54\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mHillshade_9am\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     55\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mHillshade_Noon\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     56\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mHillshade_3pm\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     57\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\n\u001B[32m     58\u001B[39m \u001B[38;5;250m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mHorizontal_Distance_To_Fire_Points\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     59\u001B[39m \u001B[33m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     60\u001B[39m \u001B[33m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     61\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     62\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mWilderness_Area\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSTRING\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     63\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSoil_Type\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSTRING\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     64\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCover_Type\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mINTEGER\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m: \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNULLABLE\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     65\u001B[39m \u001B[33m    ],\u001B[39m\n\u001B[32m     66\u001B[39m \u001B[33m)\u001B[39m\n\u001B[32m     67\u001B[39m \n\u001B[32m     68\u001B[39m \u001B[33mwith DAG(\u001B[39m\n\u001B[32m     69\u001B[39m \u001B[33m    dag_id=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdemo_vertex_ai_pipeline_integration\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     70\u001B[39m \u001B[33m    start_date=datetime.datetime(2025, 1, 1),\u001B[39m\n\u001B[32m     71\u001B[39m \u001B[33m    schedule=None,\u001B[39m\n\u001B[32m     72\u001B[39m \u001B[33m    catchup=False,\u001B[39m\n\u001B[32m     73\u001B[39m \u001B[33m    tags=[\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mvertex_ai\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mpipeline\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mml\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m],\u001B[39m\n\u001B[32m     74\u001B[39m \u001B[33m    params=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\n\u001B[32m     75\u001B[39m \u001B[38;5;250m        \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mgcs_train_dataset_path\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m GCS_TRAIN_DATASET_PATH,\u001B[39m\n\u001B[32m     76\u001B[39m \u001B[33m    \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m     77\u001B[39m \u001B[33m) as dag:\u001B[39m\n\u001B[32m     78\u001B[39m \n\u001B[32m     79\u001B[39m \u001B[33m    # Loading dataset from GCS to BigQuery (Emulating basic ETL process)\u001B[39m\n\u001B[32m     80\u001B[39m \u001B[33m    load_gcs_to_bigquery = GCSToBigqueryOperator(\u001B[39m\n\u001B[32m     81\u001B[39m \u001B[33m        task_id=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mload_csv_to_bigquery\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     82\u001B[39m \u001B[33m        bucket=GCS_BUCKET_NAME,\u001B[39m\n\u001B[32m     83\u001B[39m \u001B[33m        source_objects=[GCS_SOURCE_DATASET_PATH],\u001B[39m\n\u001B[32m     84\u001B[39m \u001B[33m        destination_project_dataset_table=f\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mBIGQUERY_DATASET_ID\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTABLE_ID\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     85\u001B[39m \u001B[33m        # Optional: Define schema, remove if auto-detect works for you\u001B[39m\n\u001B[32m     86\u001B[39m \u001B[33m        schema_fields=BIGQUERY_TABLE_SCHEMA,\u001B[39m\n\u001B[32m     87\u001B[39m \u001B[33m        # Or \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNEWLINE_DELIMITED_JSON\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPARQUET\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAVRO\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, etc.\u001B[39m\n\u001B[32m     88\u001B[39m \u001B[33m        source_format=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCSV\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     89\u001B[39m \u001B[33m        # Creates the table if it doesn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt exist\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[33m        create_disposition=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCREATE_IF_NEEDED\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     91\u001B[39m \u001B[33m        # Overwrites the table if it exists. Use \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mWRITE_APPEND\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m to append.\u001B[39m\n\u001B[32m     92\u001B[39m \u001B[33m        write_disposition=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mWRITE_TRUNCATE\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[33m        skip_leading_rows=1,  # For CSVs with a header row\u001B[39m\n\u001B[32m     94\u001B[39m \u001B[33m        field_delimiter=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,  # For CSVs\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[33m    )\u001B[39m\n\u001B[32m     96\u001B[39m \n\u001B[32m     97\u001B[39m \u001B[33m    # exporting dataset from BigQuery to GCS\u001B[39m\n\u001B[32m     98\u001B[39m \u001B[33m    bigquery_to_gcs = BigQueryToGCSOperator(\u001B[39m\n\u001B[32m     99\u001B[39m \u001B[33m        task_id=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mbigquery_to_gcs_export\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m    100\u001B[39m \u001B[33m        source_project_dataset_table=f\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mBIGQUERY_DATASET_ID\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTABLE_ID\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m    101\u001B[39m \u001B[33m        destination_cloud_storage_uris=GCS_TRAIN_DATASET_PATH,\u001B[39m\n\u001B[32m    102\u001B[39m \u001B[33m        export_format=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCSV\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m    103\u001B[39m \u001B[33m        print_header=True,\u001B[39m\n\u001B[32m    104\u001B[39m \u001B[33m    )\u001B[39m\n\u001B[32m    105\u001B[39m \n\u001B[32m    106\u001B[39m \u001B[33m    # Triggering a pipeline from a GCS compiled yaml file\u001B[39m\n\u001B[32m    107\u001B[39m \u001B[33m    run_vertex_ai_pipeline = RunPipelineJobOperator(\u001B[39m\n\u001B[32m    108\u001B[39m \u001B[33m        task_id=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mstart_vertex_ai_pipeline\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m    109\u001B[39m \u001B[33m        project_id=PROJECT_ID,\u001B[39m\n\u001B[32m    110\u001B[39m \u001B[33m        region=REGION,\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[33m        template_path=VERTEX_AI_PIPELINE_YAML,\u001B[39m\n\u001B[32m    112\u001B[39m \u001B[33m        # example of passing params to kubeflow pipeline\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[33m        parameter_values=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\n\u001B[32m    114\u001B[39m \u001B[38;5;250m            \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtraining_file_path\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m params.gcs_train_dataset_path \u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m    115\u001B[39m \u001B[33m        \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\n\u001B[32m    116\u001B[39m \u001B[33m        # Unique display name\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[33m        display_name=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtriggered-demo-pipeline-\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m ts_nodash \u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[33m    )\u001B[39m\n\u001B[32m    119\u001B[39m \n\u001B[32m    120\u001B[39m \u001B[33m    # Fetching VertexAI pipeline job information\u001B[39m\n\u001B[32m    121\u001B[39m \u001B[33m    get_vertexai_ai_pipline_status = GetPipelineJobOperator(\u001B[39m\n\u001B[32m    122\u001B[39m \u001B[33m        task_id=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mvertex_ai_pipline_status\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[33m        project_id=PROJECT_ID,\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[33m        region=REGION,\u001B[39m\n\u001B[32m    125\u001B[39m \u001B[33m        pipeline_job_id=\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m task_instance.xcom_pull(\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    126\u001B[39m \u001B[33m        \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtask_ids=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mstart_vertex_ai_pipeline\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    127\u001B[39m \u001B[33m        \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mkey=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mpipeline_job_id\u001B[39m\u001B[33m'\u001B[39m\u001B[33m) \u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[33m    )\u001B[39m\n\u001B[32m    129\u001B[39m \n\u001B[32m    130\u001B[39m \u001B[33m    # Deleting VertexAI pipeline job\u001B[39m\n\u001B[32m    131\u001B[39m \u001B[33m    delete_pipeline_job = DeletePipelineJobOperator(\u001B[39m\n\u001B[32m    132\u001B[39m \u001B[33m        task_id=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdelete_vertex_ai_pipeline_job\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m    133\u001B[39m \u001B[33m        project_id=PROJECT_ID,\u001B[39m\n\u001B[32m    134\u001B[39m \u001B[33m        region=REGION,\u001B[39m\n\u001B[32m    135\u001B[39m \u001B[33m        pipeline_job_id=\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m task_instance.xcom_pull(\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    136\u001B[39m \u001B[33m        \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtask_ids=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mstart_vertex_ai_pipeline\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    137\u001B[39m \u001B[33m        \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mkey=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mpipeline_job_id\u001B[39m\u001B[33m'\u001B[39m\u001B[33m) \u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m    138\u001B[39m \u001B[33m        trigger_rule=TriggerRule.ALL_DONE,\u001B[39m\n\u001B[32m    139\u001B[39m \u001B[33m    )\u001B[39m\n\u001B[32m    140\u001B[39m \n\u001B[32m    141\u001B[39m \u001B[33m    # Combine all steps into a DAG\u001B[39m\n\u001B[32m    142\u001B[39m \u001B[33m    (\u001B[39m\n\u001B[32m    143\u001B[39m \u001B[33m        load_gcs_to_bigquery\u001B[39m\n\u001B[32m    144\u001B[39m \u001B[33m        >> bigquery_to_gcs\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[33m        >> run_vertex_ai_pipeline\u001B[39m\n\u001B[32m    146\u001B[39m \u001B[33m        >> get_vertexai_ai_pipline_status\u001B[39m\n\u001B[32m    147\u001B[39m \u001B[33m        >> delete_pipeline_job\u001B[39m\n\u001B[32m    148\u001B[39m \u001B[33m    )\u001B[39m\n\u001B[32m    149\u001B[39m \u001B[33m\"\"\"\u001B[39m\n",
      "\u001B[31mValueError\u001B[39m: Invalid format specifier ' \"Elevation\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"' for object of type 'str'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "CAlso, this notebook assumes the Cloud Composer instance is already created by following the instructions covered in the [Run an Apache Airflow DAG in Cloud Composer](https://cloud.google.com/composer/docs/composer-3/run-apache-airflow-dag).\n",
    "\n",
    "If you haven't run it, please create Cloud Composer instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the DAG to a Local File\n",
    "First, let's save the DAG code to a local Python file."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dag_filename = \"demo_vertex_ai_pipeline_integration.py\"\n",
    "\n",
    "with open(dag_filename, \"w\") as f:\n",
    "    f.write(airflow_dag_code)\n",
    "\n",
    "print(f\"DAG saved locally as {dag_filename}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Uploading the DAG to Cloud Composer Storage\n",
    "\n",
    "To deploy this DAG to your Cloud Composer environment, you need to upload it to the DAGs folder in your Composer's associated Cloud Storage bucket.\n",
    "\n",
    "**Before running this cell, make sure you have:**\n",
    "1.  **Installed Google Cloud Storage client library:** `pip install google-cloud-storage`\n",
    "2.  **Authenticated:** Your environment needs to be authenticated to GCP (e.g., `gcloud auth application-default login` or running in a GCP VM/Cloud Run/Vertex AI Workbench).\n",
    "3.  **Identified your Composer DAGs bucket:** This is typically named `gs://us-central1-YOUR_COMPOSER_ENV_NAME-HASH-bucket/dags/`. You can find this in the Cloud Composer console.\n",
    "#"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

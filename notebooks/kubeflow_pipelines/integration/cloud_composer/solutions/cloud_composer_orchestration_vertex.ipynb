{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Using Cloud Composer to orchestrate Kubeflow pipeline on Vertex AI"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E**Learning Objectives:**\n",
    "1. Learn how to create a custom DAG for Cloud Composer\n",
    "2. Learn how to use Airflow operators for Vertex AI Pipelines\n",
    "3. Learn how to orchestrate Vertex AI Pipelines with existing ETL (Extract, Transform, Load) pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates an Airflow DAG creation and shows how you *would* programmatically interact with Google Cloud Storage to upload DAG files.\n",
    "**Important Notes:**\n",
    "1.  **DAG Execution:** Airflow DAGs are typically uploaded directly to your Cloud Composer environment's GCS DAGs folder. Airflow workers then discover and parse these files. You do *not* run the DAG code directly from this notebook to execute the Airflow workflow.\n",
    "2.  **Authentication:** To run the GCS upload code, ensure your Vertex AI Workbench environment (e.g., Colab or a local Jupyter server with `gcloud` authenticated) has the necessary Google Cloud permissions to write to your Composer DAGs bucket.\n",
    "3.  **Cloud Composer DAGs Folder:** The target GCS path for DAGs in Cloud Composer is usually `gs://YOUR_COMPOSER_BUCKET/dags/`.\n",
    "4. This notebook assumes the Cloud Composer instance is already created by following the instructions covered in the [Run an Apache Airflow DAG in Cloud Composer](https://cloud.google.com/composer/docs/composer-3/run-apache-airflow-dag). If you haven't run it, please create Cloud Composer instance using above instructions."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Open and edit Airflow DAG template: \"/dags/airflow_run_vertex_pipelines_dag.py\"**\n",
    "\n",
    "You need to edit provide actual values configuration for the constants:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-06-05T10:03:01.011236Z",
     "start_time": "2025-06-05T10:02:59.647990Z"
    }
   },
   "source": [
    "import os\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "os.environ[\"REGION\"] = REGION\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\"\n",
    "os.environ[\"ARTIFACT_STORE\"] = ARTIFACT_STORE\n",
    "VERTEX_AI_PIPELINE_YAML = \"gs://your-bucket-name/path/to/covertype_kfp_pipeline.yaml\" # TODO: Update path to your compiled KFP YAML\n",
    "GCS_SOURCE_DATASET_PATH = \"data/covertype/dataset.csv\"\n",
    "GCS_TRAIN_DATASET_PATH=\"gs://your-bucket-name/data/train_export.csv\"\n",
    "GCS_BUCKET_NAME=\"asl-public\"\n",
    "BIGQUERY_DATASET_ID=\"your-airflow_demo_dataset\"\n",
    "TABLE_ID=\"covertype\"\n",
    "\n",
    "BIGQUERY_TABLE_SCHEMA = (\n",
    "    [\n",
    "        {\"name\": \"Elevation\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Aspect\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Slope\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\n",
    "            \"name\": \"Horizontal_Distance_To_Hydrology\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Vertical_Distance_To_Hydrology\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Horizontal_Distance_To_Roadways\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\"name\": \"Hillshade_9am\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Hillshade_Noon\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Hillshade_3pm\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\n",
    "            \"name\": \"Horizontal_Distance_To_Fire_Points\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\"name\": \"Wilderness_Area\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Soil_Type\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Cover_Type\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "    ],\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([{'name': 'Elevation', 'type': 'INTEGER', 'mode': 'NULLABLE'}, {'name': 'Aspect', 'type': 'INTEGER', 'mode': 'NULLABLE'}, {'name': 'Slope', 'type': 'INTEGER', 'mode': 'NULLABLE'}, {'name': 'Horizontal_Distance_To_Hydrology', 'type': 'INTEGER', 'mode': 'NULLABLE'}, {'name': 'Vertical_Distance_To_Hydrology', 'type': 'INTEGER', 'mode': 'NULLABLE'}, {'name': 'Horizontal_Distance_To_Roadways', 'type': 'INTEGER', 'mode': 'NULLABLE'}, {'name': 'Hillshade_9am', 'type': 'INTEGER', 'mode': 'NULLABLE'}, {'name': 'Hillshade_Noon', 'type': 'INTEGER', 'mode': 'NULLABLE'}, {'name': 'Hillshade_3pm', 'type': 'INTEGER', 'mode': 'NULLABLE'}, {'name': 'Horizontal_Distance_To_Fire_Points', 'type': 'INTEGER', 'mode': 'NULLABLE'}, {'name': 'Wilderness_Area', 'type': 'STRING', 'mode': 'NULLABLE'}, {'name': 'Soil_Type', 'type': 'STRING', 'mode': 'NULLABLE'}, {'name': 'Cover_Type', 'type': 'INTEGER', 'mode': 'NULLABLE'}],)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAG Code\n",
    "Edit provided Airflow DAG code (`demo_vertex_ai_pipeline_integration.py`) that you intend to upload to your Cloud Composer environment."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Uploading the DAG to Cloud Composer Storage\n",
    "\n",
    "To deploy this DAG to your Cloud Composer environment, you need to upload it to the DAGs folder in your Composer's associated Cloud Storage bucket.\n",
    "\n",
    "**Before running this cell, make sure you have:**\n",
    "1.  **Installed Google Cloud Storage client library:** `pip install google-cloud-storage`\n",
    "2.  **Authenticated:** Your environment needs to be authenticated to GCP (e.g., `gcloud auth application-default login` or running in a GCP VM/Cloud Run/Vertex AI Workbench).\n",
    "3.  **Identified your Composer DAGs bucket:** This is typically named `gs://us-central1-YOUR_COMPOSER_ENV_NAME-HASH-bucket/dags/`. You can find this in the Cloud Composer console.\n",
    "#"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

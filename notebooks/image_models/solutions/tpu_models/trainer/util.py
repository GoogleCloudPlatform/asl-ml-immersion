import tensorflow as tf

IMG_HEIGHT = 224
IMG_WIDTH = 224
IMG_CHANNELS = 3

BATCH_SIZE = 32
# 10 is a magic number tuned for local training of this dataset.
SHUFFLE_BUFFER = 10 * BATCH_SIZE
AUTOTUNE = tf.data.experimental.AUTOTUNE
CLASS_NAMES = ["roses", "sunflowers", "tulips", "dandelion", "daisy"]

VALIDATION_IMAGES = 370
VALIDATION_STEPS = VALIDATION_IMAGES // BATCH_SIZE

CROP_SCALING = [IMG_HEIGHT + 10, IMG_WIDTH + 10]
MAX_DELTA = 63.0 / 255.0
CONTRAST_LOWER = 0.2
CONTRAST_UPPER = 1.8


def decode_img(img, reshape_dims):
    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)
    img = tf.image.convert_image_dtype(img, tf.float32)
    return tf.image.resize(img, reshape_dims)


def decode_csv(csv_row):
    record_defaults = ["path", "flower"]
    filename, label_string = tf.io.decode_csv(csv_row, record_defaults)
    image_bytes = tf.io.read_file(filename=filename)
    label = tf.math.equal(CLASS_NAMES, label_string)
    return image_bytes, label


def read_and_preprocess(image_bytes, label, random_augment=False):
    if random_augment:
        img = decode_img(image_bytes, CROP_SCALING)
        img = tf.image.random_crop(img, [IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])
        img = tf.image.random_flip_left_right(img)
        img = tf.image.random_brightness(img, MAX_DELTA)
        img = tf.image.random_contrast(img, CONTRAST_LOWER, CONTRAST_UPPER)
    else:
        img = decode_img(image_bytes, [IMG_WIDTH, IMG_HEIGHT])
    return img, label


def read_and_preprocess_with_augment(image_bytes, label):
    return read_and_preprocess(image_bytes, label, random_augment=True)


def load_dataset(csv_of_filenames, training=True):
    dataset = (
        tf.data.TextLineDataset(filenames=csv_of_filenames).map(decode_csv).cache()
    )

    if training:
        dataset = (
            dataset.map(read_and_preprocess_with_augment)
            .shuffle(SHUFFLE_BUFFER)
            .repeat(count=None)
        )
    else:
        dataset = dataset.map(read_and_preprocess).repeat(count=1)

    # Prefetch prepares the next set of batches while current batch is in use.
    return dataset.batch(batch_size=BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)

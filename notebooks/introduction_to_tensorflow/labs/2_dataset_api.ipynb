{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Dataset API\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Learn how use tf.data to read data from memory\n",
    "1. Learn how to use tf.data in a training loop\n",
    "1. Learn how use tf.data to read data from disk\n",
    "1. Learn how to write production input pipelines with feature engineering (batching, shuffling, etc.)\n",
    "\n",
    "\n",
    "In this notebook, we will start by refactoring the linear regression we implemented in the previous lab so that is takes its data from a`tf.data.Dataset`, and we will learn how to implement **stochastic gradient descent** with it. In this case, the original dataset will be synthetic and read by the `tf.data` API directly  from memory.\n",
    "\n",
    "In a second part, we will learn how to load a dataset with the `tf.data` API when the dataset resides on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the synthetic dataset of the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_POINTS = 10\n",
    "X = tf.constant(range(N_POINTS), dtype=tf.float32)\n",
    "Y = 2 * X + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with implementing a function that takes as input\n",
    "\n",
    "\n",
    "- our $X$ and $Y$ vectors of synthetic data generated by the linear function $y= 2x + 10$\n",
    "- the number of passes over the dataset we want to train on (`epochs`)\n",
    "- the size of the batches the dataset (`batch_size`)\n",
    "\n",
    "and returns a `tf.data.Dataset`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Note that the last batch may not contain the exact number of elements you specified because the dataset was exhausted.\n",
    "\n",
    "If you want batches with the exact same number of elements per batch, we will have to discard the last batch by\n",
    "setting:\n",
    "\n",
    "```python\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "```\n",
    "\n",
    "We will do that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #1:** Complete the code below to \n",
    "1. instantiate a `tf.data` dataset using [tf.data.Dataset.from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices).\n",
    "2. Set up the dataset to \n",
    "  * repeat `epochs` times,\n",
    "  * create a batch of size `batch_size`, ignoring extra elements when the batch does not divide the number of input elements evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(X, Y, epochs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X,Y))  # TODO: Your code here.\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function by iterating twice over our dataset in batches of 3 datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "x: [0. 1. 2.] y: [10. 12. 14.]\n",
      "1\n",
      "x: [3. 4. 5.] y: [16. 18. 20.]\n",
      "2\n",
      "x: [6. 7. 8.] y: [22. 24. 26.]\n",
      "3\n",
      "x: [9. 0. 1.] y: [28. 10. 12.]\n",
      "4\n",
      "x: [2. 3. 4.] y: [14. 16. 18.]\n",
      "5\n",
      "x: [5. 6. 7.] y: [20. 22. 24.]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 3\n",
    "EPOCH = 2\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    print(i)\n",
    "    print(\"x:\", x.numpy(), \"y:\", y.numpy())\n",
    "    assert len(x) == BATCH_SIZE\n",
    "    assert len(y) == BATCH_SIZE\n",
    "assert EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loss function and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function and the function that computes the gradients are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_mse(X, Y, w0, w1):\n",
    "    Y_hat = w0 * X + w1\n",
    "    errors = (Y_hat - Y) ** 2\n",
    "    return tf.reduce_mean(errors)\n",
    "\n",
    "\n",
    "def compute_gradients(X, Y, w0, w1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "    return tape.gradient(loss, [w0, w1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference now is that now, in the traning loop, we will iterate directly on the `tf.data.Dataset` generated by our `create_dataset` function. \n",
    "\n",
    "We will configure the dataset so that it iterates 250 times over our synthetic dataset in batches of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #2:** Complete the code in the cell below to call your dataset above when training the model. Note that the `step, (X_batch, Y_batch)` iterates over the `dataset`. The inside of the `for` loop should be exactly as in the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([10., 12.], dtype=float32)>)\n",
      "STEP 0 - loss: 35.70719528198242, w0: 4.079999923706055, w1: 0.7599999904632568\n",
      "STEP 100 - loss: 2.6017532348632812, w0: 2.4780430793762207, w1: 7.002389907836914\n",
      "STEP 200 - loss: 0.26831889152526855, w0: 2.153517961502075, w1: 9.037351608276367\n",
      "STEP 300 - loss: 0.027671903371810913, w0: 2.0493006706237793, w1: 9.690855979919434\n",
      "STEP 400 - loss: 0.0028539239428937435, w0: 2.0158326625823975, w1: 9.90071964263916\n",
      "STEP 500 - loss: 0.0002943490108009428, w0: 2.005084753036499, w1: 9.96811580657959\n",
      "STEP 600 - loss: 3.0356444767676294e-05, w0: 2.0016329288482666, w1: 9.989760398864746\n",
      "STEP 700 - loss: 3.1322738323069643e-06, w0: 2.0005245208740234, w1: 9.996710777282715\n",
      "STEP 800 - loss: 3.2238213520940917e-07, w0: 2.0001683235168457, w1: 9.998944282531738\n",
      "STEP 900 - loss: 3.369950718479231e-08, w0: 2.000054359436035, w1: 9.999658584594727\n",
      "STEP 1000 - loss: 3.6101481803996194e-09, w0: 2.0000178813934326, w1: 9.99988842010498\n",
      "STEP 1100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 1200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 1300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 1400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 1500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 1600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 1700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 1800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 1900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 2000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 2100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 2200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 2300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 2400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 2500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 2600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 2700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 2800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 2900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 3000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 3100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 3200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 3300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 3400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 3500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 3600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 3700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 3800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 3900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 4000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 4100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 4200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 4300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 4400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 4500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 4600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 4700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 4800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 4900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 5000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 5100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 5200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 5300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 5400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 5500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 5600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 5700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 5800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 5900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 6000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 6100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 6200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 6300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 6400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 6500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 6600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 6700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 6800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 6900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 7000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 7100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 7200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 7300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 7400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 7500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 7600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 7700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 7800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 7900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 8000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 8100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 8200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 8300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 8400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 8500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 8600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 8700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 8800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 8900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 9000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 9100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 9200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 9300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 9400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 9500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 9600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 9700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 9800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 9900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 10000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 10100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 10200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 10300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 10400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 10500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 10600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 10700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 10800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 10900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 11000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 11100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 11200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 11300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 11400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 11500 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 11600 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 11700 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 11800 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 11900 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 12000 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 12100 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 12200 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 12300 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n",
      "STEP 12400 - loss: 5.109541123538008e-10, w0: 2.000006675720215, w1: 9.999958038330078\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2500\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.02\n",
    "\n",
    "MSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\"\n",
    "\n",
    "w0 = tf.Variable(0.0)\n",
    "w1 = tf.Variable(0.0)\n",
    "\n",
    "dataset = create_dataset(X, Y, EPOCHS, BATCH_SIZE)\n",
    "for x in dataset:\n",
    "    print(x)\n",
    "    break\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(dataset):\n",
    "\n",
    "    dw0, dw1 = compute_gradients(X, Y, w0, w1)\n",
    "    w0.assign_sub(dw0 * LEARNING_RATE)\n",
    "    w1.assign_sub(dw1 * LEARNING_RATE)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "        print(MSG.format(step=step, loss=loss, w0=w0.numpy(), w1=w1.numpy()))\n",
    "\n",
    "assert loss < 0.0001\n",
    "assert abs(w0 - 2) < 0.001\n",
    "assert abs(w1 - 10) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating the CSV files\n",
    "\n",
    "We will start with the **taxifare dataset** CSV files that we wrote out in a previous lab. \n",
    "\n",
    "The taxifare datast files been saved into `../data`.\n",
    "\n",
    "Check that it is the case in the cell below, and, if not, regenerate the taxifare\n",
    "dataset by running the provious lab notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jupyter jupyter 123590 May 12 07:39 ../data/taxi-test.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 579055 May 12 07:39 ../data/taxi-train.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 399562 May 12 07:39 ../data/taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../data/taxi*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use high-level tf.data API to read the CSV files\n",
    "\n",
    "The `tf.data` API can easily read csv files using the helper function\n",
    "\n",
    "[tf.data.experimental.make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset)\n",
    "\n",
    "This function includes many convenient features, so the data is easy to work with. This includes:\n",
    "\n",
    "- This function returns OrderedDict of multiple tensors.\n",
    "- Using the column headers as dictionary keys.\n",
    "- Automatically determining the type of each column.\n",
    "\n",
    "If you have TFRecords (which is recommended), you may use\n",
    "\n",
    "[tf.data.experimental.make_batched_features_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_batched_features_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to define \n",
    "\n",
    "- the feature names into a list `CSV_COLUMNS`\n",
    "- their default values into a list `DEFAULTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\n",
    "    \"fare_amount\",\n",
    "    \"pickup_datetime\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\",\n",
    "    \"key\",\n",
    "]\n",
    "LABEL_COLUMN = \"fare_amount\"\n",
    "DEFAULTS = [[0.0], [\"na\"], [0.0], [0.0], [0.0], [0.0], [0.0], [\"na\"]]\n",
    "BATCH_SIZE=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap the call to `make_csv_dataset` into its own function that will take only the file pattern (i.e. glob) where the dataset files are to be located:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #3:** Complete the code in the `create_dataset(...)` function below to return a `tf.data` dataset made from the `make_csv_dataset`. Have a look at the [documentation here](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset). The `pattern` will be given as an argument of the function but you should set the `batch_size`, `column_names` and `column_defaults`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=OrderedDict([('fare_amount', TensorSpec(shape=(1,), dtype=tf.float32, name=None)), ('pickup_datetime', TensorSpec(shape=(1,), dtype=tf.string, name=None)), ('pickup_longitude', TensorSpec(shape=(1,), dtype=tf.float32, name=None)), ('pickup_latitude', TensorSpec(shape=(1,), dtype=tf.float32, name=None)), ('dropoff_longitude', TensorSpec(shape=(1,), dtype=tf.float32, name=None)), ('dropoff_latitude', TensorSpec(shape=(1,), dtype=tf.float32, name=None)), ('passenger_count', TensorSpec(shape=(1,), dtype=tf.float32, name=None)), ('key', TensorSpec(shape=(1,), dtype=tf.string, name=None))])>\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(\n",
    "    pattern,\n",
    ") -> tf.data.Dataset:\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tempds = create_dataset(\"../data/taxi-train*\")\n",
    "print(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a prefetched dataset, where each element is an `OrderedDict` whose keys are the feature names and whose values are tensors of shape `(1,)` (i.e. vectors).\n",
    "\n",
    "Let's iterate over the two first element of this dataset using `dataset.take(2)` and let's convert them ordinary Python dictionary with numpy array as values for more readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropoff_latitude': array([40.75573], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.97731], dtype=float32),\n",
      " 'fare_amount': array([6.9], dtype=float32),\n",
      " 'key': array([b'1792'], dtype=object),\n",
      " 'passenger_count': array([1.], dtype=float32),\n",
      " 'pickup_datetime': array([b'2011-10-05 08:14:00 UTC'], dtype=object),\n",
      " 'pickup_latitude': array([40.749386], dtype=float32),\n",
      " 'pickup_longitude': array([-73.99015], dtype=float32)}\n",
      "--------------------\n",
      "\n",
      "{'dropoff_latitude': array([40.75579], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.98281], dtype=float32),\n",
      " 'fare_amount': array([6.], dtype=float32),\n",
      " 'key': array([b'1375'], dtype=object),\n",
      " 'passenger_count': array([2.], dtype=float32),\n",
      " 'pickup_datetime': array([b'2013-03-19 16:47:38 UTC'], dtype=object),\n",
      " 'pickup_latitude': array([40.75094], dtype=float32),\n",
      " 'pickup_longitude': array([-73.99427], dtype=float32)}\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in data.items()})\n",
    "    print(f\"{'-'*20}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really need is a dictionary of features + a label. So, we have to do two things to the above dictionary:\n",
    "\n",
    "1. Remove the unwanted column \"key\"\n",
    "1. Keep the label separate from the features\n",
    "\n",
    "Let's first implement a funciton that takes as input a row (represented as an `OrderedDict` in our `tf.data.Dataset` as above) and then returns a tuple with two elements:\n",
    "\n",
    "* The first element beeing the same `OrderedDict` with the label dropped\n",
    "* The second element beeing the label itself (`fare_amount`)\n",
    "\n",
    "Note that we will need to also remove the `key` and `pickup_datetime` column, which we won't use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #4a:** Complete the code in the `features_and_labels(...)` function below. Your function should return a dictionary of features and a label. Keep in mind `row_data` is already a dictionary and you will need to remove the `pickup_datetime` and `key` from `row_data` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "UNWANTED_COLS = [\"pickup_datetime\", \"key\"]\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "\n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over 2 examples from our `tempds` dataset and apply our `feature_and_labels`\n",
    "function to each of the examples to make sure it's working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fare_amount'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL_COLUMN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('pickup_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.997475], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.68362], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.99831], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.71357], dtype=float32)>),\n",
      "             ('passenger_count',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)])\n",
      "tf.Tensor([17.7], shape=(1,), dtype=float32) \n",
      "\n",
      "OrderedDict([('pickup_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.97894], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.76277], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.99378], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.73352], dtype=float32)>),\n",
      "             ('passenger_count',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)])\n",
      "tf.Tensor([16.], shape=(1,), dtype=float32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row_data in tempds.take(2):\n",
    "    features, label = features_and_labels(row_data)\n",
    "    pprint(features)\n",
    "    print(label, \"\\n\")\n",
    "    assert UNWANTED_COLS[0] not in features.keys()\n",
    "    assert UNWANTED_COLS[1] not in features.keys()\n",
    "    assert label.shape == [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now refactor our `create_dataset` function so that it takes an additional argument `batch_size` and batch the data correspondingly. We will also use the `features_and_labels` function we implemented in order for our dataset to produce tuples of features and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #4b:** Complete the code in the `create_dataset(...)` function below to return a `tf.data` dataset made from the `make_csv_dataset`. Now, the `pattern` and `batch_size` will be given as an arguments of the function but you should set the `column_names` and `column_defaults` as before. You will also apply a `.map(...)` method to create features and labels from each example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS\n",
    "    )\n",
    "    dataset = # TODO: Your code here.\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that our batches are of the right size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "tempds = create_dataset(\"../data/taxi-train*\", batch_size=2)\n",
    "\n",
    "for X_batch, Y_batch in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in X_batch.items()})\n",
    "    print(Y_batch.numpy(), \"\\n\")\n",
    "    assert len(Y_batch) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling\n",
    "\n",
    "When training a deep learning model in batches over multiple workers, it is helpful if we shuffle the data. That way, different workers will be working on different parts of the input file at the same time, and so averaging gradients across workers will help. Also, during training, we will need to read the data indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refactor our `create_dataset` function so that we can control its behaviour and shuffle data only when the dataset is used for training.\n",
    "\n",
    "By default, the `make_csv_dataset` function applies shuffling, so please don't forget to specify `shuffle=Flase` to disable default shuffling.\n",
    "\n",
    "Instead, we will introduce a additional argument `mode` to our function to allow the function body to distinguish the case when it needs to shuffle the data (`mode == \"train\"`) from when it shouldn't (`mode == \"eval\"`).\n",
    "\n",
    "Also, before returning we will want to prefetch 1 data point ahead of time (`dataset.prefetch(1)`) to speedup training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #4c:** The last step of our `tf.data` dataset will specify shuffling and repeating of our dataset pipeline. Complete the code below to add these three steps to the Dataset pipeline\n",
    "1. follow the `.map(...)` operation which extracts features and labels with a call to `.cache()` the result.\n",
    "2. during training, use `.shuffle(...)` and `.repeat()` to shuffle batches and repeat the dataset\n",
    "3. use `.prefetch(...)` to take advantage of multi-threading and speedup training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size=1, mode=\"eval\"):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS, shuffle=False\n",
    "    )\n",
    "\n",
    "    dataset = # TODO: Your code here.\n",
    "\n",
    "    if mode == 'train':\n",
    "        dataset = # TODO: Your code here.\n",
    "\n",
    "    # take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = # TODO: Your code here.\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our function work well in both modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempds = create_dataset(\"../data/taxi-train*\", 2, \"train\")\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempds = create_dataset(\"../data/taxi-valid*\", 2, \"eval\")\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use low-level tf.data API to read the CSV files\n",
    "\n",
    "`make_csv_dataset` is a handy high-level function that offers a lot of functionalities, but sometimes it doesn't fit some use cases.\n",
    "\n",
    "To get a more flexible pipeline, we can utilize low-level tf.data APIs to fully control the behavior of the pipeline.\n",
    "\n",
    "For text-based data including CSV, we can use [`TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset?version=nightly) for low-level API.\n",
    "\n",
    "Let's try to create a function to output features in a single Tensor, which is suitable for Sequential API that we will learn in the next section, instead of an ordered dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #5:** Complete the code in `parse_csv` function and `create_dataset` function below to return single Tensors.\n",
    "1. use [`tf.strings.split()`](https://www.tensorflow.org/api_docs/python/tf/strings/split) to split each column.  In this case, separater is `,`. \n",
    "2. in `create_dataset` function call `parse_csv` function in `.map()` and apply `batch()` with the `batch_size` argument to create batched results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(row):\n",
    "    ds = None  # TODO: Your code here.\n",
    "    label = tf.strings.to_number(ds[0])\n",
    "    features = tf.strings.to_number(ds[2:6])  # use some features only\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size):\n",
    "    ds = tf.data.TextLineDataset(pattern)\n",
    "    ds = ds.map()  # TODO: Your code here.\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempds = create_dataset(\"../data/taxi-train.csv\", 2)\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we will build the model using this input pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucMoYase6URl"
   },
   "source": [
    "# Loading Images Using tf.Data.Dataset\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Retrieve Images using tf.keras.utils.get_file\n",
    "2. Load Images using Keras Pre-Processing\n",
    "3. Load Images using tf.Data.Dataset\n",
    "4. Understand basic Methods for Training\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In this notebook, we load an image dataset using tf.data.  The dataset used in this example is distributed as directories of images, with one class of image per directory.\n",
    "\n",
    "\n",
    "Each learning objective will correspond to a **#TODO** in the [student lab notebook](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/solutions/load_images_tf.data.ipynb) -- try to complete that notebook first before reviewing this solution notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hoQQiZDB6URn"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vhAMaIOBIee"
   },
   "source": [
    "## Load necessary libraries \n",
    "We will start by importing the necessary libraries for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIksPgtT8B6B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "print(\"TensorFlow version: \", tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KT6CcaqgQewg"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wO0InzL66URu"
   },
   "source": [
    "### Retrieve the images\n",
    "\n",
    "Before you start any training, you will need a set of images to teach the network about the new classes you want to recognize. You can use an archive of creative-commons licensed flower photos from Google.\n",
    "\n",
    "Note: all images are licensed CC-BY, creators are listed in the `LICENSE.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rN-Pc6Zd6awg"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "data_dir = tf.keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\",\n",
    "    fname=\"flower_photos\",\n",
    "    untar=True,\n",
    ")\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFkFK74oO--g"
   },
   "source": [
    "After downloading (218MB), you should now have a copy of the flower photos available.\n",
    "\n",
    "The directory contains 5 sub-directories, one per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhewYCxhXQBX"
   },
   "outputs": [],
   "source": [
    "image_count = len(list(data_dir.glob(\"*/*.jpg\")))\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJ1HKKdR4A7c"
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = np.array(\n",
    "    [item.name for item in data_dir.glob(\"*\") if item.name != \"LICENSE.txt\"]\n",
    ")\n",
    "CLASS_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVxsk4OW61TY"
   },
   "source": [
    "Each directory contains images of that type of flower. Here are some roses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crs7ZjEp60Ot"
   },
   "outputs": [],
   "source": [
    "roses = list(data_dir.glob(\"roses/*\"))\n",
    "\n",
    "for image_path in roses[:3]:\n",
    "    display.display(Image.open(str(image_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jobDTUs8Wxu"
   },
   "source": [
    "## Load using `keras.preprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehhW308g8soJ"
   },
   "source": [
    "A simple way to load images is to use `tf.keras.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "syDdF_LWVrWE"
   },
   "outputs": [],
   "source": [
    "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0 / 255\n",
    ")  # TODO 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAmtzsnjDNhB"
   },
   "source": [
    "Define some parameters for the loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zf695or-Flq"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "STEPS_PER_EPOCH = np.ceil(image_count / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw94ajOOVrWI"
   },
   "outputs": [],
   "source": [
    "train_data_gen = image_generator.flow_from_directory(\n",
    "    directory=str(data_dir),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    classes=list(CLASS_NAMES),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZgIZeXaDUsF"
   },
   "source": [
    "Inspect a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLp0XVG_Vgi2"
   },
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5, 5, n + 1)  # TODO 1b\n",
    "        plt.imshow(image_batch[n])  # TODO 1b\n",
    "        plt.title(CLASS_NAMES[label_batch[n] == 1][0].title())\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suh6Sjv68rY3"
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(train_data_gen)\n",
    "show_batch(image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxS1cLzM8mEp"
   },
   "source": [
    "## Load using `tf.data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ylj9fgkamgWZ"
   },
   "source": [
    "The above `keras.preprocessing` method is convienient, but has three downsides: \n",
    "\n",
    "1. It's slow. See the performance section below.\n",
    "1. It lacks fine-grained control.\n",
    "1. It is not well integrated with the rest of TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIG5CPaULegg"
   },
   "source": [
    "To load the files as a `tf.data.Dataset` first create a dataset of the file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAkQp5uxoINu"
   },
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(data_dir / \"*/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coORvEH-NGwc"
   },
   "outputs": [],
   "source": [
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91CPfUUJ_8SZ"
   },
   "source": [
    "Write a short pure-tensorflow function that converts a file path to an `(img, label)` pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arSQzIey-4D4"
   },
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)  # TODO 2a\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == CLASS_NAMES  # TODO 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGlq4IP4Aktb"
   },
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)  # TODO 2b\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # TODO 2b\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xhBRgvNqRRe"
   },
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)  # TODO 2c\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9a5GpsUOBx8"
   },
   "source": [
    "Use `Dataset.map` to create a dataset of `image, label` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SDhbo8lOBQv"
   },
   "outputs": [],
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxrl0lGdnpRz"
   },
   "outputs": [],
   "source": [
    "for image, label in labeled_ds.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYGCgJuR_9Qp"
   },
   "source": [
    "### Next Steps:  Basic methods for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwZavzgsIytz"
   },
   "source": [
    "To train a model with this dataset you will want the data:\n",
    "\n",
    "* To be well shuffled.\n",
    "* To be batched.\n",
    "* Batches to be available as soon as possible.\n",
    "\n",
    "These features can be easily added using the `tf.data` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZmZJx8ePw_5"
   },
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)  # TODO 3a\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YKnrfAeZV10"
   },
   "outputs": [],
   "source": [
    "train_ds = prepare_for_training(labeled_ds)\n",
    "\n",
    "image_batch, label_batch = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UN_Dnl72YNIj"
   },
   "outputs": [],
   "source": [
    "show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "images.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

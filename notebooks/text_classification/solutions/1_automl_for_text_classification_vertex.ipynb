{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML for Text Classification with Vertex AI\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Learn how to create a text classification dataset for AutoML using BigQuery\n",
    "1. Learn how to train AutoML to build a text classification model\n",
    "1. Learn how to evaluate a model trained with AutoML\n",
    "1. Learn how to predict on new test data with AutoML\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will use [AutoML for Text Classification](https://cloud.google.com/natural-language/automl/docs/beginners-guide) to train a text model to recognize the source of article titles:  New York Times, TechCrunch or GitHub. \n",
    "\n",
    "In a first step, we will query a public dataset on BigQuery taken from [hacker news](https://news.ycombinator.com/) ( it is an aggregator that displays tech related headlines from various  sources) to create our training set.\n",
    "\n",
    "In a second step, use the AutoML UI to upload our dataset, train a text model on it, and evaluate the model we have just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the variable values in the cell below. Note, AutoML can only be run in the [regions where it is available](https://cloud.google.com/vertex-ai/docs/general/locations#feature-availability). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"asl-ml-immersion\"  # Replace with your PROJECT\n",
    "BUCKET = PROJECT  # defaults to PROJECT\n",
    "REGION = \"us-central1\"  # Replace with your REGION\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket gs://qwiklabs-gcp-00-3b39512a845c already exists.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "exists=$(gsutil ls -d | grep -w gs://$BUCKET/)\n",
    "\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo -e \"Bucket gs://$BUCKET already exists.\"\n",
    "    \n",
    "else\n",
    "   echo \"Creating a new GCS bucket.\"\n",
    "   gsutil mb -l $REGION gs://$BUCKET\n",
    "   echo -e \"\\nHere are your current buckets:\"\n",
    "   gsutil ls\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset from BigQuery \n",
    "\n",
    "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015. \n",
    "\n",
    "Here is a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.01s: 100%|██████████| 2/2 [00:00<00:00, 1323.54query/s]                        \n",
      "Downloading: 100%|██████████| 10/10 [00:01<00:00,  7.37rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.kickstarter.com/projects/carlosxcl...</td>\n",
       "      <td>Show HN: Code Cards, Like Texas hold 'em for p...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://vancouver.en.craigslist.ca/van/roo/2035...</td>\n",
       "      <td>Best Roommate Ad Ever</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/Groundworkstech/Submicron</td>\n",
       "      <td>Deep-Submicron Backdoors</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://empowerunited.com/</td>\n",
       "      <td>Could this be the solution for the 99%?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://themanufacturingrevolution.com/braun-vs...</td>\n",
       "      <td>Braun vs. Apple: Is copying designs theft or i...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://github.com/styleguide/</td>\n",
       "      <td>GitHub Styleguide - CSS, HTML, JS, Ruby</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://zoomzum.com/10-best-firefox-add-ons-to-...</td>\n",
       "      <td>Essential Firefox Add-Ons to Make You More Pro...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://www.zintin.com</td>\n",
       "      <td>Feedback on our social media iPhone app</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://founderdating.com/comingtodinner/</td>\n",
       "      <td>Guess Who’s Coming to Dinner…To Save Our Company</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://tech.matchfwd.com/poor-mans-template-ab...</td>\n",
       "      <td>Poor Man's Template A/B Testing (in Django)</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.kickstarter.com/projects/carlosxcl...   \n",
       "1  http://vancouver.en.craigslist.ca/van/roo/2035...   \n",
       "2       https://github.com/Groundworkstech/Submicron   \n",
       "3                          http://empowerunited.com/   \n",
       "4  http://themanufacturingrevolution.com/braun-vs...   \n",
       "5                     https://github.com/styleguide/   \n",
       "6  http://zoomzum.com/10-best-firefox-add-ons-to-...   \n",
       "7                              http://www.zintin.com   \n",
       "8           http://founderdating.com/comingtodinner/   \n",
       "9  http://tech.matchfwd.com/poor-mans-template-ab...   \n",
       "\n",
       "                                               title  score  \n",
       "0  Show HN: Code Cards, Like Texas hold 'em for p...     11  \n",
       "1                              Best Roommate Ad Ever     11  \n",
       "2                           Deep-Submicron Backdoors     11  \n",
       "3            Could this be the solution for the 99%?     11  \n",
       "4  Braun vs. Apple: Is copying designs theft or i...     11  \n",
       "5            GitHub Styleguide - CSS, HTML, JS, Ruby     11  \n",
       "6  Essential Firefox Add-Ons to Make You More Pro...     11  \n",
       "7            Feedback on our social media iPhone app     11  \n",
       "8   Guess Who’s Coming to Dinner…To Save Our Company     11  \n",
       "9        Poor Man's Template A/B Testing (in Django)     11  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "\n",
    "SELECT\n",
    "    url, title, score\n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    LENGTH(title) > 10\n",
    "    AND score > 10\n",
    "    AND LENGTH(url) > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the url is http://mobile.nytimes.com/...., I want to be left with <i>nytimes</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 3/3 [00:00<00:00, 1764.04query/s]                        \n",
      "Downloading: 100%|██████████| 100/100 [00:01<00:00, 71.79rows/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>num_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blogspot</td>\n",
       "      <td>41386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>github</td>\n",
       "      <td>36525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>30891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youtube</td>\n",
       "      <td>30848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>28787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>f5</td>\n",
       "      <td>1254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>gamasutra</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>indiatimes</td>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>computerworlduk</td>\n",
       "      <td>1166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             source  num_articles\n",
       "0          blogspot         41386\n",
       "1            github         36525\n",
       "2        techcrunch         30891\n",
       "3           youtube         30848\n",
       "4           nytimes         28787\n",
       "..              ...           ...\n",
       "95               f5          1254\n",
       "96        gamasutra          1249\n",
       "97             cnbc          1229\n",
       "98       indiatimes          1223\n",
       "99  computerworlduk          1166\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "\n",
    "SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "    COUNT(title) AS num_articles\n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "    source\n",
    "ORDER BY num_articles DESC\n",
    "  LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT \n",
      "    LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title,\n",
      "    source\n",
      "FROM\n",
      "  (\n",
      "SELECT\n",
      "    title,\n",
      "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source\n",
      "    \n",
      "FROM\n",
      "    `bigquery-public-data.hacker_news.stories`\n",
      "WHERE\n",
      "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
      "    AND LENGTH(title) > 10\n",
      ")\n",
      "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex = '.*://(.[^/]+)/'\n",
    "\n",
    "\n",
    "sub_query = \"\"\"\n",
    "SELECT\n",
    "    title,\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '{0}'), '.'))[OFFSET(1)] AS source\n",
    "    \n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '{0}'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "\"\"\".format(regex)\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title,\n",
    "    source\n",
    "FROM\n",
    "  ({sub_query})\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "\"\"\".format(sub_query=sub_query)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML training, we usually need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset). AutoML however figures out on its own how to create these splits, so we won't need to do that here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feminist-software-foundation complains about r...</td>\n",
       "      <td>github</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>expose sps as web services on the fly.</td>\n",
       "      <td>github</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>show hn  scrwl   shorthand code reading and wr...</td>\n",
       "      <td>github</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>geoip module on nodejs now is a c   addon</td>\n",
       "      <td>github</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>show hn  linuxexplorer</td>\n",
       "      <td>github</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  source\n",
       "0  feminist-software-foundation complains about r...  github\n",
       "1             expose sps as web services on the fly.  github\n",
       "2  show hn  scrwl   shorthand code reading and wr...  github\n",
       "3          geoip module on nodejs now is a c   addon  github\n",
       "4                             show hn  linuxexplorer  github"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq = bigquery.Client(project=PROJECT)\n",
    "title_dataset = bq.query(query).to_dataframe()\n",
    "title_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML for text classification requires that\n",
    "* the dataset be in csv form with \n",
    "* the first column being the texts to classify or a GCS path to the text \n",
    "* the last colum to be the text labels\n",
    "\n",
    "The dataset we pulled from BiqQuery satisfies these requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full dataset contains 96203 titles\n"
     ]
    }
   ],
   "source": [
    "print(\"The full dataset contains {n} titles\".format(n=len(title_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we have roughly the same number of labels for each of our three labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        36525\n",
       "techcrunch    30891\n",
       "nytimes       28787\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_dataset.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will save our data, which is currently in-memory, to disk.\n",
    "\n",
    "We will create a csv file containing the full dataset and another containing only 1000 articles for development.\n",
    "\n",
    "**Note:** It may take a long time to train AutoML on the full dataset, so we recommend to use the sample dataset for the purpose of learning the tool. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = './data/'\n",
    "\n",
    "if not os.path.exists(DATADIR):\n",
    "    os.makedirs(DATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATASET_NAME = 'titles_full.csv'\n",
    "FULL_DATASET_PATH = os.path.join(DATADIR, FULL_DATASET_NAME)\n",
    "\n",
    "# Let's shuffle the data before writing it to disk.\n",
    "title_dataset = title_dataset.sample(n=len(title_dataset))\n",
    "\n",
    "title_dataset.to_csv(\n",
    "    FULL_DATASET_PATH, header=False, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sample 1000 articles from the full dataset and make sure we have enough examples for each label in our sample dataset (see [here](https://cloud.google.com/natural-language/automl/docs/beginners-guide) for further details on how to prepare data for AutoML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        351\n",
       "nytimes       334\n",
       "techcrunch    315\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_title_dataset = title_dataset.sample(n=1000)\n",
    "sample_title_dataset.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the sample datatset to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DATASET_NAME = 'titles_sample.csv'\n",
    "SAMPLE_DATASET_PATH = os.path.join(DATADIR, SAMPLE_DATASET_NAME)\n",
    "\n",
    "sample_title_dataset.to_csv(\n",
    "    SAMPLE_DATASET_PATH, header=False, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>baron is a bitcoin payment processor that anyo...</td>\n",
       "      <td>github</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10144</th>\n",
       "      <td>default profile pictures generated on the fly</td>\n",
       "      <td>github</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7114</th>\n",
       "      <td>pelusa - static analysis lint-type tool for ruby</td>\n",
       "      <td>github</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14841</th>\n",
       "      <td>hackerpit.com solutions</td>\n",
       "      <td>github</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56251</th>\n",
       "      <td>nyt obit of joybubbles  he hacked the phone sy...</td>\n",
       "      <td>nytimes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title   source\n",
       "8623   baron is a bitcoin payment processor that anyo...   github\n",
       "10144      default profile pictures generated on the fly   github\n",
       "7114    pelusa - static analysis lint-type tool for ruby   github\n",
       "14841                            hackerpit.com solutions   github\n",
       "56251  nyt obit of joybubbles  he hacked the phone sy...  nytimes"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_title_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://data/titles_sample.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 57.2 KiB/ 57.2 KiB]                                                \n",
      "Operation completed over 1 objects/57.2 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp data/titles_sample.csv gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model with AutoML for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the dataset in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Vertex menu click \"Datasets\" then click the \"+Create\" at the top of the window. \n",
    "\n",
    "Create a dataset called `hacker_news_titles` and specify it as a Text dataset for text classification (Single-label). Click the `Create` button at the bottom. Note that here you should choose the region that agrees with the region you specified above; e.g. we use 'us-central1'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AutoML](./assets/vertex/create_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, select the file `titles_sample.csv` from your GCS bucket. Importing the data can take about 10 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AutoML](./assets/vertex/select_gcs_file.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train an AutoML text model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is imported you can browse specific examples, or analyze label distributions. Once you are happy with what you see, proceed to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AutoML](./assets/vertex/train_model_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give your model an indicative name like `hacker_news_titles_automl` and start training. Training may take a few hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AutoML](./assets/vertex/train_model_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, navigate to the `Models` tab in Vertex AI and see your model `hacker_news_titles_automl`. Click on the model and you can \"Evaluate\" how the model performed. You'll be able to see the averall precision and recall, as well as drill down to preformances at the individual label level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AutoML](./assets/vertex/evaluate_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML will also show you a confusion matrix and you can see examples where the model made a mistake for each of the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AutoML](./assets/vertex/evaluate_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Predict with the trained AutoML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test your model directly by entering new text in the UI and having AutoML predicts the source of your snippet. First deploy your model to an endpoint. Click on `Deploy to Endpoint` and you'll be directed to a page to create the endpoint. Give the endpoint an indicative name, like `hacker_news_model_endpoint`. Keep all other options as default, and press `DEPLOY`. It make take a few minutes to create the endpoint and deploy the model to the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AutoML](./assets/vertex/deploy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the deployment has completed, you can test your model in the UI and make online predictions. Just type text into the box and click `Predict`. You'll see your model's predictions and the corresponding softmax values for each label:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AutoML](./assets/vertex/online_predict.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set up a batch prediction job. First we'll need to set up our files for prediction with AutoML text classification. To do this, we'll use a JSONL file to specify a list of documents to make predictions about and then store the JSONL file in a Cloud Storage bucket. A single line in an input JSONL file should have the format:\n",
    "\n",
    "`{\"content\": \"gs://sourcebucket/datasets/texts/source_text.txt\", \"mimeType\": \"text/plain\"}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create the GCS .txt files and create the jsonl file below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(BUCKET)\n",
    "\n",
    "SAMPLE_BATCH_INPUTS = './batch_predict_inputs.jsonl'\n",
    "\n",
    "for idx, text in sample_title_dataset.title.items():\n",
    "    # write the text sample to GCS\n",
    "    blob = bucket.blob(f'hacker_news_sample/sample_{idx}.txt')\n",
    "    blob.upload_from_string(\n",
    "            data=text,\n",
    "            content_type='text/plain'\n",
    "            )  \n",
    "    \n",
    "    # add the GCS file to local jsonl\n",
    "    with open(SAMPLE_BATCH_INPUTS, \"a\") as f:\n",
    "        f.write(f'{{\"content\\\": \\\"gs://{BUCKET}/hacker_news_sample/sample_{idx}.txt\\\", \\\"mimeType\\\": \\\"text/plain\\\"}}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the jsonl file was written correctly and that the bucket contains the sample .txt files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\": \"gs://qwiklabs-gcp-00-3b39512a845c/hacker_news_sample/sample_8623.txt\", \"mimeType\": \"text/plain\"}\n",
      "{\"content\": \"gs://qwiklabs-gcp-00-3b39512a845c/hacker_news_sample/sample_10144.txt\", \"mimeType\": \"text/plain\"}\n",
      "{\"content\": \"gs://qwiklabs-gcp-00-3b39512a845c/hacker_news_sample/sample_7114.txt\", \"mimeType\": \"text/plain\"}\n",
      "{\"content\": \"gs://qwiklabs-gcp-00-3b39512a845c/hacker_news_sample/sample_14841.txt\", \"mimeType\": \"text/plain\"}\n",
      "{\"content\": \"gs://qwiklabs-gcp-00-3b39512a845c/hacker_news_sample/sample_56251.txt\", \"mimeType\": \"text/plain\"}\n"
     ]
    }
   ],
   "source": [
    "!head -5 ./batch_predict_inputs.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-00-3b39512a845c/hacker_news_sample/sample_1.txt\n",
      "gs://qwiklabs-gcp-00-3b39512a845c/hacker_news_sample/sample_10030.txt\n",
      "gs://qwiklabs-gcp-00-3b39512a845c/hacker_news_sample/sample_10038.txt\n",
      "gs://qwiklabs-gcp-00-3b39512a845c/hacker_news_sample/sample_10133.txt\n",
      "gs://qwiklabs-gcp-00-3b39512a845c/hacker_news_sample/sample_10144.txt\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='UTF-8'>\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://$BUCKET/hacker_news_sample | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll copy the json file to our GCS bucket and kick off the batch prediction job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./batch_predict_inputs.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][108.3 KiB/108.3 KiB]                                                \n",
      "Operation completed over 1 objects/108.3 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp ./batch_predict_inputs.jsonl gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the `Create Batch Prediction` button and enter the fields for 'Batch prediction name', 'Cloud Storage source path' and 'Cloud storage location to store outputs'. We'll call our job `hacker_news_batch_job`. We'll call prediction on the jsonl file we just created and uploaded. We'll write our prediction outputs to our GCS bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create our batch prediction job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AutoML](./assets/vertex/batch_predict_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is complete we can inspect the results in our GCS bucket:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AutoML](./assets/vertex/batch_predict_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

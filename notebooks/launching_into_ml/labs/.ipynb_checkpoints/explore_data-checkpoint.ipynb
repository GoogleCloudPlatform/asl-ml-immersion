{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and create ML datasets\n",
    "\n",
    "**Learning Objectives**\n",
    "* Access and explore a public BigQuery dataset on NYC Taxi Cab rides\n",
    "* Visualize your dataset using the Seaborn library\n",
    "* Inspect and clean-up the dataset for future ML model training\n",
    "* Create a benchmark to judge future ML model performance off of\n",
    "\n",
    "## Overview\n",
    "In this notebook, we will explore data corresponding to taxi rides in New York City to build a Machine Learning model in support of a fare-estimation tool. The idea is to suggest a likely fare to taxi riders so that they are not surprised, and so that they can protest if the charge is much higher than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Extract sample data from BigQuery </h3>\n",
    "\n",
    "The dataset that we will use is <a href=\"https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips\">a BigQuery public dataset</a>. Click on the link, and look at the column names. Switch to the Details tab to verify that the number of records is one billion, and then switch to the Preview tab to look at a few rows.\n",
    "\n",
    "Let's write a SQL query to pick up interesting fields from the dataset. It's a good idea to get the timestamp in a predictable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "    FORMAT_TIMESTAMP(\n",
    "        \"%Y-%m-%d %H:%M:%S %Z\", pickup_datetime) AS pickup_datetime,\n",
    "    pickup_longitude, pickup_latitude, dropoff_longitude,\n",
    "    dropoff_latitude, passenger_count, trip_distance, tolls_amount, \n",
    "    fare_amount, total_amount \n",
    "# TODO 1: Set correct BigQuery public dataset for nyc-tlc yellow taxi cab trips\n",
    "# Tip: For projects with hyphens '-' be sure to escape with backticks ``\n",
    "FROM \n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the number of records so that we can do some neat graphs.  There is no guarantee about the order in which records are returned, and so no guarantee about which records get returned if we simply increase the LIMIT. To properly sample the dataset, let's use the HASH of the pickup time and return 1 in 100,000 records -- because there are 1 billion records in the data, we should get back approximately 10,000 records if we do this.\n",
    "\n",
    "We will also store the BigQuery result in a Pandas dataframe named \"trips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery trips\n",
    "SELECT\n",
    "    FORMAT_TIMESTAMP(\n",
    "        \"%Y-%m-%d %H:%M:%S %Z\", pickup_datetime) AS pickup_datetime,\n",
    "    pickup_longitude, pickup_latitude, \n",
    "    dropoff_longitude, dropoff_latitude,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    tolls_amount,\n",
    "    fare_amount,\n",
    "    total_amount\n",
    "FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "WHERE\n",
    "    ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 100000)) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can slice Pandas dataframes as if they were arrays\n",
    "trips[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exploring data </h3>\n",
    "\n",
    "Let's explore this dataset and clean it up as necessary. We'll use the Python Seaborn package to visualize graphs and Pandas to do the slicing and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Visualize your dataset using the Seaborn library.\n",
    "# Plot the distance of the trip as X and the fare amount as Y.\n",
    "ax = sns.regplot(x=\"\", y=\"\", fit_reg=False, ci=None, truncate=True, data=trips)\n",
    "ax.figure.set_size_inches(10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm ... do you see something wrong with the data that needs addressing?\n",
    "\n",
    "It appears that we have a lot of invalid data that is being coded as zero distance and some fare amounts that are definitely illegitimate. Let's remove them from our analysis. We can do this by modifying the BigQuery query to keep only trips longer than zero miles and fare amounts that are at least the minimum cab fare ($2.50).\n",
    "\n",
    "Note the extra WHERE clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery trips\n",
    "SELECT\n",
    "    FORMAT_TIMESTAMP(\n",
    "        \"%Y-%m-%d %H:%M:%S %Z\", pickup_datetime) AS pickup_datetime,\n",
    "    pickup_longitude, pickup_latitude, \n",
    "    dropoff_longitude, dropoff_latitude,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    tolls_amount,\n",
    "    fare_amount,\n",
    "    total_amount\n",
    "FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "WHERE\n",
    "    ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 100000)) = 1\n",
    "    # TODO 3: Filter the data to only include non-zero distance trips and fares above $2.50\n",
    "    AND "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(\n",
    "    x=\"trip_distance\", y=\"fare_amount\",\n",
    "    fit_reg=False, ci=None, truncate=True, data=trips)\n",
    "ax.figure.set_size_inches(10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's up with the streaks around 45 dollars and 50 dollars?  Those are fixed-amount rides from JFK and La Guardia airports into anywhere in Manhattan, i.e. to be expected. Let's list the data to make sure the values look reasonable.\n",
    "\n",
    "Let's also examine whether the toll amount is captured in the total amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tollrides = trips[trips[\"tolls_amount\"] > 0]\n",
    "tollrides[tollrides[\"pickup_datetime\"] == \"2012-02-27 09:19:10 UTC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notollrides = trips[trips[\"tolls_amount\"] == 0]\n",
    "notollrides[notollrides[\"pickup_datetime\"] == \"2012-02-27 09:19:10 UTC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a few samples above, it should be clear that the total amount reflects fare amount, toll and tip somewhat arbitrarily -- this is because when customers pay cash, the tip is not known.  So, we'll use the sum of fare_amount + tolls_amount as what needs to be predicted.  Tips are discretionary and do not have to be included in our fare estimation tool.\n",
    "\n",
    "Let's also look at the distribution of values within the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm ... The min, max of longitude look strange.\n",
    "\n",
    "Finally, let's actually look at the start and end of a few of the trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showrides(df, numlines):\n",
    "    lats = []\n",
    "    lons = []\n",
    "    for iter, row in df[:numlines].iterrows():\n",
    "        lons.append(row[\"pickup_longitude\"])\n",
    "        lons.append(row[\"dropoff_longitude\"])\n",
    "        lons.append(None)\n",
    "        lats.append(row[\"pickup_latitude\"])\n",
    "        lats.append(row[\"dropoff_latitude\"])\n",
    "        lats.append(None)\n",
    "\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(lons, lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showrides(notollrides, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showrides(tollrides, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you'd expect, rides that involve a toll are longer than the typical ride."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Quality control and other preprocessing </h3>\n",
    "\n",
    "We need to do some clean-up of the data:\n",
    "<ol>\n",
    "<li>New York city longitudes are around -74 and latitudes are around 41.</li>\n",
    "<li>We shouldn't have zero passengers.</li>\n",
    "<li>Clean up the total_amount column to reflect only fare_amount and tolls_amount, and then remove those two columns.</li>\n",
    "<li>Before the ride starts, we'll know the pickup and dropoff locations, but not the trip distance (that depends on the route taken), so remove it from the ML dataset</li>\n",
    "<li>Discard the timestamp</li>\n",
    "</ol>\n",
    "\n",
    "We could do preprocessing in BigQuery, similar to how we removed the zero-distance rides, but just to show you another option, let's do this in Python.  In production, we'll have to carry out the same preprocessing on the real-time input data. \n",
    "\n",
    "This sort of preprocessing of input data is quite common in ML, especially if the quality-control is dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(trips_in):\n",
    "    trips = trips_in.copy(deep=True)\n",
    "    trips.fare_amount = trips.fare_amount + trips.tolls_amount\n",
    "    del trips[\"tolls_amount\"]\n",
    "    del trips[\"total_amount\"]\n",
    "    del trips[\"trip_distance\"] # we won't know this in advance!\n",
    "\n",
    "    qc = np.all([\n",
    "        trips[\"pickup_longitude\"] > -78,\n",
    "        trips[\"pickup_longitude\"] < -70,\n",
    "        trips[\"dropoff_longitude\"] > -78,\n",
    "        trips[\"dropoff_longitude\"] < -70,\n",
    "        trips[\"pickup_latitude\"] > 37,\n",
    "        trips[\"pickup_latitude\"] < 45,\n",
    "        trips[\"dropoff_latitude\"] > 37,\n",
    "        trips[\"dropoff_latitude\"] < 45,\n",
    "        trips[\"passenger_count\"] > 0\n",
    "    ], axis=0)\n",
    "\n",
    "    return trips[qc]\n",
    "\n",
    "tripsqc = preprocess(trips)\n",
    "tripsqc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality control has removed about 300 rows (11400 - 11101) or about 3% of the data. This seems reasonable.\n",
    "\n",
    "Let's move on to creating the ML datasets.\n",
    "\n",
    "<h3> Create ML datasets </h3>\n",
    "\n",
    "Let's split the QCed data randomly into training, validation and test sets.\n",
    "Note that this is not the entire data. We have 1 billion taxicab rides. This is just splitting the 10,000 rides to show you how it's done on smaller datasets. In reality, we'll have to do it on all 1 billion rides and this won't scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = tripsqc.sample(frac=1)\n",
    "trainsize = int(len(shuffled[\"fare_amount\"]) * 0.70)\n",
    "validsize = int(len(shuffled[\"fare_amount\"]) * 0.15)\n",
    "\n",
    "df_train = shuffled.iloc[:trainsize, :]\n",
    "df_valid = shuffled.iloc[trainsize:(trainsize + validsize), :]\n",
    "df_test = shuffled.iloc[(trainsize + validsize):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out the three dataframes to appropriately named csv files. We can use these csv files for local training (recall that these files represent only 1/100,000 of the full dataset) just to verify our code works, before we run it on all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(df, filename):\n",
    "    outdf = df.copy(deep=False)\n",
    "    outdf.loc[:, \"key\"] = np.arange(0, len(outdf))  # rownumber as key\n",
    "    # Reorder columns so that target is first column\n",
    "    cols = outdf.columns.tolist()\n",
    "    cols.remove(\"fare_amount\")\n",
    "    cols.insert(0, \"fare_amount\")\n",
    "    print (cols)  # new order of columns\n",
    "    outdf = outdf[cols]\n",
    "    outdf.to_csv(filename, header=False, index_label=False, index=False)\n",
    "\n",
    "to_csv(df_train, \"taxi-train.csv\")\n",
    "to_csv(df_valid, \"taxi-valid.csv\")\n",
    "to_csv(df_test, \"taxi-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 taxi-valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Verify that datasets exist </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 .csv files corresponding to train, valid, test.  The ratio of file-sizes correspond to our split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head taxi-train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! We now have our ML datasets and are ready to train ML models, validate them and evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Benchmark </h3>\n",
    "\n",
    "Before we start building complex ML models, it is a good idea to come up with a very simple model and use that as a benchmark.\n",
    "\n",
    "My model is going to be to simply divide the mean fare_amount by the mean trip_distance to come up with a rate and use that to predict.  Let's compute the RMSE of such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_between(lat1, lon1, lat2, lon2):\n",
    "    # Baversine formula to compute distance \"as the crow flies\".\n",
    "    lat1_r = np.radians(lat1)\n",
    "    lat2_r = np.radians(lat2)\n",
    "    lon_diff_r = np.radians(lon2 - lon1)\n",
    "    sin_prod = np.sin(lat1_r) * np.sin(lat2_r)\n",
    "    cos_prod = np.cos(lat1_r) * np.cos(lat2_r) * np.cos(lon_diff_r)\n",
    "    minimum = np.minimum(1, sin_prod + cos_prod)\n",
    "    dist = np.degrees(np.arccos(minimum)) * 60 * 1.515 * 1.609344\n",
    "\n",
    "    return dist\n",
    "\n",
    "def estimate_distance(df):\n",
    "    return distance_between(\n",
    "        df[\"pickuplat\"], df[\"pickuplon\"], df[\"dropofflat\"], df[\"dropofflon\"])\n",
    "\n",
    "def compute_rmse(actual, predicted):\n",
    "    return np.sqrt(np.mean((actual - predicted) ** 2))\n",
    "\n",
    "def print_rmse(df, rate, name):\n",
    "    print (\"{1} RMSE = {0}\".format(\n",
    "        compute_rmse(df[\"fare_amount\"], rate * estimate_distance(df)), name))\n",
    "\n",
    "# TODO 4: Create a benchmark to judge future ML model performance off of\n",
    "\n",
    "# Specify the five feature columns\n",
    "FEATURES = [\"\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "# Specify the one target column for prediction\n",
    "TARGET = \"\"\n",
    "\n",
    "columns = list([TARGET])\n",
    "columns.append(\"pickup_datetime\")\n",
    "columns.extend(FEATURES)  # in CSV, target is first column, after the features\n",
    "columns.append(\"key\")\n",
    "df_train = pd.read_csv(\"taxi-train.csv\", header=None, names=columns)\n",
    "df_valid = pd.read_csv(\"taxi-valid.csv\", header=None, names=columns)\n",
    "df_test = pd.read_csv(\"taxi-test.csv\", header=None, names=columns)\n",
    "rate = df_train[\"fare_amount\"].mean() / estimate_distance(df_train).mean()\n",
    "print (\"Rate = ${0}/km\".format(rate))\n",
    "print_rmse(df_train, rate, \"Train\")\n",
    "print_rmse(df_valid, rate, \"Valid\") \n",
    "print_rmse(df_test, rate, \"Test\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Benchmark on same dataset</h2>\n",
    "\n",
    "The RMSE depends on the dataset, and for comparison, we have to evaluate on the same dataset each time. We'll use this query in later labs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_query = \"\"\"\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    \"unused\" AS key\n",
    "FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "WHERE\n",
    "    ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 10000)) = 2\n",
    "    AND trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0\n",
    "\"\"\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "df_valid = client.query(validation_query).to_dataframe()\n",
    "print_rmse(df_valid, 2.59988, \"Final Validation Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple distance-based rule gives us a RMSE of <b>$8.14</b>.  We have to beat this, of course, but you will find that simple rules of thumb like this can be surprisingly difficult to beat.\n",
    "\n",
    "Let's be ambitious, though, and make our goal to build ML models that have a RMSE of less than $6 on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

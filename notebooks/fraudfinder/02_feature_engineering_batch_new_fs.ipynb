{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# FraudFinder - Feature Engineering (batch) (New Feature Store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[FraudFinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the FraudFinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25113176ad6a"
   },
   "source": [
    "### Objective\n",
    "\n",
    "As you engineer features for model training, it's important to consider how the features are computed when making predictions with new data. For online predictions, you may have features that can be pre-computed via _batch feature engineering_. You may also features that need to be computed on-the-fly via _streaming-based feature engineering_. For these Fraudfinder labs, for computing features based on the last n _days_, you will use _batch_ feature engineering in BigQuery; for computing features based on the last n _minutes_, you will use _streaming-based_ feature engineering using Dataflow.\n",
    "\n",
    "This notebook shows how to generate new features on bank transactions by customer and terminal over the last n days, by doing batch feature engineering in SQL with BigQuery. Then, you will create a feature store using Vertex AI Feature Store, and ingest your newly-created features from BigQuery into Vertex AI Feature Store, so that a feature store can become the single source of data for both training and model inference. \n",
    "\n",
    "You will also create some placeholder values for streaming-based feature engineering, which is covered in the next notebook, `03_feature_engineering_streaming.ipynb`.\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "- [BigQuery](https://cloud.google.com/bigquery/)\n",
    "\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "- Build customer and terminal-related features\n",
    "- Create Feature store, entities and features\n",
    "- Ingest feature values in Feature store from BigQuery table\n",
    "- Read features from the feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* BigQuery\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "811f5d41ab7f",
    "tags": []
   },
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d31d1487ac34",
    "tags": []
   },
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pRUOFELefqf1",
    "tags": []
   },
   "source": [
    "# General\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Union\n",
    "\n",
    "# Data Engineering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "\n",
    "# Vertex AI and Vertex AI Feature Store\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform import EntityType, Feature, Featurestore"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Moq4QZKjY4fv"
   },
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G-4MU7kF3t4x",
    "tags": []
   },
   "source": [
    "# Define the date range of transactions for feature engineering (last 10 days up until yesterday)\n",
    "YESTERDAY = datetime.today() - timedelta(days=1)\n",
    "YEAR_MONTH_PREFIX = YESTERDAY.strftime(\"%Y-%m\")\n",
    "DATAPROCESSING_START_DATE = (YESTERDAY - timedelta(days=10)).strftime(\"%Y-%m-%d\")\n",
    "DATAPROCESSING_END_DATE = YESTERDAY.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define BiqQuery dataset and tables to calculate features.\n",
    "RAW_BQ_TRANSACTION_TABLE_URI = f\"{PROJECT_ID}.tx.tx\"\n",
    "\n",
    "INGESTION_BQ_TRANSACTION_TABLE_URI = f\"{PROJECT_ID}.tx.ingestion_tx_records\"\n",
    "\n",
    "RAW_BQ_LABELS_TABLE_URI = f\"{PROJECT_ID}.tx.txlabels\"\n",
    "FEATURES_BQ_TABLE_URI = f\"{PROJECT_ID}.tx.wide_features_table\"\n",
    "\n",
    "# Define Vertex AI Feature store settings.\n",
    "# CUSTOMERS_TABLE_NAME = f\"customers_{DATAPROCESSING_END_DATE.replace('-', '')}\"\n",
    "# CUSTOMERS_BQ_TABLE_URI = f\"{PROJECT_ID}.tx.{CUSTOMERS_TABLE_NAME}\"\n",
    "\n",
    "CUSTOMERS_FE_BQ_VIEW_URI = f\"{PROJECT_ID}.tx.v_customers_features\"\n",
    "\n",
    "TERMINALS_TABLE_NAME = f\"terminals_{DATAPROCESSING_END_DATE.replace('-', '')}\"\n",
    "\n",
    "# TERMINALS_BQ_TABLE_URI = f\"{PROJECT_ID}.tx.{TERMINALS_TABLE_NAME}\"\n",
    "\n",
    "TERMINALS_FE_BQ_VIEW_URI = f\"{PROJECT_ID}.tx.v_terminals_features\"\n",
    "\n",
    "CUSTOMERS_STREAMING_FE_TABLE_URI = f\"{PROJECT_ID}.tx.t_customers_streaming_features\"\n",
    "TERMINALS_STREAMING_FE_TABLE_URI = f\"{PROJECT_ID}.tx.t_terminals_streaming_features\"\n",
    "\n",
    "ONLINE_STORAGE_NODES = 1\n",
    "FEATURE_TIME = \"feature_ts\"\n",
    "CUSTOMER_ENTITY_ID = \"customer\"\n",
    "TERMINAL_ENTITY_ID = \"terminal\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bedfb5ba7f57"
   },
   "source": [
    "### Helpers\n",
    "\n",
    "Define a set of helper functions to run BigQuery query and create features. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6b81dbbaa636",
    "tags": []
   },
   "source": [
    "def run_bq_query(sql: str, show=False) -> Union[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "    Args:\n",
    "        sql: SQL query, as a string, to execute in BigQuery\n",
    "        show: A flag to show query result in a Pandas Dataframe\n",
    "    Returns:\n",
    "        df: DataFrame of results from query,  or error, if any\n",
    "    \"\"\"\n",
    "\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # If dry run succeeds without errors, proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    job_id = client_result.job_id\n",
    "\n",
    "    # Wait for query/job to finish running. then get & return data frame\n",
    "    result = client_result.result()\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "\n",
    "    if show:\n",
    "        df = result.to_arrow().to_pandas()\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating Destination Table for ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "create_ingestion_tx_records_table = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{INGESTION_BQ_TRANSACTION_TABLE_URI}`\n",
    "(\n",
    "  TX_ID STRING OPTIONS(description=\"Unique transaction identifier\"),\n",
    "  TX_TS TIMESTAMP OPTIONS(description=\"Timestamp of the transaction\"),\n",
    "  CUSTOMER_ID STRING OPTIONS(description=\"Unique customer identifier\"),\n",
    "  TERMINAL_ID STRING OPTIONS(description=\"Unique terminal identifier\"),\n",
    "  TX_AMOUNT FLOAT64 OPTIONS(description=\"The monetary value of the transaction\")\n",
    ")\n",
    "PARTITION BY\n",
    "  DATE(TX_TS)\n",
    "CLUSTER BY\n",
    "  CUSTOMER_ID\n",
    "OPTIONS (\n",
    "  description = \"A table to store customer transaction data, partitioned by day and clustered by customer.\"\n",
    ")\"\"\"\n",
    "print(create_ingestion_tx_records_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "run_bq_query(create_ingestion_tx_records_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKaROgJZ3t4y"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NDaDFyj3t4z"
   },
   "source": [
    "### Define customer and terminal-related features for batch feature engineering\n",
    "\n",
    "In this section, you will create features, based on historical customer behaviour and historical terminal activities. This features will be batch-generated using SQL in BigQuery, where the historical data is stored.\n",
    "\n",
    "The query below will calculate 2 sets of features: \n",
    "\n",
    "1. **Customer-related features**: which describes the spending behaviour of customer within 1, 7 and 15 days time windows using number of transactions and average amount spent in dollars ($)\n",
    "\n",
    "2. **Terminal-related features** which describes the risk of a given terminal to be exposed to fraudulent transactions within 1, 7 and 15 days using average number of fraudulent transactions in dollars ($), the number of transactions and risk index. One thing to note is that you will add some delay to take into account time that would pass between the time of transaction and the result of fraud investigation or customer claim.\n",
    "\n",
    "You will use one month of transaction data starting from the end of January and going back to compute the features.\n",
    "\n",
    "Below is the schema you should expect to see, after doing the batch feature engineering in BigQuery:\n",
    "\n",
    "|feature_time           |customer_id| customer batch features   |\n",
    "|-----------------------|-----------|---------------------------|\n",
    "|2022-01-01 17:20:15 UTC|1          |(e.g., nb_tx,  avg_tx)     |\n",
    "|2022-01-02 12:08:40 UTC|2          |(e.g., nb_tx,  avg_tx)     |\n",
    "|2022-01-03 17:30:48 UTC|3          |(e.g., nb_tx,  avg_tx)     |\n",
    "\n",
    "\n",
    "|feature_time           |terminal_id| terminal batch features|\n",
    "|-----------------------|-----------|------------------------|\n",
    "|2022-01-01 17:20:15 UTC|12345      |(e.g., risk_x_days)     |\n",
    "|2022-01-02 12:08:40 UTC|26789      |(e.g., risk_x_days)     |\n",
    "|2022-01-03 17:30:48 UTC|101112     |(e.g., risk_x_days)     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43265bd3002f"
   },
   "source": [
    "#### Create the query to create batch features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date settings to be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer feature table\n",
    "\n",
    "Customer table SQL query string:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "43Ck_vAc3t4z",
    "tags": []
   },
   "source": [
    "create_customer_batch_features_query = f\"\"\"\n",
    "CREATE OR REPLACE VIEW `{CUSTOMERS_FE_BQ_VIEW_URI}` AS\n",
    "WITH\n",
    "  -- query to join labels with features -------------------------------------------------------------------------------------------\n",
    "  get_raw_table AS (\n",
    "  SELECT\n",
    "    raw_tx.TX_TS,\n",
    "    raw_tx.TX_ID,\n",
    "    raw_tx.CUSTOMER_ID,\n",
    "    raw_tx.TERMINAL_ID,\n",
    "    raw_tx.TX_AMOUNT\n",
    "  FROM (\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      `{INGESTION_BQ_TRANSACTION_TABLE_URI}`\n",
    "    WHERE\n",
    "      DATE(TX_TS) BETWEEN DATE_SUB(CURRENT_DATETIME, INTERVAL 15 DAY) AND CURRENT_DATETIME\n",
    "    ) raw_tx),\n",
    "\n",
    "  -- query to calculate CUSTOMER spending behaviour --------------------------------------------------------------------------------\n",
    "  get_customer_spending_behaviour AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    TX_AMOUNT,\n",
    "    \n",
    "    # calc the number of customer tx over daily windows per customer (1, 7 and 15 days, expressed in seconds)\n",
    "    COUNT(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 86400 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_1DAY_WINDOW,\n",
    "    COUNT(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_7DAY_WINDOW,\n",
    "    COUNT(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_14DAY_WINDOW,\n",
    "      \n",
    "    # calc the customer average tx amount over daily windows per customer (1, 7 and 15 days, expressed in seconds, in dollars ($))\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 86400 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW,\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW,\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_14DAY_WINDOW,\n",
    "  FROM get_raw_table)\n",
    "\n",
    "# Create the table with CUSTOMER and TERMINAL features ----------------------------------------------------------------------------\n",
    "SELECT\n",
    "  current_timestamp() as feature_timestamp,\n",
    "  CUSTOMER_ID AS customer_id,\n",
    "  CAST(CUSTOMER_ID_NB_TX_1DAY_WINDOW AS INT64) AS customer_id_nb_tx_1day_window,\n",
    "  CAST(CUSTOMER_ID_NB_TX_7DAY_WINDOW AS INT64) AS customer_id_nb_tx_7day_window,\n",
    "  CAST(CUSTOMER_ID_NB_TX_14DAY_WINDOW AS INT64) AS customer_id_nb_tx_14day_window,\n",
    "  CAST(CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_1day_window,\n",
    "  CAST(CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_7day_window,\n",
    "  CAST(CUSTOMER_ID_AVG_AMOUNT_14DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_14day_window,\n",
    "FROM\n",
    "  get_customer_spending_behaviour\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(create_customer_batch_features_query)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the query \n",
    "\n",
    "You create the customer features table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "run_bq_query(create_customer_batch_features_query)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect the result \n",
    "\n",
    "You can query some data rows to validate the result of the query"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "run_bq_query(f\"SELECT * FROM `{CUSTOMERS_FE_BQ_VIEW_URI}` LIMIT 10\", show=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Terminal feature table\n",
    "\n",
    "Terminal table SQL query string:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "43Ck_vAc3t4z",
    "tags": []
   },
   "source": [
    "create_terminal_batch_features_query = f\"\"\"\n",
    "# query to calculate TERMINAL spending behaviour --------------------------------------------------------------------------------\n",
    "CREATE OR REPLACE VIEW `{TERMINALS_FE_BQ_VIEW_URI}` AS\n",
    "WITH\n",
    "  -- query to join labels with features -------------------------------------------------------------------------------------------\n",
    "  get_raw_table AS (\n",
    "  SELECT\n",
    "    raw_tx.TX_TS,\n",
    "    raw_tx.TX_ID,\n",
    "    raw_tx.CUSTOMER_ID,\n",
    "    raw_tx.TERMINAL_ID,\n",
    "    raw_tx.TX_AMOUNT,\n",
    "    raw_lb.TX_FRAUD\n",
    "  FROM (\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      `{RAW_BQ_TRANSACTION_TABLE_URI}`\n",
    "    WHERE\n",
    "      DATE(TX_TS) BETWEEN DATE_SUB(CURRENT_DATETIME, INTERVAL 15 DAY) AND CURRENT_DATETIME\n",
    "    ) raw_tx\n",
    "  LEFT JOIN \n",
    "    `{RAW_BQ_LABELS_TABLE_URI}` as raw_lb\n",
    "  ON raw_tx.TX_ID = raw_lb.TX_ID),\n",
    "\n",
    "  # query to calculate TERMINAL spending behaviour --------------------------------------------------------------------------------\n",
    "  get_variables_delay_window AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    \n",
    "    # calc total amount of fraudulent tx and the total number of tx over the delay period per terminal (7 days - delay, expressed in seconds)\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_DELAY,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_DELAY,\n",
    "      \n",
    "    # calc total amount of fraudulent tx and the total number of tx over the delayed window per terminal (window + 7 days - delay, expressed in seconds)\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 691200 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_1_DELAY_WINDOW,\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_7_DELAY_WINDOW,\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1814400 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_14_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 691200 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_1_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_7_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1814400 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_14_DELAY_WINDOW,\n",
    "  FROM get_raw_table),\n",
    "\n",
    "  # query to calculate TERMINAL risk factors ---------------------------------------------------------------------------------------\n",
    "  get_risk_factors AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    # calculate numerator of risk index\n",
    "    NB_FRAUD_1_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_1DAY_WINDOW,\n",
    "    NB_FRAUD_7_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_7DAY_WINDOW,\n",
    "    NB_FRAUD_14_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_14DAY_WINDOW,\n",
    "    # calculate denominator of risk index\n",
    "    NB_TX_1_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_1DAY_WINDOW,\n",
    "    NB_TX_7_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_7DAY_WINDOW,\n",
    "    NB_TX_14_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_14DAY_WINDOW,\n",
    "      FROM\n",
    "    get_variables_delay_window),\n",
    "\n",
    "  # query to calculate the TERMINAL risk index -------------------------------------------------------------------------------------\n",
    "  get_risk_index AS (\n",
    "    SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    TERMINAL_ID_NB_TX_1DAY_WINDOW,\n",
    "    TERMINAL_ID_NB_TX_7DAY_WINDOW,\n",
    "    TERMINAL_ID_NB_TX_14DAY_WINDOW,\n",
    "    # calculate the risk index\n",
    "    (TERMINAL_ID_NB_FRAUD_1DAY_WINDOW/(TERMINAL_ID_NB_TX_1DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_1DAY_WINDOW,\n",
    "    (TERMINAL_ID_NB_FRAUD_7DAY_WINDOW/(TERMINAL_ID_NB_TX_7DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_7DAY_WINDOW,\n",
    "    (TERMINAL_ID_NB_FRAUD_14DAY_WINDOW/(TERMINAL_ID_NB_TX_14DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_14DAY_WINDOW\n",
    "    FROM get_risk_factors \n",
    "  )\n",
    "\n",
    "# Create the table with CUSTOMER and TERMINAL features ----------------------------------------------------------------------------\n",
    "SELECT\n",
    "  current_timestamp() as feature_timestamp,\n",
    "  TERMINAL_ID AS terminal_id,\n",
    "  CAST(TERMINAL_ID_NB_TX_1DAY_WINDOW AS INT64) AS terminal_id_nb_tx_1day_window,\n",
    "  CAST(TERMINAL_ID_NB_TX_7DAY_WINDOW AS INT64) AS terminal_id_nb_tx_7day_window,\n",
    "  CAST(TERMINAL_ID_NB_TX_14DAY_WINDOW AS INT64) AS terminal_id_nb_tx_14day_window,\n",
    "  CAST(TERMINAL_ID_RISK_1DAY_WINDOW AS FLOAT64) AS terminal_id_risk_1day_window,\n",
    "  CAST(TERMINAL_ID_RISK_7DAY_WINDOW AS FLOAT64) AS terminal_id_risk_7day_window,\n",
    "  CAST(TERMINAL_ID_RISK_14DAY_WINDOW AS FLOAT64) AS terminal_id_risk_14day_window,\n",
    "FROM\n",
    "  get_risk_index\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(create_terminal_batch_features_query)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the query \n",
    "\n",
    "You create the customer features table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "run_bq_query(create_terminal_batch_features_query)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect the result \n",
    "\n",
    "You can query some data rows to validate the result of the query"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "run_bq_query(f\"SELECT * FROM `{TERMINALS_FE_BQ_VIEW_URI}` LIMIT 10\", show=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "612e2fa65748"
   },
   "source": [
    "#### Define the query to initialize the real-time features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer feature table\n",
    "\n",
    "Customer table SQL query string:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c1aed0001232",
    "tags": []
   },
   "source": [
    "initiate_real_time_customer_features_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{CUSTOMERS_STREAMING_FE_TABLE_URI}`\n",
    "(\n",
    "    customer_id STRING,\n",
    "    feature_timestamp TIMESTAMP,\n",
    "    customer_id_nb_tx_15min_window INT64,\n",
    "    customer_id_nb_tx_30min_window INT64,\n",
    "    customer_id_nb_tx_60min_window INT64,\n",
    "    customer_id_avg_amount_15min_window FLOAT64,\n",
    "    customer_id_avg_amount_30min_window FLOAT64,\n",
    "    customer_id_avg_amount_60min_window FLOAT64\n",
    ")\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "initiate_real_time_terminal_features_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{TERMINALS_STREAMING_FE_TABLE_URI}`\n",
    "(\n",
    "    terminal_id STRING,\n",
    "    feature_timestamp TIMESTAMP,\n",
    "    terminal_id_nb_tx_15min_window INT64,\n",
    "    terminal_id_nb_tx_30min_window INT64,\n",
    "    terminal_id_nb_tx_60min_window INT64,\n",
    "    terminal_id_avg_amount_15min_window FLOAT64,\n",
    "    terminal_id_avg_amount_30min_window FLOAT64,\n",
    "    terminal_id_avg_amount_60min_window FLOAT64\n",
    ")\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d35d1e69d72"
   },
   "source": [
    "#### Run the query above to initialize the real-time features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aae355c66e4a",
    "tags": []
   },
   "source": [
    "for query in [\n",
    "    initiate_real_time_customer_features_query,\n",
    "    initiate_real_time_terminal_features_query,\n",
    "]:\n",
    "    run_bq_query(query)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76733185d022"
   },
   "source": [
    "#### Inspect BigQuery features tables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "774a90225747",
    "tags": []
   },
   "source": [
    "run_bq_query(f\"SELECT * FROM `{CUSTOMERS_STREAMING_FE_TABLE_URI}` LIMIT 5\", show=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "run_bq_query(f\"SELECT * FROM `{TERMINALS_STREAMING_FE_TABLE_URI}` LIMIT 5\", show=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the final schema of the features table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Initialize Vertex AI SDK\n",
    "\n",
    "Initialize the Vertex AI SDK to get access to Vertex AI services programmatically. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c85ab891607c",
    "tags": []
   },
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfKeQmf63t42"
   },
   "source": [
    "## Create featurestore, `fraudfinder_featurestore`\n",
    "\n",
    "### Set up and start online serving\n",
    "\n",
    "Now for the exciting part! To serve data in a feature store, you need to do the following:\n",
    "\n",
    "1. Create an online store cluster to host the data.\n",
    "    * Create a `FeatureOnlineStore` instance with autoscaling.\n",
    "1. Define the data (`FeatureView`) to be served by the newly-created instance. This can either map to\n",
    "    * The BigQuery view that you just created for serving data.\n",
    "    * The `FeatureGroup` and `Feature` you'll create to host feature metadata.\n",
    "\n",
    "Bigtable serving latency is affected by the (Bigtable) load. However, when Bigtable is not overloaded, benchmarks show that the expected server-side latency is around 30 ms, measured at around 100 qps. The client-side latency is expected to be more than 5 ms higher than the server-side latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform_v1 import (\n",
    "    FeatureOnlineStoreAdminServiceClient,\n",
    "    FeatureOnlineStoreServiceClient,\n",
    "    FeatureRegistryServiceClient,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import feature as feature_pb2\n",
    "from google.cloud.aiplatform_v1.types import feature_group as feature_group_pb2\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    feature_online_store as feature_online_store_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    feature_online_store_admin_service as feature_online_store_admin_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    feature_online_store_service as feature_online_store_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    feature_registry_service as feature_registry_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import feature_view as feature_view_pb2\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    featurestore_service as featurestore_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import io as io_pb2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Admin Service Client"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "admin_client = FeatureOnlineStoreAdminServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")\n",
    "registry_client = FeatureRegistryServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create online store instance\n",
    "\n",
    "To create an online store instance.\n",
    "Create a `FeatureOnlineStore` instance with autoscaling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "FEATURE_ONLINE_STORE_ID = \"fraudfinder_featurestore\"\n",
    "\n",
    "online_store_config = feature_online_store_pb2.FeatureOnlineStore(\n",
    "    bigtable=feature_online_store_pb2.FeatureOnlineStore.Bigtable(\n",
    "        auto_scaling=feature_online_store_pb2.FeatureOnlineStore.Bigtable.AutoScaling(\n",
    "            min_node_count=1, max_node_count=1, cpu_utilization_target=50\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "create_store_lro = admin_client.create_feature_online_store(\n",
    "    feature_online_store_admin_service_pb2.CreateFeatureOnlineStoreRequest(\n",
    "        parent=f\"projects/{PROJECT_ID}/locations/{REGION}\",\n",
    "        feature_online_store_id=FEATURE_ONLINE_STORE_ID,\n",
    "        feature_online_store=online_store_config,\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify online store instance creation\n",
    "\n",
    "After the long-running operation (LRO) is complete, show the result.\n",
    "\n",
    "> **Note:** This operation might take up to 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wait for the LRO to finish and get the LRO result.\n",
    "print(create_store_lro.result())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use list to verify the store is created.\n",
    "admin_client.list_feature_online_stores(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature view instance\n",
    "\n",
    "After creating a `FeatureOnlineStore` instance, you can define the features to serve with it. To do this, create a `FeatureView` instance, which specifies the following:\n",
    "\n",
    "* A data source (BigQuery table or view URI or FeatureGroup/features ) synced to the `FeatureOnlineStore` instance for serving.\n",
    "* The cron schedule to run the sync pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create featureView with FeatureGroups/Feature\n",
    "\n",
    "> **Note:** If you've already created a feature view with BQ source, skip this section and go to [Verify online store instance creation](#scrollTo=igOmzHxx1C0X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### [Optional] Create FeatureGroup/Features\n",
    "\n",
    "Create a FeatureGroup pointing to the created BigQuery view for the demo. You then create features for each column you'd like to register.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data source preparation guidelines for Feature Registry data source\n",
    "\n",
    "Note that if you choose to use Feature Registry source, Feature Store only provides the option to support time-series sources for which Feature Store generates latest featureValues.\n",
    "\n",
    "Use the following guidelines to understand the schema and constraints while creating the BigQuery source:\n",
    "\n",
    "* The BigQuery table or view *must* have a column with `string` values to use as the (entity) IDs. You'll need to specify that this column is the ID column during `FeatureGroup` creation. Note that the size of each value in this column must be less than 4 KB.\n",
    "* The BigQuery table or view *must* have a column named `feature_timestamp` with `timestamp` values to use as timestamp column.\n",
    "* Feature Registry sources are treated as sparse by default i.e. a point in time lookup (BQ.ML_FEATURES_AT_TIME()) to generate latest featureValues per entityId.\n",
    "* Provide values for each feature is a separate column. Supported data types are `bool`, `int`, `double`, `string`, timestamp, arrays of these data types, and bytes. Note that the timestamp data type is converted to `int64` during data sync.\n",
    "* Feature Store validates the schema during `FeatureView`/`FeatureGroup`/`Feature` creation. However, it doesn't revalidate the schema during each data sync. Columns with unsupported data types added after `FeatureView` creation time are ignored.\n",
    "* The BigQuery table or view must be in either the same region as the online store, or in a multiregion that overlaps with the online store. For example, if the online store is in `us-central`, the BigQuery source can be in `us-central` or in `US`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define utility method for feature groups creation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_fs_feature_group(\n",
    "    bq_source_uri, entity_id_column, feature_group_id, feature_ids_list\n",
    "):\n",
    "\n",
    "    # Now, create the featureGroup\n",
    "    feature_group_config = feature_group_pb2.FeatureGroup(\n",
    "        big_query=feature_group_pb2.FeatureGroup.BigQuery(\n",
    "            big_query_source=io_pb2.BigQuerySource(input_uri=f\"bq://{bq_source_uri}\"),\n",
    "            # Add the entity_id_columns parameter here\n",
    "            entity_id_columns=[entity_id_column],\n",
    "        )\n",
    "    )\n",
    "    create_group_lro = registry_client.create_feature_group(\n",
    "        feature_registry_service_pb2.CreateFeatureGroupRequest(\n",
    "            parent=f\"projects/{PROJECT_ID}/locations/{REGION}\",\n",
    "            feature_group_id=feature_group_id,\n",
    "            feature_group=feature_group_config,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # After the long-running operation (LRO) is complete, show the result.\n",
    "    print(create_group_lro.result())\n",
    "\n",
    "    create_feature_lros = []\n",
    "    for id in feature_ids_list:\n",
    "        create_feature_lros.append(\n",
    "            registry_client.create_feature(\n",
    "                featurestore_service_pb2.CreateFeatureRequest(\n",
    "                    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureGroups/{feature_group_id}\",\n",
    "                    feature_id=id,\n",
    "                    feature=feature_pb2.Feature(),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Wait for FS Group creation\n",
    "    for lro in create_feature_lros:\n",
    "        # After the long-running operation (LRO) is complete, show the result.\n",
    "        print(lro.result())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "CUSTOMER_ID_COLUMN = \"customer_id\"  # entity_id\n",
    "\n",
    "CUSTOMER_BATCH_FEATURES_GROUP_ID = \"fraudfinder_customers_batch\"\n",
    "\n",
    "CUSTOMER_BATCH_FEATURE_IDS = [\n",
    "    \"customer_id_nb_tx_14day_window\",\n",
    "    \"customer_id_avg_amount_7day_window\",\n",
    "    \"customer_id_nb_tx_1day_window\",\n",
    "    \"customer_id_avg_amount_1day_window\",\n",
    "    \"customer_id_avg_amount_14day_window\",\n",
    "    \"customer_id_nb_tx_7day_window\",\n",
    "]\n",
    "\n",
    "# Creating feature Group for batch for customers\n",
    "create_fs_feature_group(\n",
    "    bq_source_uri=CUSTOMERS_FE_BQ_VIEW_URI,\n",
    "    entity_id_column=CUSTOMER_ID_COLUMN,\n",
    "    feature_group_id=CUSTOMER_BATCH_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=CUSTOMER_BATCH_FEATURE_IDS,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "CUSTOMER_STREAMING_FEATURES_GROUP_ID = \"fraudfinder_customers_streaming\"\n",
    "CUSTOMER_STREAMING_FEATURE_IDS = [\n",
    "    \"customer_id_nb_tx_15min_window\",\n",
    "    \"customer_id_nb_tx_30min_window\",\n",
    "    \"customer_id_nb_tx_60min_window\",\n",
    "    \"customer_id_avg_amount_15min_window\",\n",
    "    \"customer_id_avg_amount_30min_window\",\n",
    "    \"customer_id_avg_amount_60min_window\",\n",
    "]\n",
    "\n",
    "# Creating feature Group for streaming for customers\n",
    "create_fs_feature_group(\n",
    "    bq_source_uri=CUSTOMERS_STREAMING_FE_TABLE_URI,\n",
    "    entity_id_column=CUSTOMER_ID_COLUMN,\n",
    "    feature_group_id=CUSTOMER_STREAMING_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=CUSTOMER_STREAMING_FEATURE_IDS,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Now, create the featureGroup for terminals\n",
    "TERMINAL_ID_COLUMN = \"terminal_id\"\n",
    "\n",
    "TERMINAL_BATCH_FEATURES_GROUP_ID = \"fraudfinder_terminals_batch\"\n",
    "TERMINAL_BATCH_FEATURE_IDS = [\n",
    "    \"terminal_id_nb_tx_1day_window\",\n",
    "    \"terminal_id_nb_tx_7day_window\",\n",
    "    \"terminal_id_nb_tx_14day_window\",\n",
    "    \"terminal_id_risk_1day_window\",\n",
    "    \"terminal_id_risk_7day_window\",\n",
    "    \"terminal_id_risk_14day_window\",\n",
    "]\n",
    "\n",
    "# Creating feature Group for batch for customers\n",
    "create_fs_feature_group(\n",
    "    bq_source_uri=TERMINALS_FE_BQ_VIEW_URI,\n",
    "    entity_id_column=TERMINAL_ID_COLUMN,\n",
    "    feature_group_id=TERMINAL_BATCH_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=TERMINAL_BATCH_FEATURE_IDS,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Now, create the featureGroup for terminals streaming features\n",
    "TERMINAL_STREAMING_FEATURES_GROUP_ID = \"fraudfinder_terminals_streaming\"\n",
    "TERMINAL_STREAMING_FEATURE_IDS = [\n",
    "    \"terminal_id_nb_tx_15min_window\",\n",
    "    \"terminal_id_nb_tx_30min_window\",\n",
    "    \"terminal_id_nb_tx_60min_window\",\n",
    "    \"terminal_id_avg_amount_15min_window\",\n",
    "    \"terminal_id_avg_amount_30min_window\",\n",
    "    \"terminal_id_avg_amount_60min_window\",\n",
    "]\n",
    "\n",
    "# Creating feature Group for batch for customers\n",
    "create_fs_feature_group(\n",
    "    bq_source_uri=TERMINALS_STREAMING_FE_TABLE_URI,\n",
    "    entity_id_column=TERMINAL_ID_COLUMN,\n",
    "    feature_group_id=TERMINAL_STREAMING_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=TERMINAL_STREAMING_FEATURE_IDS,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a `FeatureView` instance for the BigQuery view and FeatureGroup/features you created earlier in this tutorial and set the sync time and frequency to 15 min. (*/15 * * * *)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "def create_online_fs_view(\n",
    "    fs_view_id,\n",
    "    fs_online_store_id,\n",
    "    feature_group_id,\n",
    "    feature_ids_list,\n",
    "    continuous,\n",
    "    cron_schedule=None,\n",
    "):\n",
    "\n",
    "    feature_registry_source = feature_view_pb2.FeatureView.FeatureRegistrySource(\n",
    "        feature_groups=[\n",
    "            feature_view_pb2.FeatureView.FeatureRegistrySource.FeatureGroup(\n",
    "                feature_group_id=feature_group_id,\n",
    "                feature_ids=feature_ids_list,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if continuous:\n",
    "        sync_config = feature_view_pb2.FeatureView.SyncConfig(continuous=True)\n",
    "    else:\n",
    "        sync_config = feature_view_pb2.FeatureView.SyncConfig(cron=cron_schedule)\n",
    "\n",
    "    create_view_lro = admin_client.create_feature_view(\n",
    "        feature_online_store_admin_service_pb2.CreateFeatureViewRequest(\n",
    "            parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{fs_online_store_id}\",\n",
    "            feature_view_id=fs_view_id,\n",
    "            run_sync_immediately=True,\n",
    "            feature_view=feature_view_pb2.FeatureView(\n",
    "                feature_registry_source=feature_registry_source,\n",
    "                sync_config=sync_config,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Wait for LRO to complete and show result\n",
    "    print(create_view_lro.result())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "CUSTOMER_BATCH_FEATURE_VIEW_ID = \"fv_fraudfinder_customers_batch\"\n",
    "\n",
    "CRON_SCHEDULE = \"TZ=America/Los_Angeles */15 * * * *\"  # Each 15min\n",
    "\n",
    "create_online_fs_view(\n",
    "    fs_view_id=CUSTOMER_BATCH_FEATURE_VIEW_ID,\n",
    "    fs_online_store_id=FEATURE_ONLINE_STORE_ID,\n",
    "    feature_group_id=CUSTOMER_BATCH_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=CUSTOMER_BATCH_FEATURE_IDS,\n",
    "    continuous=False,\n",
    "    cron_schedule=CRON_SCHEDULE,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating featurestore view for Customers Features, streaming FE pipeline.\n",
    "CUSTOMER_STREAMING_FEATURE_VIEW_ID = \"fv_fraudfinder_customers_streaming\"\n",
    "\n",
    "create_online_fs_view(\n",
    "    fs_view_id=CUSTOMER_STREAMING_FEATURE_VIEW_ID,\n",
    "    fs_online_store_id=FEATURE_ONLINE_STORE_ID,\n",
    "    feature_group_id=CUSTOMER_STREAMING_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=CUSTOMER_STREAMING_FEATURE_IDS,\n",
    "    continuous=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating featurestore view for Terminals Features, batch FE pipeline.\n",
    "TERMINAL_BATCH_FEATURE_VIEW_ID = \"fv_fraudfinder_terminals_batch\"\n",
    "\n",
    "create_online_fs_view(\n",
    "    fs_view_id=TERMINAL_BATCH_FEATURE_VIEW_ID,\n",
    "    fs_online_store_id=FEATURE_ONLINE_STORE_ID,\n",
    "    feature_group_id=TERMINAL_BATCH_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=TERMINAL_BATCH_FEATURE_IDS,\n",
    "    continuous=False,\n",
    "    cron_schedule=CRON_SCHEDULE,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating featurestore view for Terminals Features, streaming FE pipeline.\n",
    "TERMINAL_STREAMING_FEATURE_VIEW_ID = \"fv_fraudfinder_terminals_streaming\"\n",
    "\n",
    "create_online_fs_view(\n",
    "    fs_view_id=TERMINAL_STREAMING_FEATURE_VIEW_ID,\n",
    "    fs_online_store_id=FEATURE_ONLINE_STORE_ID,\n",
    "    feature_group_id=TERMINAL_STREAMING_FEATURES_GROUP_ID,\n",
    "    feature_ids_list=TERMINAL_STREAMING_FEATURE_IDS,\n",
    "    continuous=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the `FeatureView` instance is created by listing all the feature views within the online store."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Again, list all feature view under the FEATURE_ONLINE_STORE_ID to confirm\n",
    "admin_client.list_feature_views(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start sync manually\n",
    "\n",
    "The sync pipeline executes according to the schedule specified in the `FeatureView` instance.\n",
    "\n",
    "To skip the wait and execute the sync pipeline immediately, start the sync manually."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "sync_response = admin_client.sync_feature_view(\n",
    "    feature_view=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}/featureViews/{TERMINAL_BATCH_FEATURE_VIEW_ID}\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sync_response` contains the ID of the sync job.\n",
    "\n",
    "Use `get_feature_view_sync` to check the status of the job."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    feature_view_sync = admin_client.get_feature_view_sync(\n",
    "        name=sync_response.feature_view_sync\n",
    "    )\n",
    "    if feature_view_sync.run_time.end_time.seconds > 0:\n",
    "        status = \"Succeed\" if feature_view_sync.final_status.code == 0 else \"Failed\"\n",
    "        print(f\"Sync {status} for {feature_view_sync.name}.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Sync ongoing, waiting for 30 seconds.\")\n",
    "    time.sleep(30)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "admin_client.list_feature_view_syncs(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}/featureViews/{TERMINAL_BATCH_FEATURE_VIEW_ID}\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "sync_response = admin_client.sync_feature_view(\n",
    "    feature_view=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}/featureViews/{CUSTOMER_BATCH_FEATURE_VIEW_ID}\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    feature_view_sync = admin_client.get_feature_view_sync(\n",
    "        name=sync_response.feature_view_sync\n",
    "    )\n",
    "    if feature_view_sync.run_time.end_time.seconds > 0:\n",
    "        status = \"Succeed\" if feature_view_sync.final_status.code == 0 else \"Failed\"\n",
    "        print(f\"Sync {status} for {feature_view_sync.name}.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Sync ongoing, waiting for 30 seconds.\")\n",
    "    time.sleep(30)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "admin_client.list_feature_view_syncs(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}/featureViews/{CUSTOMER_BATCH_FEATURE_VIEW_ID}\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start online serving\n",
    "\n",
    "After the data sync is complete, use the `FetchFeatureValues` API to retrieve the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "data_client = FeatureOnlineStoreServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `FeatureView` already defines the features needed for the model (via the BigQuery view in this demo). To fetch the data, submit a `fetch_feature_values` request specifying the `FeatureView` resource path and the ID of the entity."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(FEATURE_ONLINE_STORE_ID)\n",
    "print(CUSTOMER_BATCH_FEATURE_VIEW_ID)\n",
    "\n",
    "customer_key = \"0001071169708317\"  # Put known id here\n",
    "\n",
    "FEATURE_VIEW_FULL_ID = f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}/featureViews/{CUSTOMER_BATCH_FEATURE_VIEW_ID}\"\n",
    "\n",
    "try:\n",
    "    fe_data = data_client.fetch_feature_values(\n",
    "        request=feature_online_store_service_pb2.FetchFeatureValuesRequest(\n",
    "            feature_view=FEATURE_VIEW_FULL_ID,\n",
    "            data_key=feature_online_store_service_pb2.FeatureViewDataKey(\n",
    "                key=customer_key\n",
    "            ),\n",
    "            data_format=feature_online_store_service_pb2.FeatureViewDataFormat.PROTO_STRUCT,\n",
    "        )\n",
    "    )\n",
    "    features_map = {k: v for k, v in fe_data.proto_struct.items()}\n",
    "    print(\n",
    "        json.dumps(\n",
    "            features_map,\n",
    "        )\n",
    "    )\n",
    "except Exception as exp:\n",
    "    print(\"ERROR: \" + str(exp))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1335c1b0761"
   },
   "source": [
    "### Inspect your feature store in the Vertex AI console\n",
    "\n",
    "You can also inspect your feature store in the [Vertex AI Feature Store console](https://console.cloud.google.com/vertex-ai/feature-store/online-stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END\n",
    "\n",
    "Now you can go to the next notebook `03_feature_engineering_streaming_new_fs.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c5b719f3dcd"
   },
   "source": [
    "## Clean up"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02_feature_engineering_batch.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

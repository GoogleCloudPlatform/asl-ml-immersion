{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78713331-03e6-40b1-a551-173ffbc4b453",
   "metadata": {},
   "source": [
    "# Image Segmentation with U-Net\n",
    "In this lab you will learn how to build and train an image segmentation model with `tf.keras` and (optionally) train at scale with Vertex AI.\n",
    "\n",
    "Learning Objectives:\n",
    "* Learn how to preprocess and augment data for image segmentation\n",
    "* Learn how to build a U-net architecture with `tf.keras`\n",
    "* Learn how to train this model with Vertex AI\n",
    "* Learn how to serve and parse predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30a6bb-f944-44ee-8844-d236371c0cb3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3003ab5-ff97-44c1-9b61-bd27b294810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2355b7c-8953-4ebe-b2b6-09c64386d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "PROJECT = !(gcloud config get-value project)\n",
    "PROJECT = PROJECT[0]\n",
    "REGION = \"us-central1\"\n",
    "BUCKET = PROJECT\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "OUTPUT_DIR = f\"gs://{BUCKET}/pets/models/{TIMESTAMP}\"\n",
    "\n",
    "# silence TF info logs\n",
    "%env TF_CPP_MIN_LOG_LEVEL=3\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d24ea-ecdb-484f-b01d-fd5a6f6c902b",
   "metadata": {},
   "source": [
    "#### Visualize the data\n",
    "The dataset used in this lab in the [Oxford Pet Dataset](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet). It contains images of animals with segmentation masks. The goal of an image segmentation model is to take in as input an image and predict the segmentation mask. Think multi-classification problem, where each pixel is labelled as background, interior, or outline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745edf2-6545-4e6a-890f-41b717ec80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess(data):\n",
    "    input_image = tf.image.resize(data[\"image\"], (128, 128))\n",
    "    input_mask = tf.image.resize(data[\"segmentation_mask\"], (128, 128))\n",
    "\n",
    "    input_image = tf.image.convert_image_dtype(input_image, tf.float32)  # [0,1]\n",
    "    input_mask -= 1  # {1,2,3} to {0,1,2}\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe2a50-e432-4724-a481-d1a4db90cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "# Load in the dataset with tfds\n",
    "dataset, info = tfds.load(\n",
    "    \"oxford_iiit_pet\",\n",
    "    data_dir=\"gs://asl-public/data/tensorflow_datasets\",\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "train = dataset[\"train\"].map(read_and_preprocess, num_parallel_calls=AUTO)\n",
    "test = dataset[\"test\"].map(read_and_preprocess, num_parallel_calls=AUTO)\n",
    "\n",
    "f, ax = plt.subplots(2, 5, figsize=(16, 5))\n",
    "for idx, (img, mask) in enumerate(train.take(5)):\n",
    "    ax[0, idx].imshow(tf.keras.preprocessing.image.array_to_img(img))\n",
    "    ax[0, idx].axis(\"off\")\n",
    "    mask = tf.reshape(mask, [128, 128])\n",
    "    ax[1, idx].imshow(mask.numpy())\n",
    "    ax[1, idx].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60388966-e4e6-45d5-9849-252ad9c3b702",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "Create a function that randomly flips the image and mask left or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbccd8c1-7f87-43fe-ba3b-e38286602c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENT_PROB = 0.5  # probability of data augmentation\n",
    "\n",
    "\n",
    "def augment(img, mask):\n",
    "    if tf.random.uniform(()) < AUGMENT_PROB:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "train = train.map(augment)\n",
    "test = test.map(augment)\n",
    "\n",
    "f, ax = plt.subplots(2, 5, figsize=(16, 5))\n",
    "for idx, (img, mask) in enumerate(train.take(5)):\n",
    "    ax[0, idx].imshow(tf.keras.preprocessing.image.array_to_img(img))\n",
    "    ax[0, idx].axis(\"off\")\n",
    "    mask = tf.reshape(mask, [128, 128])\n",
    "    ax[1, idx].imshow(mask.numpy())\n",
    "    ax[1, idx].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41318ddf-3610-4257-abce-0ac3257d23fe",
   "metadata": {},
   "source": [
    "### Modified U-Net\n",
    "\n",
    "The model being used here is a modified [U-Net](https://arxiv.org/pdf/1505.04597v1.pdf). A U-Net typically consists of an encoder which downsamples an image to an encoding, and a mirrored decoder which upsamples the encoding back to the desired mask. The decoder blocks have a number of skip connections that directly connect the encoder blocks to the decoder.\n",
    "\n",
    "In this notebook, we will use a pretrained MobileNetV2 to create the encoding and a set of upsampling layers (stride=2) to get back to the desired mask. When doing so, however, we will pull out layers with the desired sizes so that the upsampling skip layers use corresponding weights from the pretrained models. The layer names such as `block_1_expand_relu`, `block_3_expand_relu`, etc, correspond to these output layers of the pretrained MobileNetV2. If you would like to see the entire structure of the pretrained MobileNetV2, feel free to load it into memory and analyze it with `tf.keras.utils.plot_model` or `model.summary()`\n",
    "\n",
    "Note that the output will consist of 3 channels. This is because we have 3 possible labels for each pixel (background, outline, interior). Think of this as multi-classification problem with three possible classes.\n",
    "\n",
    "![image](https://storage.googleapis.com/asl-public/public-images/unet.png)\n",
    "Architecture diagram from original research paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfcc91-4f84-4b24-9cb6-46506b2efefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify shape and channels of input images\n",
    "INPUT_SHAPE = [128, 128, 3]\n",
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6c952-10af-434a-bfe3-68e9f8c9e6e9",
   "metadata": {},
   "source": [
    "#### Create the Encoder (Downstack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387c7d2-6b5b-4823-94b6-56706e8ee184",
   "metadata": {},
   "source": [
    "First, create an encoder which downsamples the input images. To keep things simple, use a pretrained model (MobileNetV2). Set `include_top=False` since you don't want to include the fully-connected layer at the top of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a65a7-5b53-4d98-9f25-8e81ee0f188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model into memory\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=INPUT_SHAPE, include_top=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f611d3-55d2-4bb5-a693-b09a6b0ca8dc",
   "metadata": {},
   "source": [
    "Recall that Unet uses outputs of the encoder to form skip connections to the decoder. Access these layers by name to create the output of the encoder, then instantiate it as a `tf.keras.Model`.\n",
    "\n",
    "To make this architecture train faster you can freeze the weights of the encoder so they are not learned at training time. You can do this by setting the `trainable` attibute to `False` on the `tf.keras.Model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b48bdb-d525-47e3-99b7-fc896f949caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the activations of these layers to form skip connections\n",
    "layer_names = [\n",
    "    \"block_1_expand_relu\",  # 64x64\n",
    "    \"block_3_expand_relu\",  # 32x32\n",
    "    \"block_6_expand_relu\",  # 16x16\n",
    "    \"block_13_expand_relu\",  # 8x8\n",
    "    \"block_16_project\",  # 4x4\n",
    "]\n",
    "\n",
    "# Outputs\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=base_model_outputs,\n",
    "    name=\"pretrained_mobilenet\",\n",
    ")\n",
    "\n",
    "down_stack.trainable = False\n",
    "\n",
    "print(f\"Encoder Inputs:\\n{down_stack.inputs}\")\n",
    "print(\"Encoder Outputs:\")\n",
    "for i in range(len(down_stack.outputs)):\n",
    "    print(down_stack.outputs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bb30f-65a0-496c-91e6-23675e315c69",
   "metadata": {},
   "source": [
    "As you can see the encoder is expecting an image of height/width `[128, 128]` as input and is downsampling to `[64, 64]`, `[32, 32]`, `[16, 16]`, `[8, 8]`, and `[4, 4]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27fbd3-a402-488e-adb8-f77715a44603",
   "metadata": {},
   "source": [
    "#### Create the Decoder (Upstack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6900f-e411-4588-9dd2-e4e77d9a7c95",
   "metadata": {},
   "source": [
    "First, create a function that returns upsampling layers as simple `tf.keras.Sequential` models. Remember that the upsampling layers should double the image sizes. To do this we can use a [transposed convolutional layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) with `strides=2` and `padding=\"same\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8a561-5fe0-4264-a71f-9d4dff069b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, kernel_size, name, strides=2):\n",
    "    # TODO: Write the upsample function to perform upsamples, batch normalization, and activation\n",
    "    # This function should return a tf.keras.Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b0b41-efc8-4abd-9b42-557e2a8149f7",
   "metadata": {},
   "source": [
    "Use the upsample function to create the upsampling layers that bring the dimensionality back up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf91284-71a0-4952-a2d4-5ba87c0eb0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_stack = [\n",
    "    upsample(512, 3, \"upsample_4x4_to_8x8\"),\n",
    "    upsample(256, 3, \"upsample_8x8_to_16x16\"),\n",
    "    upsample(128, 3, \"upsample_16x16_to_32x32\"),\n",
    "    upsample(64, 3, \"upsample_32x32_to_64x64\"),\n",
    "]\n",
    "\n",
    "up_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5430ad2-b73c-40de-bc4b-edcb6a620872",
   "metadata": {},
   "source": [
    "Send inputs through the encoder, create and establish the skip connections while upsampling through the decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5583a6-cd86-48d6-8976-19c87f055996",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=INPUT_SHAPE, name=\"input_image\")\n",
    "\n",
    "# Downsampling through the model\n",
    "skips = down_stack(inputs)\n",
    "x = skips[-1]\n",
    "skips = reversed(skips[:-1])\n",
    "\n",
    "for idx, (up, skip) in enumerate(zip(up_stack, skips)):\n",
    "# TODO: Implement the upsampling and establish skip connections "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2ed7e-7d30-453c-8f7b-e435d8387183",
   "metadata": {},
   "source": [
    "The upstack as we have it now outputs with height/width `[64, 64]`. Create the last layer manually, which will be a `Conv2DTranspose` to upsample back to the original input shape. Then, instatiate the end-to-end architecture as a `tf.keras.Model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1116b2c-70f5-4ec1-a5ca-551d333e2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the last layer of the model\n",
    "last = tf.keras.layers.Conv2DTranspose(\n",
    "    OUTPUT_CHANNELS, 3, strides=2, padding=\"same\"\n",
    ")  # 64x64 -> 128x128\n",
    "\n",
    "x = last(x)\n",
    "\n",
    "# TODO: Initialize and compile the model\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b35a7d-3e6d-4158-8c7e-f9270e53c3e3",
   "metadata": {},
   "source": [
    "As you can see, the input is an image with shape `[128, 128, 3]`. This image gets downsampled using the pretrained MobileNetV2, then upsampled back to it's original size with `Conv2DTranspose`. Also, note the skip connections, critical to the Unet architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6eca3b-3bc4-483f-9ee8-33a1a82cf2f2",
   "metadata": {},
   "source": [
    "#### Train the model locally\n",
    "**Note**: Without a GPU, training for 10 epochs can take ~ 45 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d77ae-91c3-4dc2-be29-b8ed528402e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "STEPS_PER_EPOCH = info.splits[\"train\"].num_examples // BATCH_SIZE\n",
    "VALIDATION_STEPS = info.splits[\"test\"].num_examples // BATCH_SIZE\n",
    "MODEL_DIR = \"./trained_unet\"\n",
    "train = train.batch(BATCH_SIZE).repeat()\n",
    "test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31ed5e-3e3f-4671-b429-05bc1a4c399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=test,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ff752-3e6c-4a4d-8fd6-85770dc61959",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[[\"val_accuracy\", \"accuracy\"]].plot(\n",
    "    title=\"Accuracy\"\n",
    ")\n",
    "pd.DataFrame(history.history)[[\"val_loss\", \"loss\"]].plot(title=\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd6b25-a9ce-43a5-8e4a-6e74a914b087",
   "metadata": {},
   "source": [
    "#### Visualize predictions\n",
    "These are helper functions to visualize the predictions of the trained model. \n",
    "\n",
    "`create_mask`: Returns the mask assigning each pixel to the 'class' with the highest probabilty (background, outline, interior). \n",
    "\n",
    "`display`: Plots an input image, the true mask (label) of the image, and the predicted mask from our model.\n",
    "\n",
    "`show_prediction`: Calls `display` for a specified number of input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08317fbc-ffc9-43ea-9665-b8440e3f10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    \"\"\"Given prediction mask, return highest probability per pixel\"\"\"\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]\n",
    "\n",
    "\n",
    "def display(display_list):\n",
    "    \"\"\"Plot input image, true mask, and predicted mask\"\"\"\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_predictions(dataset, num):\n",
    "    \"\"\"Display a certain number of predictions from a tf.data.Dataset\"\"\"\n",
    "    for image, mask in dataset.take(num):\n",
    "        pred_mask = model.predict(image)\n",
    "        display([image[0], mask[0], create_mask(pred_mask)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63379f5a-fa40-4b03-9f7b-aa32e573d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e147d9-77ed-4a23-93a4-f5c20beb72a9",
   "metadata": {},
   "source": [
    "#### (Optional) Train on Vertex AI\n",
    "In this section you containerize the training application and submit a custom training job to Vertex AI. The steps to do this are:\n",
    "\n",
    "* Create Python script of the training application\n",
    "* Create Dockerfile to bundle training script and deps\n",
    "* Build and push Docker image\n",
    "* Create and run Vertex AI Custom Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c226705-66ea-4aab-a0bf-1d41afeedba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./pets_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d85de-faca-475c-a012-0b609cb4383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./pets_trainer/train.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import fire\n",
    "\n",
    "def read_and_preprocess(data):\n",
    "    input_image = tf.image.resize(data['image'], (128,128))\n",
    "    input_mask = tf.image.resize(data['segmentation_mask'], (128,128))\n",
    "    \n",
    "    input_image = tf.image.convert_image_dtype(input_image, tf.float32) # [0,1]\n",
    "    input_mask -= 1 # {1,2,3} to {0,1,2}\n",
    "    return input_image, input_mask\n",
    "    \n",
    "def augment(img, mask):\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "    return img, mask\n",
    "    \n",
    "def create_train_test_datasets(dataset, batch_size):\n",
    "    train_ds = dataset['train'].map(read_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = dataset['test'].map(read_and_preprocess)\n",
    "    train_ds = train_ds.cache().map(augment).shuffle(10 * batch_size).batch(batch_size).repeat()\n",
    "    train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_ds = test_ds.batch(batch_size)\n",
    "    return train_ds, test_ds\n",
    "    \n",
    "def upsample(filters, size, name):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU()], name=name)\n",
    "\n",
    "def build_unet(input_shape, output_channels):\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False)\n",
    "    \n",
    "    # Use the activations of these layers to form skip connections\n",
    "    layer_names = [\n",
    "        'block_1_expand_relu',   # 64x64\n",
    "        'block_3_expand_relu',   # 32x32\n",
    "        'block_6_expand_relu',   # 16x16\n",
    "        'block_13_expand_relu',  # 8x8\n",
    "        'block_16_project',      # 4x4\n",
    "    ]\n",
    "    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "    \n",
    "    # Create the feature extraction model\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs,\n",
    "                                name='pretrained_mobilenet')\n",
    "\n",
    "    down_stack.trainable = False\n",
    "    \n",
    "    up_stack = [\n",
    "        upsample(512, 3, 'upsample_4x4_to_8x8'),\n",
    "        upsample(256, 3, 'upsample_8x8_to_16x16'),\n",
    "        upsample(128, 3, 'upsample_16x16_to_32x32'),\n",
    "        upsample(64, 3,  'upsample_32x32_to_64x64')\n",
    "    ]\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, name='input_image')\n",
    "    \n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(inputs)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "    \n",
    "    # Upsampling and establishing the skip connections\n",
    "    for idx, (up, skip) in enumerate(zip(up_stack, skips)):\n",
    "        x = up(x)\n",
    "        concat = tf.keras.layers.Concatenate(name='expand_{}'.format(idx))\n",
    "        x = concat([x, skip])\n",
    "        \n",
    "    # This is the last layer of the model\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "        output_channels, 3, strides=2,\n",
    "        padding='same')  #64x64 -> 128x128\n",
    "    \n",
    "    x = last(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    \n",
    "def train_evaluate(\n",
    "    batch_size=64,\n",
    "    tf_dataset='oxford_iiit_pet',\n",
    "    data_dir='gs://asl-public/data/tensorflow_datasets',\n",
    "    input_shape=[128,128,3],\n",
    "    output_channels=3,\n",
    "    epochs=10,\n",
    "    output_dir=None\n",
    "):\n",
    "    \n",
    "    # Download dataset from tfds\n",
    "    dataset, info = tfds.load(tf_dataset, data_dir=data_dir, with_info=True)\n",
    "    \n",
    "    # Create train and test datasets\n",
    "    train_ds, test_ds = create_train_test_datasets(dataset, batch_size)\n",
    "    \n",
    "    steps_per_epoch = info.splits['train'].num_examples // batch_size\n",
    "    validation_steps = info.splits['test'].num_examples // batch_size\n",
    "    \n",
    "    model = build_unet(input_shape, output_channels)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        validation_data=test_ds,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps)\n",
    "    \n",
    "    if output_dir:\n",
    "        model.save(output_dir)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af602d9-9e61-4004-8b40-ceffee2e7ff1",
   "metadata": {},
   "source": [
    "#### Write Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff976b4b-8763-42ea-830c-1e990a3d5b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./pets_trainer/Dockerfile\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-12.py310:latest\n",
    "\n",
    "# Installs fire\n",
    "RUN pip install -U fire tensorflow_datasets\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558085f1-9299-43ce-beed-aa647e23a738",
   "metadata": {},
   "source": [
    "#### Build and push containerized training application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed8786-983a-4d1c-867d-08ac3ada2cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"pets_trainer_tf\"\n",
    "TAG = \"latest\"\n",
    "TRAINING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT}/{IMAGE_NAME}:{TAG}\"\n",
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae31c41-4d1e-43d9-ada0-373c9dc4fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --machine-type=e2-highcpu-32 --timeout=15m --tag $TRAINING_CONTAINER_IMAGE_URI pets_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af521f55-ee30-4f74-8c3c-f1a58d6093ce",
   "metadata": {},
   "source": [
    "#### Run Custom Training Job on Vertex AI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42523ce1-d5e2-4a26-9230-c9dc484a9c9f",
   "metadata": {},
   "source": [
    "Check to make sure bucket exists and if not create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db153c-dbaa-4464-9826-4785639ab09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGING_BUCKET = f\"gs://{BUCKET}-staging\"\n",
    "\n",
    "%env STAGING_BUCKET={STAGING_BUCKET}\n",
    "%env REGION={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32657482-4205-4048-ae98-aaf068b64cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "exists=$(gsutil ls -d | grep -w ${STAGING_BUCKET}/)\n",
    "\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo -e \"Bucket gs://${BUCKET} already exists.\"\n",
    "    \n",
    "else\n",
    "   echo \"Creating a new GCS bucket.\"\n",
    "   gsutil mb -l ${REGION} ${STAGING_BUCKET}\n",
    "   echo -e \"\\nHere are your current buckets:\"\n",
    "   gsutil ls\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47ab5f-6d00-4955-8318-dac7bf0ebd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify variables used to configure hyperparameter tuning job\n",
    "DISPLAY_NAME = \"pets-segmentation\"\n",
    "JOB_NAME = f\"unet-custom-training-{TIMESTAMP}\"\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629ab7e-5bee-4968-9dc6-57351e440459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Define required specifications\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-8\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAINING_CONTAINER_IMAGE_URI,\n",
    "            \"args\": [f\"--output_dir={OUTPUT_DIR}\", f\"--epochs={EPOCHS}\"],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "custom_job = aiplatform.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    project=PROJECT,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d5aace-fd9f-4393-9049-a7d242298022",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494bd46-175c-4e5a-8803-7bddb9c00870",
   "metadata": {},
   "source": [
    "Congrats! You've succesfully trained a custom U-net model for image segmentation and operationalized model training with Vertex AI."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

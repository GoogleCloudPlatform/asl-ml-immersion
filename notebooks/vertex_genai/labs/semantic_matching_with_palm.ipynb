{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46uGq6ZSnt09"
   },
   "source": [
    "# Semantic Matching with the PaLM API\n",
    "\n",
    "**Learning Objectives**\n",
    "  1. Learn how to identify matching product descriptions\n",
    "  1. Learn how to design a prompt for semantic matching\n",
    "  2. Learn how to evaluate performance of a prompt for semantic matching\n",
    "  1. Learn how to query the Vertex PaLM API\n",
    "  \n",
    "Semantic matching is the problem of classifying a pair of entities $(x_1, x_2)$ as being a good match or not. So it is a classification setup that is a very flexible: Namely, it comprises general information retrieval (where the first entity can be a textual query and the second entity can be a paragraph for instance), entity resolutions, or database-record fuzzy-matching. In this notebook we will focus on matching textual descriptions of retail products. More specifically:\n",
    "  \n",
    "  \n",
    "**Use case description:** An online retail company scours the web to compare prices of products in their inventory with those offered by their competitors. Their first priority is to implement a model that compares the information on two product webpages and outputs a classification indicating whether the different product descriptions on the webpages correspond to an identical product, which we will refer to as a 'match'. We use the [Amazon-Google Products dataset](https://dbs.uni-leipzig.de/file/Amazon-GoogleProducts.zip) in the [entity resolution benchmark](https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution) created by Leipzig University. \n",
    "\n",
    "**Model description:** This notebook illustrates how to use PaLM API in Vertex AI to match product descriptions. The [Amazon-GoogleProducts dataset](https://dbs.uni-leipzig.de/file/Amazon-GoogleProducts.zip) contains product information about the products scrapped on Amazon or Google websites. It includes the products title, description, price, and manufacturer, athough this information is worded differently on the two websites. In this notebook, we will focus solely on building a model using the product titles. The idea is straightforward: we will create a prompt that asks the PaLM API whether the product titles match or not. Although the information about the products is limited to these titles, we still achieve an accuracy close to 100% on the test set.\n",
    "\n",
    "**Evaluation method:** In order to avoid overfitting the prompt design on our dataset, we first split the our dataset sample of paired descriptions into an evaluation set (evalset) containing 60 examples and a test set (testset) also consisting of 60 examples. The choice of 60 examples aligns with the current limit quota of 60 requests per minute for the PaLM API in Vertex AI. Subsequently, we devise the prompt using the evalset and report the model's accuracy on the testset. Both the evaluation and test sets are roughly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTFK37CpOROH"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-wMuJbNfKMs"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "vertexai.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [dataset of product information](https://dbs.uni-leipzig.de/file/Amazon-GoogleProducts.zip) scraped from Google and Amazon websites. The dataset is part of a [benchmark for semantic matching and entity resolution](https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution) from Leipzig University. It contains 3 tables which are included in this repo:\n",
    "\n",
    "```python\n",
    "../data/Amazon.csv.gz \n",
    "../data/GoogleProducts.csv.gz\n",
    "../Amzon_GoogleProducts_perfectMapping.csv.gz\n",
    "\n",
    "```\n",
    "\n",
    "The first table contains product information listed on Amazon, including the product title, description, and manufacturer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = pd.read_csv(\"../data/Amazon.csv.gz\")\n",
    "amazon.columns = [\n",
    "    \"idAmazon\",\n",
    "    \"amazon_title\",\n",
    "    \"amazon_description\",\n",
    "    \"amazon_manufacturer\",\n",
    "    \"amazon_price\",\n",
    "]\n",
    "amazon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second table contains the same information but for product information scrapped from Google website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google = pd.read_csv(\"../data/GoogleProducts.csv.gz\")\n",
    "google.columns = [\n",
    "    \"idGoogleBase\",\n",
    "    \"google_title\",\n",
    "    \"google_description\",\n",
    "    \"google_manufacturer\",\n",
    "    \"google_price\",\n",
    "]\n",
    "google.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last table contains a matching of product information on both website corresponding to the same product, but possibly described differently on the two websites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = pd.read_csv(\"../data/Amzon_GoogleProducts_perfectMapping.csv.gz\")\n",
    "matching.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this raw data, we have pre-generated for you an eval and test set:\n",
    "\n",
    "```python\n",
    "../data/product_matching_eval.csv\n",
    "../data/product_matching_test.csv\n",
    "```\n",
    "\n",
    "\n",
    "We will use the eval set to design the prompt and use the test set to evaluate the PaLM API performance on this prompt. This way, the performance we report will be closer to the real performance on never-seen-pairs of product descriptions.  \n",
    "\n",
    "\n",
    "To genrate the eval and test split, we used the function in the cell below. It takes a sample (whose size is controlled by `SAMPLE_SIZE`) of matching product ID's in the `matching` dataframe, and it joins the information of the Google and Amazon product information contained in the `google` and `amazon` dataframes. Then it extracts pairs of matching Google and Amazon descriptions and creates a table of matching descriptions with columns `description_1` (Google), `description_2` (Amazon), and a target column named `match` whose value is set to `yes` since we have only matching pairs so far.\n",
    "To create description pairs of not matching product, we permutate the second description columns while keeping the first description fixed, and concatenate this new dataframe of not matching descriptions to the one of matching description. We shuffle and then split the resulting table into two equal sized dataframes, which we save on disk as our eval and test splits. \n",
    "\n",
    "Observe that we only use the `title` column as product description. So there is much more info in the raw data. Nevertheless, we will see that PaLM will achieve a performance close to 100% on the test set. Remarkable!\n",
    "\n",
    "**Note:** Uncomment the last line if you want to re-generate the eval and test set on a different sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 60\n",
    "\n",
    "\n",
    "def generate_test_and_eval_sets(sample_size=SAMPLE_SIZE):\n",
    "    sample = matching.sample(sample_size)\n",
    "\n",
    "    # Join the product information to the df of matching ID's\n",
    "    matched_products = sample.merge(\n",
    "        right=amazon, how=\"left\", on=\"idAmazon\"\n",
    "    ).merge(right=google, how=\"left\", on=\"idGoogleBase\")\n",
    "\n",
    "    google_descriptions = list(matched_products[\"google_title\"])\n",
    "    amazon_descriptions = list(matched_products[\"amazon_title\"])\n",
    "\n",
    "    # Create the dataframe of matching descriptions\n",
    "    matching_descriptions = pd.DataFrame(\n",
    "        {\n",
    "            \"description_1\": google_descriptions,\n",
    "            \"description_2\": amazon_descriptions,\n",
    "            \"match\": \"yes\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create the dataframe of not matching descriptions\n",
    "    amazon_descriptions_perm = [\n",
    "        amazon_descriptions[i - 1] for i in range(len(amazon_descriptions))\n",
    "    ]\n",
    "    not_matching_descriptions = pd.DataFrame(\n",
    "        {\n",
    "            \"description_1\": google_descriptions,\n",
    "            \"description_2\": amazon_descriptions_perm,\n",
    "            \"match\": \"no\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    full_dataset = pd.concat(\n",
    "        [matching_descriptions, not_matching_descriptions], axis=0\n",
    "    ).sample(len(matching_descriptions) * 2)\n",
    "\n",
    "    evalset = full_dataset[:sample_size]\n",
    "    testset = full_dataset[sample_size : 2 * sample_size]\n",
    "    evalset.to_csv(\"../data/product_matching_eval.csv\", index=None)\n",
    "    testset.to_csv(\"../data/product_matching_test.csv\", index=None)\n",
    "\n",
    "\n",
    "# Uncomment to generate a different data sample\n",
    "# generate_test_and_eval_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Obe_UZjcnxfr"
   },
   "source": [
    "The next cell loads the eval and test datasets that we pre-generated. The two CSV files contain 60 examples of product description pairs. Each pair is labeled with a `match` value of `yes` if the descriptions describe the same product and `no` otherwise. `description_1` comes from product title on Google while `description_2` comes from Amazon products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZbeDFq-epFp"
   },
   "outputs": [],
   "source": [
    "evalset = pd.read_csv(\"../data/product_matching_eval.csv\")\n",
    "testset = pd.read_csv(\"../data/product_matching_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMqMQgBxnx3F"
   },
   "source": [
    "Let's have a quick look at a few entries in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1j5R8sWofR7i",
    "outputId": "000dae29-62a7-4ada-e8c9-83fc2263eec6"
   },
   "outputs": [],
   "source": [
    "evalset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2Mo_6Zqny_4"
   },
   "source": [
    "Both splits are roughly balanced. To make sure that both splits are roughly balanced, we count the number of class instances for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-WDRNVygJDG",
    "outputId": "6119fbfa-3305-45fc-f2a4-d710ffd2666b"
   },
   "outputs": [],
   "source": [
    "evalset.match.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "carLBOfPgMG8",
    "outputId": "333bafef-7794-413e-edae-12b7c751662f"
   },
   "outputs": [],
   "source": [
    "testset.match.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiLqKVXRg-n2"
   },
   "source": [
    "#  Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPOcktSQn1Q0"
   },
   "source": [
    "We start by instanciating our client to the `text-bison@001` version of the PaLM 2 which is a large language model (LLM) developed by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qrIOtzTNc-J"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QgnNIWrn1ki"
   },
   "source": [
    "Using this client, we implement in the next cell a simple function that takes two product descriptions and a parameterized prompt as input, and outputs `yes` or `no` depending on whether the PaLM model thinks the product descriptions are matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the function below so that it queries the PaLM API using the `generation_model` client.\n",
    "\n",
    "**Hint:** Jump to the cell after next to see how this function is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "on-_-NgyOA1h"
   },
   "outputs": [],
   "source": [
    "def are_products_matching(d1, d2, prompt):\n",
    "    prompt = None  # TODO: Substitute d1 and d2 in the parametrized prompt\n",
    "    answer = None  # TODO: Call the PaLM API with the prompt\n",
    "    return answer.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCfgRBmtn25a"
   },
   "source": [
    "The next cell allows us to rapidly test on the evaluation set whether a given prompt seems to be working for this use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, write a prompt instructing PaLM to answer `yes` if two product descriptions are matching and `no` otherwise.\n",
    "Make sure to parametrize your prompt with `{desc1}` adn `{desc2}` so that different product descriptions `desc1` and `desc2` can be\n",
    "switched at run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xaqND_6Jh5o0",
    "outputId": "693945d5-fd44-47d2-8487-1aff7cffe90a"
   },
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "# TODO\n",
    "\"\"\"\n",
    "\n",
    "index = random.randint(0, len(evalset) - 1)\n",
    "\n",
    "d1 = evalset.iloc[index][\"description_1\"]\n",
    "d2 = evalset.iloc[index][\"description_2\"]\n",
    "ground_truth = evalset.iloc[index][\"match\"]\n",
    "prediction = are_products_matching(d1, d2, prompt=PROMPT)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Are the following two descriptions describing the same product?\n",
    "\n",
    "Description 1: {d1}\n",
    "\n",
    "Descriptions 2: {d2}\n",
    "\n",
    "MODEL ANSWER: {prediction}\n",
    "GROUND TRUTH: {ground_truth}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rH81Xc9iSGH"
   },
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep0gfUg-oIT2"
   },
   "source": [
    "We now analyze the performance of our model on the test set.\n",
    "\n",
    "Large language models may sometimes output something other than \"yes\" or \"no,\" even if we ask them politely to do so. This could be due to safety filters being triggered or the model simply not understanding the question. Therefore, we first need to determine the proportion of requests that our model fails to answer. In this case, it is around 6%, which may be acceptable. Further prompt engineering or model tuning could help to reduce this number.\n",
    "\n",
    "The second aspect to consider is the performance of the model on the requests\n",
    "that succeded (i.e. for whose the output was actually `yes` or `no`).\n",
    "Since the test set is balanced, we can compute the model accuracy, which is 98%. This means that only a single example in the test set received a prediction that was different from the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuBvf49yjPZK"
   },
   "source": [
    "## Scoring the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "es_OWg7FoJgh"
   },
   "source": [
    "To simplify evaluation, we implement a function in the next cell that will add a `prediction` column to our `testset`. This column will contain the predictions received from the PaLM API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the function below so that it adds a column `prediction` to the input dataframe `dataset` containing the PaLM predictions using the prompt you created. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myMTJyk7ieWy"
   },
   "outputs": [],
   "source": [
    "def apply_prompt(prompt, dataset):\n",
    "    scored_dataset = dataset.copy()\n",
    "    # TODO: Update the line below to that the LLM predictions are\n",
    "    # stored in the `predictions` column of the dataframe `scored_dataset`\n",
    "    scored_dataset[\"predictions\"] = None\n",
    "    return scored_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JThUONtioKRO"
   },
   "source": [
    "Let's apply this function to our `testset` using our simple prompt that we designed on the `evalset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNQB_AHIijFk"
   },
   "outputs": [],
   "source": [
    "scored_testset = apply_prompt(prompt=PROMPT, dataset=testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2habZ6dvoMMD"
   },
   "source": [
    "Since requests to the PaLM API are limited and capped to 60 requests per minute, we save our scoring to disk so that we can analyze it offline if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZhJ8h7yjEGX"
   },
   "outputs": [],
   "source": [
    "scored_testset.to_csv(\"scored_evalset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OjoWMSioMlw"
   },
   "source": [
    "Here are the predictions of our model. We can see that most examples are classified correctly, although some requests failed, resulting in empty predictions. We will need to analyze these cases separately and compute the accuracy only for the requests that succeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "wYWoymLxipfJ",
    "outputId": "396efcfa-f80b-4d49-d258-31663617b35e"
   },
   "outputs": [],
   "source": [
    "scored_testset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQXJrSIHjVp3"
   },
   "source": [
    "### Failed predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXij9XUjoNbn"
   },
   "source": [
    "The cell below list the number of failed predictions, that is, predictions which are anything other that `yes` or `no`. There are several possible causes to such a behavior. PaLM, as all LLM's, has been trained to predict the next most likely word from a sequence of words. Therefore, although we instruct PaLM explicitely in our prompt to answer by `yes` or `no`, it may happen that certain product descriptions confuse PaLM, resulting in something different from `yes` or `no`. Another issue is that the language in the product descriptions may trigger a safety filter, which then will replace PaLM raw answer by some standard warning text. These safety filters can be triggered by words in the product descriptions that are too evocating of health or medical issues for instance, or violence, among many [other safety settings](https://developers.generativeai.google/guide/safety_setting). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, write the code that creates a boolean Pandas series\n",
    "`failed_predictions_mask` that contains `True` if the PaLM API prediction stored in `scored_testset` failed and `False` otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUOH1fWxjUuf",
    "outputId": "efd6ba43-3cab-4180-b249-8d0f7ce8219e"
   },
   "outputs": [],
   "source": [
    "failed_predictions_mask = None  # TODO\n",
    "\n",
    "failed_predictions_mask.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycBnLeV8i1pP",
    "outputId": "9ccba179-5b48-44ac-c068-f347557f13f3"
   },
   "outputs": [],
   "source": [
    "proportion_of_failed_predictions = failed_predictions_mask.sum() / len(\n",
    "    failed_predictions_mask\n",
    ")\n",
    "print(\n",
    "    \"Proportion of failed requests:\",\n",
    "    round(proportion_of_failed_predictions, 3) * 100,\n",
    "    \"%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RPK4TbVoPNf"
   },
   "source": [
    "The next cell examines the failed requests. Some of the terms may have triggered safety filters, but this would require further investigation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "S_qPb_vslWJW",
    "outputId": "344d4238-58d9-449f-d790-33b70229b4b5"
   },
   "outputs": [],
   "source": [
    "scored_testset[failed_predictions_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk6s6IGwl-8G"
   },
   "source": [
    "## Accuracy on succeeded predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpBVT1BHoQ4Q"
   },
   "source": [
    "Let us now compute the model accuracy on the requests that succeeded. First, we remove all the failed requests from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVlp6zALmKoI"
   },
   "outputs": [],
   "source": [
    "scored_testset_with_predictions = scored_testset[~failed_predictions_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7y9biD8oRv6"
   },
   "source": [
    "Then, we compute the number of correct answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, compute the accuracy of the PaLM API on the predictions that succeded\n",
    "\n",
    "**Note:** The accuracy of our model is the number of correct answers divided by the number of predictions that succeeded. In this case, the accuracy should be roughly 98%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmIQG6wKoU1t"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, extract from `scored_testset_with_predictions` the uncorrect predictions to inspect them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZCP5TiRmqvV"
   },
   "outputs": [],
   "source": [
    "# Extract the incorrect prediction for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Orchestrating a training pipeline with Vertex Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "\n",
    "1.  **Understand the fundamentals of MLOps**: Grasp the importance of building automated and reproducible machine learning pipelines.\n",
    "2.  **Learn to use the KFP SDK**: Get hands-on experience with the Kubeflow Pipelines SDK to define a pipeline's workflow in Python.\n",
    "3.  **Learn to use Google Cloud Pipeline Components**: Learn how to use pre-built components that provide a simplified interface to Vertex AI services.\n",
    "4.  **Build an end-to-end training pipeline**: Create a pipeline that automates data preparation, model training, and model deployment.\n",
    "5.  **Run and monitor a pipeline**: Learn how to compile the pipeline, submit it to Vertex AI for execution, and monitor its progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT = !(gcloud config get-value project)\n",
    "PROJECT = PROJECT[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Vertex AI pipeline** is a serverless tool that orchestrates your machine learning workflows. Under the hood, Vertex AI Pipelines uses [Kubeflow Pipelines (KFP)](https://www.kubeflow.org/docs/components/pipelines/v1/sdk/sdk-overview/), an open-source platform, which means you can define your pipelines using the KFP SDK and benefit from a large ecosystem of pre-built components.\n",
    "\n",
    "Think of a pipeline as a graph of interconnected components. Each **component** is a self-contained set of code that performs a single step in your ML workflow, such as:\n",
    "\n",
    "*   Preparing data.\n",
    "*   Training a model.\n",
    "*   Evaluating a model.\n",
    "*   Deploying a model.\n",
    "\n",
    "These components take inputs and produce outputs, which are then passed to downstream components, creating a dependency graph that Vertex AI executes for you. This approach has several advantages:\n",
    "\n",
    "1.  **Reproducibility**: Each pipeline run is logged, and its artifacts are stored, which makes it easy to reproduce experiments and track model lineage.\n",
    "2.  **Scalability**: Vertex AI handles the underlying infrastructure, so you can run your pipelines at scale without worrying about provisioning and managing servers.\n",
    "3.  **Modularity**: Since pipelines are composed of individual components, you can easily reuse components across different pipelines and share them with your team.\n",
    "4.  **Automation**: You can trigger pipeline runs on a schedule or in response to events, which is a key component of a robust MLOps strategy.\n",
    "\n",
    "In this lab, you will define your pipeline in a Python file using the **KFP SDK**. You will also use pre-built **Google Cloud Pipeline Components** that provide a simplified, high-level interface to Vertex AI services like AutoML and Vertex AI Endpoints. This makes it very easy to orchestrate sophisticated workflows without having to write a lot of boilerplate code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building and deploying the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline is composed of **components**. A component is a self-contained set of code that performs one step in your ML workflow. In this section, you will define the components that make up your pipeline and then you will orchestrate them in a single Python function.\n",
    "\n",
    "You will use two types of components:\n",
    "\n",
    "1.  **Custom component**: A component that you build yourself. In this lab, you will create a simple component that generates a BigQuery query.\n",
    "2.  **Pre-built components**: The `google-cloud-pipeline-components` library provides a set of pre-built components that make it easy to interact with Vertex AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pipeline variables\n",
    "PIPELINE_NAME = f\"fraud-finder-automl-pipeline-{ID}\"\n",
    "\n",
    "# Feature Store component variables\n",
    "BQ_DATASET = \"tx\"\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{ID}\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "# Dataset component variables\n",
    "DATASET_NAME = f\"fraud_finder_dataset_{ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A custom lightweight component\n",
    "\n",
    "For simple components that don't have a lot of boilerplate code, you can use the KFP SDK to create **lightweight components**. These are Python functions that are converted into pipeline components. KFP handles the process of building a container image for the component for you.\n",
    "\n",
    "The first component in your pipeline will be a lightweight component that generates a BigQuery query. This query will select all the records from the `v_ff_training_dataset` view and save them to a new table. This new table will be the source for our Vertex AI dataset. The main reason for creating this component is to **parameterize the query** and make the pipeline more reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./pipeline_vertex/create_load_query_component.py\n",
    "# Copyright 2025 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n",
    "# use this file except in compliance with the License. You may obtain a copy of\n",
    "# the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\n",
    "\"\"\"Lightweight component ingest features.\"\"\"\n",
    "from typing import Dict, List, NamedTuple\n",
    "\n",
    "from kfp.dsl import Metrics, Output, component\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def create_train_dataset_query(\n",
    "    source_view: str,\n",
    "    destination_table: str) -> str:\n",
    "    \"\"\"\n",
    "    Creating or updating BigQuery training dataset shapshot.\n",
    "    \"\"\"\n",
    "    # using only labeled records:\n",
    "    query_template = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{destination_table}` AS SELECT * FROM {source_view}  WHERE tx_fraud IS NOT NULL\n",
    "    \"\"\"\n",
    "    return query_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the pipeline\n",
    "\n",
    "Now that you have created the custom component, you can define the pipeline's workflow. You will use the KFP SDK to define a Python function that describes the graph of components that make up your pipeline. The `@dsl.pipeline` decorator compiles your Python function into a pipeline definition that can be submitted to Vertex AI.\n",
    "\n",
    "The pipeline will have the following steps:\n",
    "\n",
    "1.  **`create_train_dataset_query`**: This is the custom component you just created. It will generate a BigQuery query and return it as a string.\n",
    "2.  **`BigqueryQueryJobOp`**: This is a pre-built component that takes a BigQuery query as input and executes it. This component will create the training dataset table in BigQuery.\n",
    "3.  **`TabularDatasetCreateOp`**: This pre-built component creates a new Vertex AI Tabular Dataset from a BigQuery table.\n",
    "4.  **`AutoMLTabularTrainingJobRunOp`**: This is the core component of the pipeline. It takes the Vertex AI Dataset as input and trains a tabular classification model using AutoML. You will configure it to:\n",
    "    *   Use `tx_fraud` as the target column.\n",
    "    *   Split the data into training, validation, and test sets.\n",
    "    *   Use a 1-hour training budget.\n",
    "5.  **`EndpointCreateOp`**: Once the model is trained, this component will create a new Vertex AI Endpoint. An endpoint is a resource that you can use to serve predictions from your model.\n",
    "6.  **`ModelDeployOp`**: This final component will deploy the trained model to the endpoint. Once the model is deployed, you will be able to send it prediction requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./pipeline_vertex/pipeline_vertex_automl.py\n",
    "# Copyright 2025 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n",
    "# use this file except in compliance with the License. You may obtain a copy of\n",
    "# the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\n",
    "\"\"\"Kubeflow Fraudfinder Pipeline.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from google_cloud_pipeline_components.v1.automl.training_job import (\n",
    "    AutoMLTabularTrainingJobRunOp,\n",
    ")\n",
    "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    ")\n",
    "from google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\n",
    "\n",
    "from kfp import dsl\n",
    "from create_load_query_component import create_train_dataset_query\n",
    "\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT = os.getenv(\"PROJECT\")\n",
    "REGION = os.getenv(\"REGION\", \"us-central1\")\n",
    "DATASET_SOURCE = os.getenv(\"DATASET_SOURCE\")\n",
    "PIPELINE_NAME = os.getenv(\"PIPELINE_NAME\", \"fraudfinder\")\n",
    "ENDPOINT_NAME = os.getenv(\"ENDPOINT_NAME\", \"ff_model_endpoint\")\n",
    "DISPLAY_NAME = os.getenv(\"MODEL_DISPLAY_NAME\", PIPELINE_NAME)\n",
    "TARGET_COLUMN = os.getenv(\"TARGET_COLUMN\", \"tx_fraud\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "SERVING_MACHINE_TYPE = os.getenv(\"SERVING_MACHINE_TYPE\", \"n1-standard-4\")\n",
    "ID = os.getenv(\"ID\")\n",
    "\n",
    "VERTEX_DATASET_SOURCE=f\"bq://{PROJECT}.{DATASET_SOURCE}\"\n",
    "\n",
    "FEATURESTORE_ID=f\"fraudfinder_{ID}\"\n",
    "# Feature Store component variables\n",
    "BQ_DATASET = \"tx\"\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{ID}\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "bucket_name = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "column_specs = {\n",
    "    'tx_amount': \"numeric\",\n",
    "    'customer_id_avg_amount_14day_window': \"numeric\",\n",
    "    'customer_id_avg_amount_15min_window': \"numeric\",\n",
    "    'customer_id_avg_amount_1day_window': \"numeric\",\n",
    "    'customer_id_avg_amount_30min_window': \"numeric\",\n",
    "    'customer_id_avg_amount_60min_window': \"numeric\",\n",
    "    'customer_id_avg_amount_7day_window': \"numeric\",\n",
    "    'customer_id_nb_tx_14day_window': \"numeric\",\n",
    "    'customer_id_nb_tx_15min_window': \"numeric\",\n",
    "    'customer_id_nb_tx_1day_window': \"numeric\",\n",
    "    'customer_id_nb_tx_30min_window': \"numeric\",\n",
    "    'customer_id_nb_tx_60min_window': \"numeric\",\n",
    "    'customer_id_nb_tx_7day_window': \"numeric\",\n",
    "    'terminal_id_avg_amount_15min_window': \"numeric\",\n",
    "    'terminal_id_avg_amount_30min_window': \"numeric\",\n",
    "    'terminal_id_avg_amount_60min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_14day_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_15min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_1day_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_30min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_60min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_7day_window': \"numeric\",\n",
    "    'terminal_id_risk_14day_window': \"numeric\",\n",
    "    'terminal_id_risk_1day_window': \"numeric\",\n",
    "    'terminal_id_risk_7day_window': \"numeric\"\n",
    "}\n",
    "\n",
    "SOURCE_VIEW = f\"{BQ_DATASET}.v_ff_training_dataset\"\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"{PIPELINE_NAME}-vertex-automl-pipeline\",\n",
    "    description=f\"AutoML Vertex Pipeline for {PIPELINE_NAME}\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def create_pipeline():\n",
    "    \n",
    "    #Prepare SQL Query for BigQuery Job\n",
    "    bq_load_query_op = create_train_dataset_query(\n",
    "        source_view=SOURCE_VIEW,\n",
    "        destination_table=DATASET_SOURCE\n",
    "    )\n",
    "\n",
    "    # Use the BigqueryQueryJobOp to ingest training dataset\n",
    "    bq_job_op = BigqueryQueryJobOp(\n",
    "        project=PROJECT,\n",
    "        query=bq_load_query_op.output,\n",
    "        #query_parameters=bq_query_params_list,\n",
    "    )\n",
    "    \n",
    "    #Create Dataset\n",
    "    dataset_create_task = TabularDatasetCreateOp(\n",
    "        project=PROJECT,\n",
    "        display_name=DISPLAY_NAME,\n",
    "        bq_source=VERTEX_DATASET_SOURCE\n",
    "    ).after(bq_job_op)\n",
    "\n",
    "    # Run the AutoML Tabular Training Job\n",
    "    automl_training_task = AutoMLTabularTrainingJobRunOp(\n",
    "        project=PROJECT,\n",
    "        display_name=DISPLAY_NAME,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        dataset=dataset_create_task.outputs[\"dataset\"],\n",
    "        target_column=TARGET_COLUMN,\n",
    "        timestamp_split_column_name='timestamp',\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1,\n",
    "        # Feature list configuration\n",
    "        column_specs=column_specs,\n",
    "        # column_transformations=column_transformations,\n",
    "        # New parameters for budget and early stopping\n",
    "        budget_milli_node_hours=1000,  # 1000 milli-node hours = 1 node hour\n",
    "        disable_early_stopping=False   # Explicitly set to False to enable early stopping\n",
    "    )\n",
    "    \n",
    "    # Create Vertex AI Endpoint\n",
    "    endpoint_create_task = EndpointCreateOp(\n",
    "        project=PROJECT,\n",
    "        display_name=ENDPOINT_NAME,\n",
    "    ).after(automl_training_task)\n",
    "\n",
    "    # Deploy model to the Vertex AI Endpoint\n",
    "    model_deploy_task = ModelDeployOp(  # pylint: disable=unused-variable\n",
    "        model=automl_training_task.outputs[\"model\"],\n",
    "        endpoint=endpoint_create_task.outputs[\"endpoint\"],\n",
    "        deployed_model_display_name=DISPLAY_NAME,\n",
    "        dedicated_resources_machine_type=SERVING_MACHINE_TYPE,\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "Now that you have defined your pipeline in Python, you need to compile it into a format that Vertex AI can understand. The KFP SDK provides a compiler that takes your Python function and converts it into a YAML file. This YAML file contains a static definition of your pipeline's workflow and can be submitted to Vertex AI for execution.\n",
    "\n",
    "Before you compile the pipeline, you need to define some environment variables. Your pipeline code is designed to be reusable, so instead of hardcoding values like your project ID or a GCS bucket path, you pass these values to the pipeline at runtime. The KFP compiler will embed the values of these environment variables in the compiled YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ARTIFACT_STORE = f\"gs://{PROJECT}-kfp-artifact-store\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATASET_SOURCE = \"tx.train_table_automl_demo\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT={PROJECT}\n",
    "%env REGION={REGION}\n",
    "%env DATASET_SOURCE={DATASET_SOURCE}\n",
    "%env ID={ID}\n",
    "%env BUCKET_NAME={BUCKET_NAME}\n",
    "%env ENDPOINT_NAME={ENDPOINT_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PIPELINE_ROOT` variable points to a GCS bucket that will be used to store the **artifacts** of your pipeline runs. An artifact is an output that is generated by a component, such as a trained model or a dataset. Vertex AI will automatically store these artifacts for you, which is important for tracking model lineage and reproducing experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the KFP CLI to compile the pipeline. The `--py` flag points to your Python file, and the `--output` flag is the name of the YAML file that will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIPELINE_YAML = \"fraudfinder_automl_vertex_pipeline.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kfp dsl compile --py pipeline_vertex/pipeline_vertex_automl.py --output $PIPELINE_YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can also use the Python SDK to compile the pipeline:\n",
    "\n",
    "```python\n",
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=create_pipeline, \n",
    "    package_path=PIPELINE_YAML,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the pipeline file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head {PIPELINE_YAML}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline for execution\n",
    "\n",
    "With the compiled pipeline in hand, you can now submit it to Vertex AI for execution. You will use the Vertex AI Python SDK to do this.\n",
    "\n",
    "The `aiplatform.PipelineJob` class is used to configure and run a pipeline. You will provide the following parameters:\n",
    "\n",
    "*   `display_name`: A human-readable name for the pipeline run.\n",
    "*   `template_path`: The path to the compiled YAML file.\n",
    "*   `enable_caching`: If set to `True`, Vertex AI will try to reuse the outputs of previous component executions if the inputs have not changed. This can save you a lot of time and money when you are iterating on your pipelines.\n",
    "\n",
    "Once you have configured the `PipelineJob`, you can call the `run()` method to start the execution. You will be able to monitor the progress of the pipeline run in the Vertex AI UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"automl_fraudfinder_kfp_pipeline\",\n",
    "    template_path=PIPELINE_YAML,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You have successfully built and run an end-to-end machine learning pipeline using Vertex AI Pipelines. \n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "*   Define a pipeline's workflow in Python using the KFP SDK.\n",
    "*   Use a combination of custom and pre-built Google Cloud Pipeline Components to orchestrate a workflow that uses BigQuery and AutoML.\n",
    "*   Compile a pipeline into a YAML file and submit it to Vertex AI for execution.\n",
    "\n",
    "This pipeline provides a solid foundation for building more sophisticated MLOps workflows. From here, you could:\n",
    "\n",
    "*   **Automate the pipeline**: Use Cloud Scheduler and Cloud Functions to trigger the pipeline on a schedule.\n",
    "*   **Add an evaluation component**: Before deploying the model, you could add a component that evaluates the model's performance on a test set and only deploys the model if it meets a certain quality threshold.\n",
    "*   **Experiment with different model architectures**: You could adapt this pipeline to train a custom model with Vertex AI Training instead of using AutoML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Fraudfinder - Inference Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to use Feature Store and Model Endpoint for Realtime Inference\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "* Invoke a Vetex AI Feature Store and Vetex AI Endpoint to test Realtime predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf",
    "tags": []
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries\n",
    "Next you will import the libraries needed for this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that currently this notebook uses KFP SDK v1, whereas the environment includes KFP v2. As an interim solution, we will downlevel KFP and the Google Cloud Pipeline Components in order to use the v1 code here as-is. See the [KFP migration guide](https://www.kubeflow.org/docs/components/pipelines/v2/migration/) for more details of moving from v1 to v2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Vertex AI SDK\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "REGION=\"us-central1\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from google.cloud import aiplatform as vertex_ai"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vertex AI SDK\n",
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": "# Define helper methods",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "def read_from_sub(project_id, subscription_name, messages=10):\n",
    "    \"\"\"\n",
    "    Read messages from a Pub/Sub subscription\n",
    "    Args:\n",
    "        project_id: project ID\n",
    "        subscription_name: the name of a Pub/Sub subscription in your project\n",
    "        messages: number of messages to read\n",
    "    Returns:\n",
    "        msg_data: list of messages in your Pub/Sub subscription as a Python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    import ast\n",
    "\n",
    "    from google.api_core import retry\n",
    "    from google.cloud import pubsub_v1\n",
    "\n",
    "    subscriber = pubsub_v1.SubscriberClient()\n",
    "    subscription_path = subscriber.subscription_path(project_id, subscription_name)\n",
    "\n",
    "    # Wrap the subscriber in a 'with' block to automatically call close() to\n",
    "    # close the underlying gRPC channel when done.\n",
    "    with subscriber:\n",
    "        # The subscriber pulls a specific number of messages. The actual\n",
    "        # number of messages pulled may be smaller than max_messages.\n",
    "        response = subscriber.pull(\n",
    "            subscription=subscription_path,\n",
    "            max_messages=messages,\n",
    "            retry=retry.Retry(deadline=300),\n",
    "        )\n",
    "\n",
    "        if len(response.received_messages) == 0:\n",
    "            print(\"no messages\")\n",
    "            return\n",
    "\n",
    "        ack_ids = []\n",
    "        msg_data = []\n",
    "        for received_message in response.received_messages:\n",
    "            msg = ast.literal_eval(received_message.message.data.decode(\"utf-8\"))\n",
    "            msg_data.append(msg)\n",
    "            ack_ids.append(received_message.ack_id)\n",
    "\n",
    "        # Acknowledges the received messages so they will not be sent again.\n",
    "        subscriber.acknowledge(subscription=subscription_path, ack_ids=ack_ids)\n",
    "\n",
    "        print(\n",
    "            f\"Received and acknowledged {len(response.received_messages)} messages from {subscription_path}.\"\n",
    "        )\n",
    "\n",
    "        return msg_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from google.cloud.aiplatform import Featurestore, EntityType, Feature\n",
    "\n",
    "def features_lookup(ff_feature_store, entity, entity_ids):\n",
    "    '''\n",
    "    Function that retrieves feature values from Vertex AI Feature Store\n",
    "    '''\n",
    "    entity_type = ff_feature_store.get_entity_type(entity)\n",
    "    aggregated_features = entity_type.read(entity_ids=entity_ids,feature_ids=\"*\")\n",
    "    aggregated_features_preprocessed = preprocess(aggregated_features)\n",
    "    features = aggregated_features_preprocessed.iloc[0].to_dict()\n",
    "    return features\n",
    "\n",
    "def preprocess(payload):\n",
    "    '''\n",
    "    Function that pre-processes the payload values\n",
    "    '''\n",
    "    # replace NaN's\n",
    "    for key , value in payload.items():\n",
    "        if value is None:\n",
    "            payload[key] = 0.0\n",
    "    return payload"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#ENDPOINT_ID=\"...\"\n",
    "#FEATURESTORE_ID='fraudfinder_...'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(ENDPOINT_ID)\n",
    "from google.cloud import aiplatform as aiplatform\n",
    "# Instantiate Vertex AI Endpoint object\n",
    "endpoint_obj = aiplatform.Endpoint(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Instantiate Vertex AI Feature Store object\n",
    "try:\n",
    "    ff_feature_store = Featurestore(FEATURESTORE_ID)\n",
    "except NameError:\n",
    "    print(f\"\"\"The feature store {FEATURESTORE_ID} does not exist!\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "messages_tx = read_from_sub(\n",
    "    project_id=PROJECT_ID, subscription_name=\"ff-tx-sub\", messages=1\n",
    ")\n",
    "\n",
    "messages_tx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(compact=True)\n",
    "\n",
    "# Payload for manual test:\n",
    "# payload_json = {\n",
    "#     \"TX_ID\": \"61210be0744c43232990152d3eb2c2deb6035d8b\",\n",
    "#     \"TX_TS\": \"2025-09-06 17:27:51\",\n",
    "#     \"CUSTOMER_ID\": \"7389471951168361\",\n",
    "#     \"TERMINAL_ID\": \"45087784\",\n",
    "#     \"TX_AMOUNT\": 32.77\n",
    "# }\n",
    "\n",
    "# Reading 1-st message\n",
    "payload_json = messages_tx[0]\n",
    "\n",
    "payload={}\n",
    "payload[\"tx_amount\"] = payload_json[\"TX_AMOUNT\"]\n",
    "\n",
    "# look up the customer features from Vertex AI Feature Store\n",
    "customer_features = features_lookup(ff_feature_store, \"customer\",[payload_json[\"CUSTOMER_ID\"]])\n",
    "# print the customer features from Vertex AI Feature Store\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"customer_features:\")\n",
    "pp.pprint(customer_features)\n",
    "\n",
    "# look up the terminal features from Vertex AI Feature Store\n",
    "terminal_features = features_lookup(ff_feature_store, \"terminal\",[payload_json[\"TERMINAL_ID\"]])\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"terminal features:\")\n",
    "pp.pprint(terminal_features)\n",
    "\n",
    "# Add customer features to payload\n",
    "payload.update(customer_features)\n",
    "\n",
    "# Add terminal features to payload\n",
    "payload.update(terminal_features)\n",
    "\n",
    "del payload['entity_id']\n",
    "\n",
    "payload = preprocess(payload)\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"[Payload to be sent to Vertex AI endpoint]\")\n",
    "pp.pprint(payload)\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "result = endpoint_obj.predict(instances = [payload])\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "pp.pprint(f\"[Prediction result]: {result}\")\n",
    "print(\"-------------------------------------------------------\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

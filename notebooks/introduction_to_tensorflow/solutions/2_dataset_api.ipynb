{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize data load and preprocessing with tf.data\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Learn how to use tf.data to read data from memory\n",
    "1. Learn how to use tf.data to read data from disk\n",
    "1. Learn how to write production input pipelines with feature engineering (batching, shuffling, etc.)\n",
    "1. Learn how to optimize pipeline with tf.data\n",
    "\n",
    "\n",
    "In this notebook, we will start by refactoring the linear regression we implemented in the previous lab so that it takes its data from a`tf.data.Dataset`, and we will learn how to implement **stochastic gradient descent** with it. In this case, the original dataset will be synthetic and read by the `tf.data` API directly from memory.\n",
    "\n",
    "We will use TensorFlow for framework, but **tf.data works with any frameworks like JAX or Pytorch**.\n",
    "\n",
    "In a second part, we will learn how to load a dataset with the `tf.data` API when the dataset resides on disk, and then learn how to optimize the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 16:28:22.827584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756916902.850386 2956739 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756916902.857055 2956739 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the synthetic dataset of the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1756916905.676411 2956739 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "N_POINTS = 10\n",
    "X = tf.constant(range(N_POINTS), dtype=tf.float32)\n",
    "Y = 2 * X + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by implementing a function that takes as input\n",
    "\n",
    "- our $X$ and $Y$ vectors of synthetic data generated by the linear function $y= 2x + 10$\n",
    "- the number of passes over the dataset we want to train on (`epochs`)\n",
    "- the size of the batches in the dataset (`batch_size`)\n",
    "and returns a `tf.data.Dataset`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Note that the last batch may not contain the exact number of elements you specified because the dataset was exhausted.\n",
    "\n",
    "If you want batches with the exact same number of elements per batch, we will have to discard the last batch by\n",
    "setting:\n",
    "\n",
    "```python\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "```\n",
    "\n",
    "We will do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, Y, epochs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function by iterating twice over our dataset in batches of 3 data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0. 1. 2.] y: [10. 12. 14.]\n",
      "x: [3. 4. 5.] y: [16. 18. 20.]\n",
      "x: [6. 7. 8.] y: [22. 24. 26.]\n",
      "x: [9. 0. 1.] y: [28. 10. 12.]\n",
      "x: [2. 3. 4.] y: [14. 16. 18.]\n",
      "x: [5. 6. 7.] y: [20. 22. 24.]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 3\n",
    "EPOCH = 2\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    print(\"x:\", x.numpy(), \"y:\", y.numpy())\n",
    "    assert len(x) == BATCH_SIZE\n",
    "    assert len(y) == BATCH_SIZE\n",
    "assert EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function and the function that computes the gradients are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(X, Y, w0, w1):\n",
    "    Y_hat = w0 * X + w1\n",
    "    errors = (Y_hat - Y) ** 2\n",
    "    return tf.reduce_mean(errors)\n",
    "\n",
    "\n",
    "def compute_gradients(X, Y, w0, w1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "    return tape.gradient(loss, [w0, w1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference now is that now, in the training loop, we will iterate directly on the `tf.data.Dataset` generated by our `create_dataset` function. \n",
    "\n",
    "We will configure the dataset so that it iterates 250 times over our synthetic dataset in batches of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0 - loss: 109.76800537109375, w0: 0.23999999463558197, w1: 0.4399999976158142\n",
      "\n",
      "STEP 100 - loss: 9.363959312438965, w0: 2.55655837059021, w1: 6.674341678619385\n",
      "\n",
      "STEP 200 - loss: 1.393267273902893, w0: 2.2146825790405273, w1: 8.717182159423828\n",
      "\n",
      "STEP 300 - loss: 0.20730558037757874, w0: 2.082810878753662, w1: 9.505172729492188\n",
      "\n",
      "STEP 400 - loss: 0.03084510937333107, w0: 2.03194260597229, w1: 9.809128761291504\n",
      "\n",
      "STEP 500 - loss: 0.004589457996189594, w0: 2.012321710586548, w1: 9.926374435424805\n",
      "\n",
      "STEP 600 - loss: 0.0006827632314525545, w0: 2.0047526359558105, w1: 9.971602439880371\n",
      "\n",
      "STEP 700 - loss: 0.00010164897685172036, w0: 2.0018346309661865, w1: 9.989042282104492\n",
      "\n",
      "STEP 800 - loss: 1.5142451957217418e-05, w0: 2.000706911087036, w1: 9.995771408081055\n",
      "\n",
      "STEP 900 - loss: 2.256260358990403e-06, w0: 2.0002737045288086, w1: 9.998367309570312\n",
      "\n",
      "STEP 1000 - loss: 3.3405058275093324e-07, w0: 2.000105381011963, w1: 9.999371528625488\n",
      "\n",
      "STEP 1100 - loss: 4.977664502803236e-08, w0: 2.000040054321289, w1: 9.999757766723633\n",
      "\n",
      "STEP 1200 - loss: 6.475602276623249e-09, w0: 2.0000154972076416, w1: 9.99991226196289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 250\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.02\n",
    "\n",
    "MSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\\n\"\n",
    "\n",
    "w0 = tf.Variable(0.0)\n",
    "w1 = tf.Variable(0.0)\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    dw0, dw1 = compute_gradients(X_batch, Y_batch, w0, w1)\n",
    "    w0.assign_sub(dw0 * LEARNING_RATE)\n",
    "    w1.assign_sub(dw1 * LEARNING_RATE)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(X_batch, Y_batch, w0, w1)\n",
    "        print(MSG.format(step=step, loss=loss, w0=w0.numpy(), w1=w1.numpy()))\n",
    "\n",
    "assert loss < 0.0001\n",
    "assert abs(w0 - 2) < 0.001\n",
    "assert abs(w1 - 10) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating the CSV files\n",
    "\n",
    "We will start with the **taxifare dataset** CSV files that we wrote out in a previous lab. \n",
    "\n",
    "The taxifare dataset files have been saved into `../data`.\n",
    "\n",
    "Check that it is the case in the cell below, and, if not, regenerate the taxifare\n",
    "dataset by running the previous lab notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jupyter jupyter 123675 Aug 29 17:55 ../data/taxi-test.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 579140 Aug 29 17:55 ../data/taxi-train.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 399647 Aug 29 17:55 ../data/taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../data/taxi*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Low-level tf.data API to read the CSV files\n",
    "\n",
    "To get a more flexible pipeline, we can utilize low-level tf.data APIs to fully control the behavior of the pipeline.\n",
    "\n",
    "For text-based data including CSV, we can use `TextLineDataset` to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Dataset object (`ds`) is still just a definition, and it hasn't loaded the actual data yet.<br>\n",
    "Let's iterate over the first two elements of this dataset using `dataset.take(2)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'fare_amount,pickup_datetime,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key', shape=(), dtype=string)\n",
      "tf.Tensor(b'11.3,2011-01-28 20:42:59 UTC,-73.999022,40.739146,-73.990369,40.717866,1,0', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for data in ds.take(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems it loads the header row as the first element. Since it's not part of the training data, lets' skip it with the `skip()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'11.3,2011-01-28 20:42:59 UTC,-73.999022,40.739146,-73.990369,40.717866,1,0', shape=(), dtype=string)\n",
      "tf.Tensor(b'7.7,2011-06-27 04:28:06 UTC,-73.987443,40.729221,-73.979013,40.758641,1,1', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\").skip(1)\n",
    "\n",
    "for data in ds.take(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features with `.map()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've loaded the CSV file as a text file, and each row was simply represented as a single string value containing speparators (`,`).\n",
    "\n",
    "Let's write a parsing function that takes a row and splits it into multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(row):\n",
    "    return tf.strings.split(row, \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure it works by calling this function in the for loop with `.take()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(8,), dtype=string, numpy=\n",
      "array([b'11.3', b'2011-01-28 20:42:59 UTC', b'-73.999022', b'40.739146',\n",
      "       b'-73.990369', b'40.717866', b'1', b'0'], dtype=object)>\n",
      "<tf.Tensor: shape=(8,), dtype=string, numpy=\n",
      "array([b'7.7', b'2011-06-27 04:28:06 UTC', b'-73.987443', b'40.729221',\n",
      "       b'-73.979013', b'40.758641', b'1', b'1'], dtype=object)>\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\").skip(1)\n",
    "\n",
    "for data in ds.take(2):\n",
    "    values = parse_csv(data)\n",
    "    pprint(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling the function in a for loop, we can wrap it in a `.map()` method to include it in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'11.3' b'2011-01-28 20:42:59 UTC' b'-73.999022' b'40.739146'\n",
      " b'-73.990369' b'40.717866' b'1' b'0'], shape=(8,), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'7.7' b'2011-06-27 04:28:06 UTC' b'-73.987443' b'40.729221'\n",
      " b'-73.979013' b'40.758641' b'1' b'1'], shape=(8,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\").skip(1).map(parse_csv)\n",
    "\n",
    "for data in ds.take(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extend the `parse_csv` function.<br>\n",
    "In machine learning training, we want to pass training data in tuples `(features, label)`.\n",
    "\n",
    "In this CSV file we have these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare_amount,pickup_datetime,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key\n"
     ]
    }
   ],
   "source": [
    "!head -1 ../data/taxi-train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to predict the `fare_amount` value, using `pickuplon`, `pickuplat`, `dropofflon` and `dropofflat` as features. If so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_csv(row):\n",
    "    ds = tf.strings.split(row, \",\")\n",
    "    # Label: fare_amount\n",
    "    label = tf.strings.to_number(ds[0])\n",
    "    # Feature: pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "    features = tf.strings.to_number(ds[2:6])\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: \n",
      "  [-73.99902   40.739147 -73.99037   40.717865], \n",
      "label: \n",
      "  11.300000190734863 \n",
      "++++\n",
      "features: \n",
      "  [-73.98744  40.72922 -73.97901  40.75864], \n",
      "label: \n",
      "  7.699999809265137 \n",
      "++++\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\").skip(1)\n",
    "ds = ds.map(parse_csv)\n",
    "\n",
    "for features, label in ds.take(2):\n",
    "    print(f\"features: \\n  {features}, \\nlabel: \\n  {label} \\n++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, a machine learning training module requires batched data. Let's refactor our pipeline to batch the data by adding `.batch(BATCH_SIZE)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: \n",
      "  [[-73.99902   40.739147 -73.99037   40.717865]\n",
      " [-73.98744   40.72922  -73.97901   40.75864 ]\n",
      " [-73.98254   40.735725 -73.954796  40.77839 ]\n",
      " [-74.001945  40.740505 -73.91385   40.75856 ]], \n",
      "label: \n",
      "  [11.3  7.7 10.5 16.2] \n",
      "++++\n",
      "features: \n",
      "  [[-73.99337   40.753384 -73.8609    40.7329  ]\n",
      " [-73.99624   40.721848 -73.98942   40.718052]\n",
      " [-73.97705   40.75846  -73.9849    40.744694]\n",
      " [-73.9694    40.757545 -73.95005   40.776077]], \n",
      "label: \n",
      "  [33.5  6.9  6.1  9.5] \n",
      "++++\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "ds = tf.data.TextLineDataset(\"../data/taxi-train.csv\").skip(1)\n",
    "ds = ds.map(parse_csv).batch(BATCH_SIZE)\n",
    "\n",
    "for features, label in ds.take(2):\n",
    "    print(f\"features: \\n  {features}, \\nlabel: \\n  {label} \\n++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset is an iterator of *batches*, instead of *rows*, which is suitable for mini-batch training for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling & Repeating\n",
    "\n",
    "When training a deep learning model in batches over multiple workers, it is helpful if we shuffle the data. That way, different workers will be working on different parts of the input file at the same time, and so averaging gradients across workers will help.<br>\n",
    "We can add shuffling with `.shuffle()`. But please note that the shuffle buffer specified in `buffer_size` will be stored on memory, and it is not suitable for full shuffling on very large scale datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap our data pipeline in a `create_dataset` function so that we can control its behaviour and shuffle data only when the dataset is used for training.\n",
    "\n",
    "We will introduce an additional argument `mode` to our function to allow the function body to distinguish the case when it needs to shuffle the data (`mode == \"train\"`) from when it shouldn't (`mode == \"eval\"`).\n",
    "\n",
    "Also, let's add `.repeat()` to read the data indefinitely during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size, mode=\"eval\"):\n",
    "    ds = tf.data.TextLineDataset(pattern).skip(1)\n",
    "    ds = ds.map(parse_csv).repeat()\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our function works well in both modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-73.94901 ,  40.77736 , -73.991936,  40.74759 ],\n",
      "       [-73.98325 ,  40.756077, -73.90924 ,  40.765625]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([14.9,  9.7], dtype=float32)>)]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell multiple times to see the results are different.\n",
    "tempds = create_dataset(\"../data/taxi-train.csv\", 2, \"train\")\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-73.96768 ,  40.79274 , -73.9689  ,  40.791676],\n",
      "       [-74.00532 ,  40.72768 , -73.97028 ,  40.75662 ]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 6.1, 16. ], dtype=float32)>)]\n"
     ]
    }
   ],
   "source": [
    "tempds = create_dataset(\"../data/taxi-valid.csv\", 2, \"eval\")\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Performance with tf.Data\n",
    "\n",
    "Maximizing the performance of data loading and preprocessing phase is critical for many machine learning use cases.\n",
    "\n",
    "`tf.data` offers a number of ways to optimize the process, depending on the cause of performance bottlenecks.<br>\n",
    "Let's take a look at some scenarios.\n",
    "\n",
    "For comparison, we use this `benchmark` function that simulates a training application loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Performance bottleneck in heavy map operation \n",
    "\n",
    "While feature transformation `.map()` is flexible and convenient, this process can be a performance bottleneck when the preprocessing function contains heavy operations.\n",
    "\n",
    "Let's simulate that case by adding sleep time into our parse function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def heavy_parse_csv(row):\n",
    "    ds = tf.strings.split(row, \",\")\n",
    "    label = tf.strings.to_number(ds[0])\n",
    "    features = tf.strings.to_number(ds[2:6])\n",
    "\n",
    "    # Perform a heavy preprocessing...\n",
    "    tf.py_function(lambda: time.sleep(0.001), [], ())\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size=128):\n",
    "    ds = tf.data.TextLineDataset(pattern).skip(1)\n",
    "    ds = ds.map(heavy_parse_csv)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 19.778359695977997\n"
     ]
    }
   ],
   "source": [
    "tempds = create_dataset(\"../data/taxi-train.csv\")\n",
    "benchmark(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow looks like this. The map operation between data read and training is the bottleneck in this case.\n",
    "\n",
    "![Map bottleneck](https://www.tensorflow.org/guide/images/data_performance/sequential_map.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can optimize this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 1: Parallelize map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because input elements are independent of one another, the pre-processing can be parallelized across multiple CPU cores. To make this possible, the map transformation provides the num_parallel_calls argument to specify the level of parallelism.\n",
    "\n",
    "In `.map()` you can specify the `num_parallel_calls` arg along with the function. The number of parallelism can be auto-tuned by specifying `tf.data.AUTOTUNE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size=128):\n",
    "    ds = tf.data.TextLineDataset(pattern).skip(1)\n",
    "    ds = ds.map(heavy_parse_csv, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5.041047303995583\n"
     ]
    }
   ],
   "source": [
    "tempds = create_dataset(\"../data/taxi-train.csv\")\n",
    "benchmark(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is much faster!\n",
    "\n",
    "The flow now looks like this.\n",
    "![parallelized](https://www.tensorflow.org/guide/images/data_performance/parallel_map.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Solution 2: Vectorize the map operation\n",
    "`.map()` processes each individual element returned by a `Dataset`. Our current function is structured to work on one CSV element at a time. However, processing data in batches is always more efficient when feasible.\n",
    "\n",
    "Let's vectorize our function (that is, have it operate over a batch of inputs at once) and apply the `batch` transformation before the `map` transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def heavy_parse_csv_batch(row):\n",
    "    ds = tf.strings.split(row, \",\").to_tensor()\n",
    "    label = tf.strings.to_number(ds[:, 0])\n",
    "    features = tf.strings.to_number(ds[:, 2:6])\n",
    "\n",
    "    # Perform a heavy preprocessing...\n",
    "    tf.py_function(lambda: time.sleep(0.001), [], ())\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size=128):\n",
    "    ds = tf.data.TextLineDataset(pattern).skip(1)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds = ds.map(heavy_parse_csv_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1.23425766202854\n"
     ]
    }
   ],
   "source": [
    "tempds = create_dataset(\"../data/taxi-train.csv\")\n",
    "benchmark(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Performance Bottleneck in I/O (Data Loading) \n",
    "Let's take a look at the next scenario.\n",
    "\n",
    "In a real-world setting, the input data may be stored remotely (for example on Google Cloud Storage in a different location). A dataset pipeline that works well when reading data locally might become bottlenecked on I/O when reading data remotely because of the following differences between local and remote storage:\n",
    "\n",
    "- **Time-to-first-byte**: Reading the first byte of a file from remote storage can take orders of magnitude longer than from local storage.\n",
    "- **Read throughput**: While remote storage typically offers large aggregate bandwidth, reading a single file might only be able to utilize a small fraction of this bandwidth.\n",
    "\n",
    "In addition, once the raw bytes are loaded into memory, it may also be necessary to deserialize and/or decrypt the data (e.g. protobuf), which requires additional computation. This overhead is present irrespective of whether the data is stored locally or remotely, but can be worse in the remote case if data is not prefetched effectively.\n",
    "\n",
    "Let's create a custom dataset to simulate this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IOBoundDataset(tf.data.Dataset):\n",
    "    def _generator(num_samples):\n",
    "        # Opening the file\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        for sample_idx in range(num_samples):\n",
    "            # Reading each line from the file\n",
    "            time.sleep(0.15)\n",
    "\n",
    "            yield (sample_idx,)\n",
    "\n",
    "    def __new__(cls, file_name, num_samples=5):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_signature=tf.TensorSpec(shape=(1,), dtype=tf.int64),\n",
    "            args=(num_samples,),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 42.26503488502931\n"
     ]
    }
   ],
   "source": [
    "ds = IOBoundDataset(\"dummy_file.csv\").repeat(20)\n",
    "benchmark(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Solution 1: Cache\n",
    "Since machine learning training often involves using the same dataset repeatedly, a good strategy is to cache the data during the first epoch and then retrieve it from the cache in subsequent epochs, rather than reloading it from a remote source each time.\n",
    "\n",
    "You can simply insert `.cache()` to use caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3.1102266230154783\n"
     ]
    }
   ],
   "source": [
    "ds = IOBoundDataset(\"dummy_file.csv\").cache().repeat(20)\n",
    "benchmark(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks much faster! However, be careful about what you cache since the cached data will be stored on memory. For example, it's not realistic to cache a terabyte-scale dataset.\n",
    "\n",
    "For example, if you have a CSV file that contains paths of videos for training, instead of caching the actual video file, consider caching the small CSV file by inserting `cache()` before the video load function.\n",
    "\n",
    "```python \n",
    "dataset.map(parse_csv_fn).cache().map(load_video_fn)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Solution 2: Interleave\n",
    "\n",
    "To mitigate the impact of the various data extraction overheads, the tf.data.Dataset.interleave transformation can be used to parallelize the data loading step, interleaving the contents of other datasets (such as data file readers).\n",
    "\n",
    "Let's say we have multiple sharded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"dummy_file_shard001.csv\",\n",
    "    \"dummy_file_shard002.csv\",\n",
    "    \"dummy_file_shard003.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can insert `interleave()` to interleave multiple file load operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 6.420450509001967\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "ds = ds.interleave(IOBoundDataset)\n",
    "\n",
    "benchmark(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how they are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n",
      "tf.Tensor([3], shape=(1,), dtype=int64)\n",
      "tf.Tensor([3], shape=(1,), dtype=int64)\n",
      "tf.Tensor([3], shape=(1,), dtype=int64)\n",
      "tf.Tensor([4], shape=(1,), dtype=int64)\n",
      "tf.Tensor([4], shape=(1,), dtype=int64)\n",
      "tf.Tensor([4], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for d in ds:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset contains values from 0 to 5, and here we can see the data load from 3 files are interleaved.\n",
    "\n",
    "This is the flow image of interleaving (with 2 files in this case)\n",
    "![sequential interleave](https://www.tensorflow.org/guide/images/data_performance/sequential_interleave.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `.map()`, you can parallelize this interleave operation by adding `num_parallel_calls` for further performance gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2.24809078394901\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "ds = ds.interleave(IOBoundDataset, num_parallel_calls=len(files))\n",
    "\n",
    "benchmark(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parallel interleave](https://www.tensorflow.org/guide/images/data_performance/parallel_interleave.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By conbining these techniques, you can design a highly optimized data load and transform pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

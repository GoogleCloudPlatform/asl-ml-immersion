{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering in Keras \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Use Lambda layers to perform feature engineering on geolocation features \n",
    "2. Create bucketized and crossed feature columns\n",
    " \n",
    "\n",
    "## Overview \n",
    "\n",
    "In this notebook, we use Keras to build a taxifare price prediction model and utilize feature engineering to improve the fare amount prediction for NYC taxi cab rides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing the necessary libraries for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Discretization,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "from tensorflow.keras.layers.experimental.preprocessing import HashedCrossing\n",
    "\n",
    "# set TF error log verbosity\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load taxifare dataset\n",
    "\n",
    "The Taxi Fare dataset for this lab is 106,545 rows and has been pre-processed and split for use in this lab.  The fare_amount is the target, the continuous value weâ€™ll train a model to predict.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the files look like we expect them to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../data/taxi-*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/taxi-*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an input pipeline \n",
    "\n",
    "Typically, you will use a two step proces to build the pipeline. Step one is to define the columns of data; i.e., which column we're predicting for, and the default values.  Step 2 is to define two functions - a function to define the features and label you want to use and a function to load the training data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\n",
    "    \"fare_amount\",\n",
    "    \"pickup_datetime\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\",\n",
    "    \"key\",\n",
    "]\n",
    "LABEL_COLUMN = \"fare_amount\"\n",
    "DEFAULTS = [[0.0], [\"na\"], [0.0], [0.0], [0.0], [0.0], [0.0], [\"na\"]]\n",
    "UNWANTED_COLS = [\"pickup_datetime\", \"key\"]\n",
    "\n",
    "INPUT_COLS = [\n",
    "    c for c in CSV_COLUMNS if c != LABEL_COLUMN and c not in UNWANTED_COLS\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to define features and labesl\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "\n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "\n",
    "    return features, label\n",
    "\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def load_dataset(pattern, batch_size=1, mode=\"eval\"):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS\n",
    "    )\n",
    "    dataset = dataset.map(features_and_labels)  # features, label\n",
    "    if mode == \"train\":\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "        # take advantage of multi-threading; 1=AUTOTUNE\n",
    "        dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Baseline DNN Model in Keras\n",
    "\n",
    "Now let's build the Deep Neural Network (DNN) model in Keras using the functional API. Unlike the sequential API, we will need to specify the input and hidden layers.  Note that we are creating a linear regression baseline model with no feature engineering. Recall that a baseline model is a solution to a problem without applying any machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple Keras DNN using its Functional API\n",
    "def rmse(y_true, y_pred):  # Root mean square error\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_dnn_model():\n",
    "    # input layer\n",
    "    inputs = {\n",
    "        colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "        for colname in INPUT_COLS\n",
    "    }\n",
    "\n",
    "    # Constructor for DenseFeatures takes a list of numeric columns\n",
    "    concat_inputs = Concatenate()(inputs.values())\n",
    "\n",
    "    # two hidden layers of [32, 8] just in like the BQML DNN\n",
    "    h1 = Dense(32, activation=\"relu\", name=\"h1\")(concat_inputs)\n",
    "    h2 = Dense(8, activation=\"relu\", name=\"h2\")(h1)\n",
    "\n",
    "    # final output is a linear activation because this is regression\n",
    "    output = Dense(1, activation=\"linear\", name=\"fare\")(h2)\n",
    "    model = models.Model(inputs, output)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build our DNN model and inspect the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_dnn_model()\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    model, \"dnn_model.png\", show_shapes=False, rankdir=\"LR\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "To train the model, simply call [model.fit()](https://keras.io/models/model/#fit).  Note that we should really use many more NUM_TRAIN_EXAMPLES (i.e. a larger dataset). We shouldn't make assumptions about the quality of the model based on training/evaluating it on a small sample of the full data.\n",
    "\n",
    "We start by setting up the environment variables for training, creating the input pipeline datasets, and then train our baseline DNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5\n",
    "NUM_EVALS = 5\n",
    "NUM_EVAL_EXAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds = load_dataset(\"../data/taxi-train*\", TRAIN_BATCH_SIZE, \"train\")\n",
    "evalds = load_dataset(\"../data/taxi-valid*\", 1000, \"eval\").take(\n",
    "    NUM_EVAL_EXAMPLES // 1000\n",
    ")\n",
    "\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "history = model.fit(\n",
    "    trainds,\n",
    "    validation_data=evalds,\n",
    "    epochs=NUM_EVALS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the model loss curve\n",
    "\n",
    "Next, we will use matplotlib to draw the model's loss curves for training and validation.  A line plot is also created showing the mean squared error loss over the training epochs for both the train (blue) and test (orange) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history, metrics):\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for idx, key in enumerate(metrics):\n",
    "        ax = fig.add_subplot(nrows, ncols, idx + 1)\n",
    "        plt.plot(history.history[key])\n",
    "        plt.plot(history.history[f\"val_{key}\"])\n",
    "        plt.title(f\"model {key}\")\n",
    "        plt.ylabel(key)\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\", \"validation\"], loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history, [\"loss\", \"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with the model locally\n",
    "\n",
    "To predict with Keras, you simply call [model.predict()](https://keras.io/models/model/#predict) and pass in the cab ride you want to predict the fare amount for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "    {\n",
    "        \"pickup_longitude\": tf.convert_to_tensor([-73.982683]),\n",
    "        \"pickup_latitude\": tf.convert_to_tensor([40.742104]),\n",
    "        \"dropoff_longitude\": tf.convert_to_tensor([-73.983766]),\n",
    "        \"dropoff_latitude\": tf.convert_to_tensor([40.755174]),\n",
    "        \"passenger_count\": tf.convert_to_tensor([3.0]),\n",
    "    },\n",
    "    steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Model Performance Using Feature Engineering \n",
    "\n",
    "We now improve our model's performance by creating new feature engineerings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocation/Coordinate Feature Columns\n",
    "\n",
    "The pick-up/drop-off longitude and latitude data are crucial to predicting the fare amount as fare amounts in NYC taxis are largely determined by the distance traveled. As such, we need to teach the model the Euclidean distance between the pick-up and drop-off points.\n",
    "\n",
    "Recall that latitude and longitude allows us to specify any location on Earth using a set of coordinates. In our training data set, we restricted our data points to only pickups and drop offs within NYC. New York city has an approximate longitude range of -74.05 to -73.75 and a latitude range of 40.63 to 40.85.\n",
    "\n",
    "#### Computing Euclidean distance\n",
    "The dataset contains information regarding the pickup and drop off coordinates. However, there is no information regarding the distance between the pickup and drop off points. Therefore, we create a new feature that calculates the distance between each pair of pickup and drop off points. We can do this using the Euclidean Distance, which is the straight-line distance between any two coordinate points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff * londiff + latdiff * latdiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling latitude and longitude\n",
    "\n",
    "It is very important for numerical variables to get scaled before they are \"fed\" into the neural network. Here we use min-max scaling (also called normalization) on the geolocation features.  Later in our model, you will see that these values are shifted and rescaled so that they end up ranging from 0 to 1.\n",
    "\n",
    "First, we create a function named 'scale_longitude', where we pass in all the longitudinal values and add 78 to each value.  Note that our scaling longitude ranges from -70 to -78. Thus, the value 78 is the maximum longitudinal value.  The delta or difference between -70 and -78 is 8.  We add 78 to each longitudinal value and then divide by 8 to return a scaled value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_longitude(lon_column):\n",
    "    return (lon_column + 78) / 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a function named 'scale_latitude', where we pass in all the latitudinal values and subtract 37 from each value.  Note that our scaling latitude ranges from -37 to -45. Thus, the value 37 is the minimal latitudinal value.  The delta or difference between -37 and -45 is 8.  We subtract 37 from each latitudinal value and then divide by 8 to return a scaled value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_latitude(lat_column):\n",
    "    return (lat_column - 37) / 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our DNN model now with custom Lambda layers. We'll set `NBUCKETS = 10` to specify 10 buckets when bucketizing the latitude and longitude.\n",
    "\n",
    "So here we define:\n",
    "- 2 Lambda layers with scale_longitude function\n",
    "- 2 Lambda layers with scale_latitude function\n",
    "- a Lambda layers with euclidean function\n",
    "- other feature engineering logics with predefined preprocessing layers\n",
    "- DNN model architecture\n",
    "- model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBUCKETS = 10\n",
    "\n",
    "\n",
    "# DNN MODEL\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_dnn_model():\n",
    "    # input layer is all float\n",
    "    inputs = {\n",
    "        colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "        for colname in INPUT_COLS\n",
    "    }\n",
    "\n",
    "    transformed = {}\n",
    "\n",
    "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
    "    transformed[\"scaled_plon\"] = Lambda(scale_longitude, name=\"scale_plon\")(\n",
    "        inputs[\"pickup_longitude\"]\n",
    "    )\n",
    "    transformed[\"scaled_dlon\"] = Lambda(scale_longitude, name=\"scale_dlon\")(\n",
    "        inputs[\"dropoff_longitude\"]\n",
    "    )\n",
    "\n",
    "    # Scaling latitude from range [37, 45] to [0, 1]\n",
    "    transformed[\"scaled_plat\"] = Lambda(scale_latitude, name=\"scale_plat\")(\n",
    "        inputs[\"pickup_latitude\"]\n",
    "    )\n",
    "    transformed[\"scaled_dlat\"] = Lambda(scale_latitude, name=\"scale_dlat\")(\n",
    "        inputs[\"dropoff_latitude\"]\n",
    "    )\n",
    "\n",
    "    # Apply euclidean function\n",
    "    transformed[\"euclidean_distance\"] = Lambda(euclidean, name=\"euclidean\")(\n",
    "        [\n",
    "            inputs[\"pickup_longitude\"],\n",
    "            inputs[\"pickup_latitude\"],\n",
    "            inputs[\"dropoff_longitude\"],\n",
    "            inputs[\"dropoff_latitude\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    latbuckets = np.linspace(start=0.0, stop=1.0, num=NBUCKETS).tolist()\n",
    "    lonbuckets = np.linspace(start=0.0, stop=1.0, num=NBUCKETS).tolist()\n",
    "\n",
    "    # Bucketization with Discretization layer\n",
    "    plon = Discretization(lonbuckets, name=\"plon_bkt\")(\n",
    "        transformed[\"scaled_plon\"]\n",
    "    )\n",
    "    plat = Discretization(latbuckets, name=\"plat_bkt\")(\n",
    "        transformed[\"scaled_plat\"]\n",
    "    )\n",
    "    dlon = Discretization(lonbuckets, name=\"dlon_bkt\")(\n",
    "        transformed[\"scaled_dlon\"]\n",
    "    )\n",
    "    dlat = Discretization(latbuckets, name=\"dlat_bkt\")(\n",
    "        transformed[\"scaled_dlat\"]\n",
    "    )\n",
    "\n",
    "    # Feature Cross with HashedCrossing layer\n",
    "    p_fc = HashedCrossing(num_bins=NBUCKETS * NBUCKETS, name=\"p_fc\")(\n",
    "        (plon, plat)\n",
    "    )\n",
    "    d_fc = HashedCrossing(num_bins=NBUCKETS * NBUCKETS, name=\"d_fc\")(\n",
    "        (dlon, dlat)\n",
    "    )\n",
    "    pd_fc = HashedCrossing(num_bins=NBUCKETS**4, name=\"pd_fc\")((p_fc, d_fc))\n",
    "\n",
    "    # Embedding with Embedding layer\n",
    "    transformed[\"pd_embed\"] = Flatten()(\n",
    "        Embedding(input_dim=NBUCKETS**4, output_dim=10, name=\"pd_embed\")(\n",
    "            pd_fc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    transformed[\"passenger_count\"] = inputs[\"passenger_count\"]\n",
    "    dnn_inputs = Concatenate()(transformed.values())\n",
    "\n",
    "    # two hidden layers of [32, 8]\n",
    "    h1 = Dense(32, activation=\"relu\", name=\"h1\")(dnn_inputs)\n",
    "    h2 = Dense(8, activation=\"relu\", name=\"h2\")(h1)\n",
    "\n",
    "    # final output is a linear activation because this is regression\n",
    "    output = Dense(1, activation=\"linear\", name=\"fare\")(h2)\n",
    "    model = models.Model(inputs, output)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_dnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model architecture has changed now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model, \"dnn_model_engineered.png\", show_shapes=False, rankdir=\"LR\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see this new model is applying various feature engineering techniques not only with predefined preprocessing layers but also with custom Lambda layers.<br>\n",
    "All the preprocessed Tensors are combined in the `Concatenate` layer, and after that we can stack Dense layers.\n",
    "\n",
    "Now let's train this new model and see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 10\n",
    "NUM_EVALS = 10\n",
    "NUM_EVAL_EXAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds = load_dataset(\"../data/taxi-train*\", TRAIN_BATCH_SIZE, \"train\")\n",
    "evalds = load_dataset(\"../data/taxi-valid*\", 1000, \"eval\").take(\n",
    "    NUM_EVAL_EXAMPLES // 1000\n",
    ")\n",
    "\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "history = model.fit(\n",
    "    trainds,\n",
    "    validation_data=evalds,\n",
    "    epochs=NUM_EVALS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's visualize the DNN model layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history, [\"loss\", \"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's a prediction with this new model with engineered features on the example we had above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "    {\n",
    "        \"pickup_longitude\": tf.convert_to_tensor([-73.982683]),\n",
    "        \"pickup_latitude\": tf.convert_to_tensor([40.742104]),\n",
    "        \"dropoff_longitude\": tf.convert_to_tensor([-73.983766]),\n",
    "        \"dropoff_latitude\": tf.convert_to_tensor([40.755174]),\n",
    "        \"passenger_count\": tf.convert_to_tensor([3.0]),\n",
    "    },\n",
    "    steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Copyright 2021 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

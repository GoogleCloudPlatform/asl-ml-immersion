{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251219ff-472d-4cd5-99ed-192f811a7bdb",
   "metadata": {},
   "source": [
    "# Context Caching with Gemini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70e5bb-c551-47bc-b386-60b2426b9beb",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "\n",
    "1.  Understand the concept of context caching and its benefits when working with large language models.\n",
    "2.  Learn how to use the Vertex AI SDK to create and utilize cached content with Gemini models.\n",
    "3.  Compare the performance of using cached content versus generating content from scratch, highlighting the speed and cost advantages.\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to use context caching with Gemini models in Vertex AI. \n",
    "\n",
    "Context caching allows you to store the processed content, such as research papers, long videos or audios along with system instructions, so you don't have to re-process it every time. <br>\n",
    "When you query the model, it can leverage the stored context, leading to faster response times and reduced resource consumption. This is particularly useful when working with large documents or when using the same context across multiple queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb88194-2240-45b2-a28d-4a69495d770b",
   "metadata": {},
   "source": [
    "##Â Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6629f614-6ac6-4e38-a280-2ac456b0365c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import Part\n",
    "from vertexai.preview import caching\n",
    "from vertexai.preview.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d81b19-c67c-40cd-9ddb-ebbd71547fb1",
   "metadata": {},
   "source": [
    "Here we define the contents variable is a list of `Part` objects, each containing a reference to a research paper in PDF format stored in Google Cloud Storage.<br>\n",
    "These are the papers that will be used for context caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "473eb6f0-0ebf-419b-b638-5132faccd2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert researcher. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "Now look at these research papers, and answer the following questions.\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79a0b6-d13c-44ad-968b-c2dd41cf614c",
   "metadata": {},
   "source": [
    "## Create context caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe710d-c4fe-4700-81cf-0229dac3e7fa",
   "metadata": {},
   "source": [
    "Let's create the cached content. It uses `caching.CachedContent.create` to set up a cache with specified parameters. The parameters are:\n",
    "\n",
    "*   `model_name`: Specifies the Gemini model to use (\"gemini-1.5-pro-002\" in this case).\n",
    "*   `system_instruction`: Sets the instructions for how the model should behave.\n",
    "*   `contents`: The actual documents or other data you want to store in the cache.\n",
    "*   `ttl`: The time-to-live of the cache (60 minutes in this case), after which the cache will expire.\n",
    "*   `display_name`: A name for easy identification.\n",
    "\n",
    "The output of this cell is the unique identifier `cached_content.name` that is used to retrieve cached content later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b96b7c7-652b-490c-96cb-a94ff9538ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5149940940688850944\n"
     ]
    }
   ],
   "source": [
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    ttl=datetime.timedelta(minutes=60),\n",
    "    display_name=\"example-cache\",\n",
    ")\n",
    "\n",
    "print(cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27cd75-ada2-483a-9b18-2bbed9d57da5",
   "metadata": {},
   "source": [
    "Let's take a look at the created context cache!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72af9180-0442-4efb-a5f4-9a3beccbf9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<vertexai.caching._caching.CachedContent object at 0x7fa9e072cdc0>: {\n",
       "   \"name\": \"projects/237937020997/locations/us-central1/cachedContents/5149940940688850944\",\n",
       "   \"model\": \"projects/takumiohym-sandbox/locations/us-central1/publishers/google/models/gemini-1.5-pro-002\",\n",
       "   \"createTime\": \"2025-01-28T05:37:08.263884Z\",\n",
       "   \"updateTime\": \"2025-01-28T05:37:08.263884Z\",\n",
       "   \"expireTime\": \"2025-01-28T06:37:08.212948Z\",\n",
       "   \"displayName\": \"example-cache\",\n",
       "   \"usageMetadata\": {\n",
       "     \"totalTokenCount\": 43127,\n",
       "     \"textCount\": 153,\n",
       "     \"imageCount\": 167\n",
       "   }\n",
       " },\n",
       " <vertexai.caching._caching.CachedContent object at 0x7fa9e072c8b0>: {\n",
       "   \"name\": \"projects/237937020997/locations/us-central1/cachedContents/3570303371388649472\",\n",
       "   \"model\": \"projects/takumiohym-sandbox/locations/us-central1/publishers/google/models/gemini-1.5-pro-002\",\n",
       "   \"createTime\": \"2025-01-28T05:29:13.873488Z\",\n",
       "   \"updateTime\": \"2025-01-28T05:29:13.873488Z\",\n",
       "   \"expireTime\": \"2025-01-28T06:29:13.845257Z\",\n",
       "   \"displayName\": \"example-cache\",\n",
       "   \"usageMetadata\": {\n",
       "     \"totalTokenCount\": 43127,\n",
       "     \"textCount\": 153,\n",
       "     \"imageCount\": 167\n",
       "   }\n",
       " }]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caching.CachedContent.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c62b1b-30c5-40f0-b5f8-b888a79f5ff9",
   "metadata": {},
   "source": [
    "## Generate with Cached Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac7dac-e797-4c49-b8b2-a7b6c0850560",
   "metadata": {},
   "source": [
    "Now we can use the cached content to generate answers. The `GenerativeModel.from_cached_content` method loads the previously created cached content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3559ca98-a075-4597-b13d-0469b2b26935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also retrieve the cached contents with context identifier\n",
    "# cached_content = caching.CachedContent(cached_content_name=caching.CachedContent.list()[0].name)\n",
    "\n",
    "cached_model = GenerativeModel.from_cached_content(\n",
    "    cached_content=cached_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2205dd5-c41e-4fce-9f5a-131028d03fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first paper, \"Gemini: A Family of Highly Capable Multimodal Models,\" introduces Gemini 1.0, Google's family of multimodal models. These models are capable of understanding and generating text, images, audio, and video, and excel in reasoning, coding, and multilingual tasks. The paper details the model architecture, training infrastructure, and evaluation on various benchmarks, demonstrating state-of-the-art performance in many areas, including surpassing human expert performance on the MMLU benchmark.\n",
      "\n",
      "The second paper, \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,\" introduces Gemini 1.5 Pro, an enhanced version capable of processing vastly longer contexts (up to 10 million tokens). This allows the model to handle very long documents, multiple hours of video, and days of audio. The paper focuses on evaluating Gemini 1.5 Pro's long-context capabilities through both synthetic and real-world tasks, including retrieval from extensive materials, long-document QA, long-video QA, and in-context language learning. Notably, Gemini 1.5 Pro demonstrates comparable performance to Gemini 1.0 Ultra while requiring less compute.\n",
      "CPU times: user 30.2 ms, sys: 12.9 ms, total: 43.1 ms\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = cached_model.generate_content(\"What are the papers about?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf99fa3-6123-4564-9f46-d9f8485e26eb",
   "metadata": {},
   "source": [
    "## Generate without cached context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeae236-de34-4459-939c-50733a9a3201",
   "metadata": {},
   "source": [
    "Let's compare the processing time by generating an answer **without** using the cached content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f658918c-0966-4731-8f17-bdd23a94fd52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "\n",
    "contents.append(\"What are the papers about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c8be69-cdbe-4003-abcf-ff3c6d5226da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first paper, \"Gemini: A Family of Highly Capable Multimodal Models,\" introduces Gemini, a family of multimodal models developed by Google.  Gemini is designed to understand and generate text, images, audio, and video, excelling in tasks requiring reasoning, coding, and understanding across different modalities (like interpreting charts and diagrams within text).  The paper discusses the model's architecture, training, and performance on various benchmarks, emphasizing its state-of-the-art results and potential for real-world applications. A key aspect is Gemini's ability to handle a 32k context length, enabling it to process long documents and complex multimodal inputs.\n",
      "\n",
      "The second paper, \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,\" focuses on Gemini 1.5 Pro, an enhanced version of the Gemini model.  The primary advancement is a vastly expanded context window, allowing the model to process up to 10 million tokens. This enables the model to handle exceptionally long inputs like entire books, hours of video, or days of audio, significantly surpassing the capabilities of previous models. The paper emphasizes the model's improved efficiency, long-context performance, and its retained or even improved performance on core benchmarks compared to earlier Gemini versions, demonstrating its potential for complex, long-form multimodal understanding and generation.  A key example given is Gemini 1.5 Pro's ability to learn to translate a low-resource language (Kalamang) based solely on a grammar book and dictionary provided in context.\n",
      "\n",
      "CPU times: user 73.6 ms, sys: 15.5 ms, total: 89.1 ms\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7362ad8-8d9c-482a-bf56-482a7824ba22",
   "metadata": {},
   "source": [
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

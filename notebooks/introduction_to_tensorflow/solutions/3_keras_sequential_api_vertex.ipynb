{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Keras Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives**\n",
    "  1. Build a DNN model using the Keras Sequential API\n",
    "  1. Learn how to train a model with Keras\n",
    "  1. Learn how to save/load, and deploy a Keras model on GCP\n",
    "  1. Learn how to deploy and make predictions with the Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Keras sequential API](https://keras.io/models/sequential/) allows you to create TensorFlow models layer-by-layer. This is useful for building most kinds of machine learning models but it does not allow you to create models that share layers, re-use layers or have multiple inputs or outputs. \n",
    "\n",
    "In this lab, we'll see how to build a simple deep neural network model using the Keras Sequential API. Once we have trained our model, we will deploy it using Vertex AI and see how to call our model for online prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the necessary libraries for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Set `PATH` to include the directory containing saved_model_cli\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}\n",
    "\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf  # Used only for tf.data loader\n",
    "from google.cloud import aiplatform\n",
    "from keras import Input\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(keras.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the taxifare dataset, using the CSV files that we created in the first notebook of this sequence. Those files have been saved into `../data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/taxi*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf.data to read the CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote these functions for reading data from the CSV files above in the [previous notebook](./2a_dataset_api.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_csv(row):\n",
    "    ds = tf.strings.split(row, \",\")\n",
    "    # Label: fare_amount\n",
    "    label = tf.strings.to_number(ds[0])\n",
    "    # Feature: pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "    features = tf.strings.to_number(ds[2:6])  # use some features only\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size, mode=\"eval\"):\n",
    "    ds = tf.data.TextLineDataset(pattern)\n",
    "    ds = ds.map(parse_csv).repeat()\n",
    "    if mode == \"train\":\n",
    "        ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple keras DNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the DNN model. The Sequential model is a linear stack of layers and when building a model using the Sequential API, you configure each layer of the model in turn. Once all the layers have been added, you compile the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a keras DNN model using Sequential API\n",
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(4,), name=\"input_layer\"),\n",
    "        Dense(units=32, activation=\"relu\", name=\"hidden_1\"),\n",
    "        Dense(units=8, activation=\"relu\", name=\"hidden_2\"),\n",
    "        Dense(units=1, activation=\"linear\", name=\"output\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to prepare the model for training, you must configure the learning process. This is done using the compile method. The compile method takes three arguments:\n",
    "\n",
    "* An optimizer. This could be the string identifier of an existing optimizer (such as `rmsprop` or `adagrad`), or an instance of the [Optimizer class](https://keras.io/api/optimizers/).\n",
    "* A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function from the [Losses class](https://keras.io/api/losses/) (such as `categorical_crossentropy` or `mse`), or it can be a custom objective function.\n",
    "* A list of metrics. For any machine learning problem you will want a set of metrics to evaluate your model. A metric could be the string identifier of an existing metric or a custom metric function.\n",
    "\n",
    "We will add an additional custom metric called `rmse` (Root Mean Squared Error). This custom metric is defined using `keras.ops` functions, making it compatible across different Keras backends (TensorFlow, JAX, PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a custom evalution metric\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"rmse\")\n",
    "def rmse(y_true, y_pred):\n",
    "    return keras.ops.sqrt(\n",
    "        keras.ops.mean(keras.ops.square(y_pred[:, 0] - y_true))\n",
    "    )\n",
    "\n",
    "\n",
    "# Compile the keras model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train your model, Keras provides two functions that can be used:\n",
    " 1. `.fit()` for training a model for a fixed number of epochs (iterations on a dataset).\n",
    " 2. `.train_on_batch()` runs a single gradient update on a single batch of data. \n",
    " \n",
    "The `.fit()` function works for various formats of data such as NumPy array, list of Tensors, `tf.data` datasets, and Python generators. The `.train_on_batch()` method is for more fine-grained control over training and accepts only a single batch of data.\n",
    "\n",
    "Our `create_dataset` function above generates batches of training examples, so we can use `.fit`. \n",
    "\n",
    "We start by setting up some parameters for our training job and create the datasets for the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 10  # training dataset will repeat, wrap around\n",
    "NUM_EVALS = 10  # how many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 1000  # enough to get a reasonable sample\n",
    "\n",
    "trainds = create_dataset(\n",
    "    pattern=\"../data/taxi-train.csv\", batch_size=BATCH_SIZE, mode=\"train\"\n",
    ")\n",
    "\n",
    "evalds = create_dataset(\n",
    "    pattern=\"../data/taxi-valid.csv\", batch_size=BATCH_SIZE, mode=\"eval\"\n",
    ").take(NUM_EVAL_EXAMPLES // BATCH_SIZE)\n",
    "\n",
    "print(f\"Training dataset object: {trainds}\")\n",
    "print(f\"Evaluation dataset object: {evalds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various arguments you can set when calling the Keras `fit` method (see [documentation](https://keras.io/api/models/model_training_apis/#fit-method)). Key parameters include:\n",
    "- `x`: Input data. In our case, this is `trainds`, a `tf.data.Dataset` object that yields batches of (features, labels). Keras 3 can handle `tf.data.Dataset` inputs even with non-TensorFlow backends. See the [documentation](https://keras.io/api/models/model_training_apis/#fit-method) to understand other types of input.\n",
    "- `epochs`: The number of times to iterate over the entire training dataset. We're training for 10 epochs.\n",
    "- `validation_data`: Data on which to evaluate the loss and any model metrics at the end of each epoch. In our case, this is `evalds`, another `tf.data.Dataset` object.\n",
    "- `callbacks`: A list of [callbacks](https://keras.io/api/callbacks/) to apply during training. We use a `TensorBoard` callback to save logs for visualization with TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "LOGDIR = \"./taxi_trained\"\n",
    "shutil.rmtree(path=LOGDIR, ignore_errors=True)  # start fresh each time\n",
    "\n",
    "history = model.fit(\n",
    "    x=trainds,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=NUM_EVALS,\n",
    "    validation_data=evalds,\n",
    "    callbacks=[TensorBoard(LOGDIR)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've run data through the model, we can call `.summary()` on the model to get a high-level summary of our network. We can also plot the training and evaluation curves for the metrics we computed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `.fit` returns a History object which collects all the events recorded during training. Similar to Tensorboard, we can plot the training and validation curves for the model loss and rmse by accessing these elements of the History object.\n",
    "\n",
    "Because we haven't done any feature engineering yet, the performance might not be impressive. That's something we'll address in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_COLS = [\"rmse\", \"val_rmse\"]\n",
    "\n",
    "pd.DataFrame(history.history)[RMSE_COLS].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_COLS = [\"loss\", \"val_loss\"]\n",
    "\n",
    "pd.DataFrame(history.history)[LOSS_COLS].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions with our trained model, we can call the `predict` method (see [documentation](https://keras.io/api/models/model_training_apis/#predict-method)). \n",
    "\n",
    "For direct calls on the model object, as shown in the code cell below, we pass a NumPy array containing the input features. The `steps` parameter can be used if an iterator is passed, but for a direct NumPy array input, it's usually not needed for a small number of predictions.\n",
    "\n",
    "Note: When we later deploy this model to a Vertex AI Endpoint, the input format for prediction requests to the endpoint will typically be a JSON dictionary, which may differ from the direct `model.predict()` call here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_input = np.array([[-73.982683, 40.742104, -73.983766, 40.755174]])\n",
    "model.predict(prediction_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For others to use our trained model, especially in production environments, we need to save (or \"export\") it. Keras 3 offers a new, unified way to save models with `model.save('my_model.keras')`, which saves the architecture, weights, and optimizer state in a single file. (see the [Keras saving and serialization guide](https://keras.io/guides/serialization_and_saving/)).\n",
    "\n",
    "However, for deploying to specific platforms like Vertex AI using TensorFlow Serving containers, the TensorFlow SavedModel format is often recommended. In Keras 3, when using the TensorFlow backend, you can export to this format using `model.export(filepath)` as shown in the next cell. You can serialize keras models in this format even when you are using a different backend (JAX or PyTorch), but that may need a slightly different deployment configurations or infrastructure.\n",
    "\n",
    "After exporting to SavedModel format, we can inspect its signature using the `saved_model_cli` tool. This helps understand the expected input and output tensor names and shapes for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./export\"\n",
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "EXPORT_PATH = os.path.join(OUTPUT_DIR, TIMESTAMP)\n",
    "\n",
    "SAVEDMODEL_PATH = os.path.join(EXPORT_PATH, \"savedmodel\")\n",
    "KERAS_PATH = os.path.join(EXPORT_PATH, \"model.keras\")\n",
    "\n",
    "model.export(SAVEDMODEL_PATH)\n",
    "model.save(KERAS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to load the model as a Keras model object, use `keras.models.load_model` and the `.keras` serialization format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(KERAS_PATH)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the saved_model we saved using the `saved_model cli`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show \\\n",
    "    --tag_set serve \\\n",
    "    --signature_def serving_default \\\n",
    "    --dir {SAVEDMODEL_PATH}\n",
    "\n",
    "!find {SAVEDMODEL_PATH}\n",
    "os.environ['SAVEDMODEL_PATH'] = SAVEDMODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy our model to Vertex AI Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will deploy our trained model to Google Cloud's Vertex AI and see how we can make online predictions. \n",
    "\n",
    "First, we have to upload the model along with a serving container image. Vertex AI hosts pre-built Docker images for serving listed in [the documentation](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers).\n",
    "\n",
    "**Important Note:** The following steps for deploying to Vertex AI assume TensorFlow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-line-1",
     "flake8-noqa-line-8-E501"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "MODEL_DISPLAYNAME = f\"taxifare-{TIMESTAMP}\"\n",
    "\n",
    "print(f\"MODEL_DISPLAYNAME: {MODEL_DISPLAYNAME}\")\n",
    "\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-18:latest\"\n",
    ")\n",
    "\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create GCS bucket if it doesn't exist already...\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "\n",
    "if [ -n \"$exists\" ]; then\n",
    "    echo -e \"Bucket exists, let's not recreate it.\"\n",
    "else\n",
    "    echo \"Creating a new GCS bucket.\"\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "    echo \"\\nHere are your current buckets:\"\n",
    "    gsutil ls\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!gsutil cp -R $SAVEDMODEL_PATH gs://$BUCKET/$MODEL_DISPLAYNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the Model\n",
    "\n",
    "The `aiplatform.Model.upload` method registers the model with Vertex AI Model Registry, creating a discoverable and deployable model resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uploaded_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAYNAME,\n",
    "    artifact_uri=f\"gs://{BUCKET}/{MODEL_DISPLAYNAME}\",\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Deploy to an Endpoint\n",
    "\n",
    "After uploading, `uploaded_model.deploy` creates an endpoint and deploys the model to it. An endpoint is a dedicated Vertex AI resource for serving predictions. Parameters include machine_type (machine resources) and accelerator_type/accelerator_count (for GPU acceleration).\n",
    "\n",
    "**The deployment takes around 10 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MACHINE_TYPE = \"n1-standard-2\"\n",
    "\n",
    "endpoint = uploaded_model.deploy(\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once deployed, the model is accessible for online prediction requests. \n",
    "\n",
    "The key within this JSON object must match the input tensor's name in your model's serving signature. \n",
    "For instance, if your model's input layer is named `'input_layer'`, your JSON input would look like `{\"input_layer\": [...]}`, and the each data point needs to be shaped in (4,).  This ensures the prediction service correctly maps your input data to the expected model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instances = [\n",
    "    {\"input_layer\": [-73.982683, 40.742104, -73.983766, 40.755174]},\n",
    "    {\"input_layer\": [-73.976738, 40.751321, -73.986671, 40.748831]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint.predict(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "When deploying a model to an endpoint for online prediction, the minimum `min-replica-count` is 1, and it is charged per node hour. So let's delete the endpoint to reduce unnecessary charges. Before we can delete the endpoint, we first undeploy all attached models... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...then delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

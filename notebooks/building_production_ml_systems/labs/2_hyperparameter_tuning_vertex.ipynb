{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Learn how to use `cloudml-hypertune` to report the results for Cloud hyperparameter tuning trial runs\n",
    "2. Learn how to configure the `.yaml` file for submitting a Cloud hyperparameter tuning job\n",
    "3. Submit a hyperparameter tuning job to Vertex AI\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Let's see if we can improve upon that by tuning our hyperparameters.\n",
    "\n",
    "Hyperparameters are parameters that are set *prior* to training a model, as opposed to parameters which are learned *during* training. \n",
    "\n",
    "These include learning rate and batch size, but also model design parameters such as type of activation function and number of hidden units.\n",
    "\n",
    "Here are the four most common ways to finding the ideal hyperparameters:\n",
    "1. Manual\n",
    "2. Grid Search\n",
    "3. Random Search\n",
    "4. Bayesian Optimzation\n",
    "\n",
    "**1. Manual**\n",
    "\n",
    "Traditionally, hyperparameter tuning is a manual trial and error process. A data scientist has some intuition about suitable hyperparameters which they use as a starting point, then they observe the result and use that information to try a new set of hyperparameters to try to beat the existing performance. \n",
    "\n",
    "Pros\n",
    "- Educational, builds up your intuition as a data scientist\n",
    "- Inexpensive because only one trial is conducted at a time\n",
    "\n",
    "Cons\n",
    "- Requires a lot of time and patience\n",
    "\n",
    "**2. Grid Search**\n",
    "\n",
    "On the other extreme we can use grid search. Define a discrete set of values to try for each hyperparameter then try every possible combination. \n",
    "\n",
    "Pros\n",
    "- Can run hundreds of trials in parallel using the cloud\n",
    "- Guaranteed to find the best solution within the search space\n",
    "\n",
    "Cons\n",
    "- Expensive\n",
    "\n",
    "**3. Random Search**\n",
    "\n",
    "Alternatively define a range for each hyperparameter (e.g. 0-256) and sample uniformly at random from that range. \n",
    "\n",
    "Pros\n",
    "- Can run hundreds of trials in parallel using the cloud\n",
    "- Requires less trials than Grid Search to find a good solution\n",
    "\n",
    "Cons\n",
    "- Expensive (but less so than Grid Search)\n",
    "\n",
    "**4. Bayesian Optimization**\n",
    "\n",
    "Unlike Grid Search and Random Search, Bayesian Optimization takes into account information from past trials to select parameters for future trials. The details of how this is done is beyond the scope of this notebook, but if you're interested you can read how it works here [here](https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization). \n",
    "\n",
    "Pros\n",
    "- Picks values intelligently based on results from past trials\n",
    "- Less expensive because requires fewer trials to get a good result\n",
    "\n",
    "Cons\n",
    "- Requires sequential trials for best results, takes longer\n",
    "\n",
    "**Vertex AI HyperTune**\n",
    "\n",
    "Vertex AI HyperTune, powered by [Google Vizier](https://ai.google/research/pubs/pub46180), uses Bayesian Optimization by default, but [also supports](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview#search_algorithms) Grid Search and Random Search. \n",
    "\n",
    "\n",
    "When tuning just a few hyperparameters (say less than 4), Grid Search and Random Search work well, but when tuning several hyperparameters and the search space is large Bayesian Optimization is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorboard\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# Set `PATH` to include the directory containing tensorboard\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}\n",
    "\n",
    "%load_ext tensorboard\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Change below if necessary\n",
    "PROJECT = !gcloud config get-value project  # noqa: E999\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "%env PROJECT=$PROJECT\n",
    "%env BUCKET=$BUCKET\n",
    "%env REGION=$REGION\n",
    "%env TFVERSION=2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set ai/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make code compatible with Vertex AI Training Service\n",
    "In order to make our code compatible with Vertex AI Training Service we need to make the following changes:\n",
    "\n",
    "1. Upload data to Google Cloud Storage \n",
    "2. Move code into a trainer Python package\n",
    "4. Submit training job with `gcloud` to train on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to Google Cloud Storage (GCS)\n",
    "\n",
    "Cloud services don't have access to our local files, so we need to upload them to a location the Cloud servers can read from. In this case we'll use GCS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET/taxifare/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move code into python package\n",
    "\n",
    "In the [previous lab](./1_training_at_scale.ipynb), we moved our code into a python package for training on Vertex AI. Let's just check that the files are there. You should see the following files in the `taxifare/trainer` directory:\n",
    " - `__init__.py`\n",
    " - `model.py`\n",
    " - `task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la taxifare/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use hyperparameter tuning in your training job you must perform the following steps:\n",
    "\n",
    " 1. Specify the hyperparameter tuning configuration for your training job by including `parameters` in the `StudySpec` of your Hyperparameter Tuning Job.\n",
    "\n",
    " 2. Include the following code in your training application:\n",
    "\n",
    "  - Parse the command-line arguments representing the hyperparameters you want to tune, and use the values to set the hyperparameters for your training trial (we already exposed these parameters as command-line arguments in the earlier lab).\n",
    "\n",
    "  - Report your hyperparameter metrics during training. Note that while you could just report the metrics at the end of training, it is better to set up a callback, to take advantage of Early Stopping.\n",
    "\n",
    "  - Read in the environment variable `$AIP_MODEL_DIR`, set by Vertex AI and containing the trial number, as our `output-dir`. As the training code will be submitted several times in a parallelized fashion, it is safer to use this variable than trying to assemble a unique id within the trainer code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Complete the TODOs just above the `train_and_evaluate` function below. \n",
    "\n",
    " - Instantiate the `cloudml_hypertune` HyperTune reporting object\n",
    " - Inside the on_epoch_end function of the Keras Callback, set up cloudml-hypertune to report the results of each trial by calling its helper function, `report_hyperparameter_tuning_metric`, and define the hyperparameter tuning metric using its `metric_value` parameter\n",
    " \n",
    "Note that compared to the code version in the earlier lab, here we added `import hypertune`, as well as the new callback `callbacks=[ ... , HPTCallback()]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile ./taxifare/trainer/model.py\n",
    "\"\"\"Data prep, train and evaluate DNN model.\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import hypertune\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import callbacks\n",
    "from keras.layers import (\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Discretization,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    HashedCrossing,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "\n",
    "def parse_csv(row):\n",
    "    ds = tf.strings.split(row, \",\")\n",
    "    # Label: fare_amount\n",
    "    label = tf.strings.to_number(ds[0])\n",
    "    # Features: pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "    feature = tf.strings.to_number(ds[2:6])  # use some features only\n",
    "    # Passing feature in tuple so that we can handle them separately.\n",
    "    return (feature[0], feature[1], feature[2], feature[3]), label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size, num_repeat, mode=\"eval\"):\n",
    "    ds = tf.data.Dataset.list_files(pattern)\n",
    "    ds = ds.flat_map(tf.data.TextLineDataset)\n",
    "    ds = ds.map(parse_csv)\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.repeat(num_repeat).batch(batch_size, drop_remainder=True)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def parse_lat_lon(row):\n",
    "    columns = tf.strings.split(row, \",\")\n",
    "    # latitude idx: 3 and 5, longitude idx: 2 and 4\n",
    "    lat_strings = tf.gather(columns, [3, 5])\n",
    "    lon_strings = tf.gather(columns, [2, 4])\n",
    "    lat_features = tf.strings.to_number(lat_strings)\n",
    "    lon_features = tf.strings.to_number(lon_strings)\n",
    "    return lat_features, lon_features\n",
    "\n",
    "\n",
    "def adapt_normalize(train_data_path):\n",
    "    ds = tf.data.Dataset.list_files(train_data_path)\n",
    "    ds = ds.flat_map(tf.data.TextLineDataset)\n",
    "    lat_lon_ds = ds.map(parse_lat_lon).batch(10000)\n",
    "\n",
    "    lat_scaler = keras.layers.Normalization(axis=None)\n",
    "    lon_scaler = keras.layers.Normalization(axis=None)\n",
    "\n",
    "    lat_values, lon_values = next(iter(lat_lon_ds))\n",
    "\n",
    "    lat_scaler.adapt(lat_values)\n",
    "    lon_scaler.adapt(lon_values)\n",
    "\n",
    "    print(\"Computed statistics for latitude:\")\n",
    "    print(f\"mean: {lat_scaler.mean}, variance: {lat_scaler.variance}\")\n",
    "    print(\"+++++\")\n",
    "    print(\"Computed statistics for longitude:\")\n",
    "    print(f\"mean: {lon_scaler.mean}, variance: {lon_scaler.variance}\")\n",
    "\n",
    "    return lat_scaler, lon_scaler\n",
    "\n",
    "\n",
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff * londiff + latdiff * latdiff)\n",
    "\n",
    "\n",
    "def transform(inputs, nbuckets, normalizers):\n",
    "    lat_scaler, lon_scaler = normalizers\n",
    "\n",
    "    # Normalize longitude\n",
    "    scaled_plon = lon_scaler(inputs[\"pickup_longitude\"])\n",
    "    scaled_dlon = lon_scaler(inputs[\"dropoff_longitude\"])\n",
    "\n",
    "    # Normalize latitude\n",
    "    scaled_plat = lat_scaler(inputs[\"pickup_latitude\"])\n",
    "    scaled_dlat = lat_scaler(inputs[\"dropoff_latitude\"])\n",
    "\n",
    "    # Lambda layer for the custom euclidean function\n",
    "    euclidean_distance = Lambda(euclidean, name=\"euclidean\")(\n",
    "        [scaled_plon, scaled_plat, scaled_dlon, scaled_dlat]\n",
    "    )\n",
    "\n",
    "    # Discretization\n",
    "    latbuckets = np.linspace(start=-5, stop=5, num=nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(start=-5, stop=5, num=nbuckets).tolist()\n",
    "\n",
    "    plon = Discretization(lonbuckets, name=\"plon_bkt\")(scaled_plon)\n",
    "    plat = Discretization(latbuckets, name=\"plat_bkt\")(scaled_plat)\n",
    "    dlon = Discretization(lonbuckets, name=\"dlon_bkt\")(scaled_dlon)\n",
    "    dlat = Discretization(latbuckets, name=\"dlat_bkt\")(scaled_dlat)\n",
    "\n",
    "    # Feature Cross with HashedCrossing layer\n",
    "    p_fc = HashedCrossing(num_bins=(nbuckets + 1) ** 2, name=\"p_fc\")((plon, plat))\n",
    "    d_fc = HashedCrossing(num_bins=(nbuckets + 1) ** 2, name=\"d_fc\")((dlon, dlat))\n",
    "    pd_fc = HashedCrossing(num_bins=(nbuckets + 1) ** 4, name=\"pd_fc\")((p_fc, d_fc))\n",
    "\n",
    "    # Embedding with Embedding layer\n",
    "    pd_embed = Flatten()(\n",
    "        Embedding(input_dim=(nbuckets + 1) ** 4, output_dim=10, name=\"pd_embed\")(\n",
    "            pd_fc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    transformed = Concatenate()([\n",
    "        scaled_plon,\n",
    "        scaled_dlon,\n",
    "        scaled_plat,\n",
    "        scaled_dlat,\n",
    "        euclidean_distance, \n",
    "        pd_embed\n",
    "    ])\n",
    "\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    squared_error = tf.keras.ops.square(y_pred[:, 0] - y_true)\n",
    "    return tf.keras.ops.sqrt(tf.keras.ops.mean(squared_error))\n",
    "\n",
    "def build_dnn_model(nbuckets, nnsize, lr, normalizers):\n",
    "    INPUT_COLS = [\n",
    "        \"pickup_longitude\",\n",
    "        \"pickup_latitude\",\n",
    "        \"dropoff_longitude\",\n",
    "        \"dropoff_latitude\",\n",
    "    ]\n",
    "\n",
    "    inputs = {\n",
    "        colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "        for colname in INPUT_COLS\n",
    "    }\n",
    "\n",
    "    # transforms\n",
    "    x = transform(inputs, nbuckets, normalizers)\n",
    "\n",
    "    for layer, nodes in enumerate(nnsize):\n",
    "        x = Dense(nodes, activation=\"relu\", name=f\"h{layer}\")(x)\n",
    "    output = Dense(1, name=\"fare\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "    lr_optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=lr_optimizer, loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "# TODO 1\n",
    "hpt = # TODO: Your code goes here\n",
    "\n",
    "\n",
    "# Reporting callback\n",
    "# TODO 1\n",
    "class HPTCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global hpt\n",
    "        # TODO: Your code goes here\n",
    "\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    nbuckets = hparams[\"nbuckets\"]\n",
    "    lr = hparams[\"lr\"]\n",
    "    nnsize = [int(s) for s in hparams[\"nnsize\"].split()]\n",
    "    eval_data_path = hparams[\"eval_data_path\"]\n",
    "    num_evals = hparams[\"num_evals\"]\n",
    "    num_examples_to_train_on = hparams[\"num_examples_to_train_on\"]\n",
    "    output_dir = hparams[\"output_dir\"]\n",
    "    train_data_path = hparams[\"train_data_path\"]\n",
    "\n",
    "    model_export_path = os.path.join(output_dir, \"model.keras\")\n",
    "    serving_model_export_path = os.path.join(output_dir, \"savedmodel\")\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoint.keras\")\n",
    "    tensorboard_path = os.path.join(output_dir, \"tensorboard\")\n",
    "\n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "\n",
    "    normalizers = adapt_normalize(eval_data_path)\n",
    "\n",
    "    model = build_dnn_model(nbuckets, nnsize, lr, normalizers)\n",
    "    logging.info(model.summary())\n",
    "    \n",
    "    trainds = create_dataset(\n",
    "        pattern=train_data_path, batch_size=batch_size, num_repeat=None, mode=\"train\"\n",
    "    )\n",
    "\n",
    "    evalds = create_dataset(\n",
    "        pattern=eval_data_path, batch_size=batch_size, num_repeat=1, mode=\"eval\"\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "\n",
    "    checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, verbose=1\n",
    "    )\n",
    "    tensorboard_cb = callbacks.TensorBoard(tensorboard_path, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb, HPTCallback()],\n",
    "    )\n",
    "\n",
    "    # Save the Keras model file.\n",
    "    model.save(model_export_path)\n",
    "    # Exporting the model in savedmodel for serving.\n",
    "    model.export(serving_model_export_path)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile taxifare/trainer/task.py\n",
    "\"\"\"Argument definitions for model training code in `trainer.model`.\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Batch size for training steps\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location pattern of eval files\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        default=\"32 8\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nbuckets\",\n",
    "        help=\"Number of buckets to divide lat and lon with\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", help=\"learning rate for optimizer\", type=float, default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help=\"Number of times to evaluate model on eval data training.\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help=\"Number of examples to train on.\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        default=os.getenv(\"AIP_MODEL_DIR\"),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location pattern of train files containing eval URLs\",\n",
    "        required=True,\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    hparams = args.__dict__\n",
    "    print(\"output_dir\", hparams[\"output_dir\"])\n",
    "    model.train_and_evaluate(hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile taxifare/setup.py\n",
    "\"\"\"Using `setuptools` to create a source distribution.\"\"\"\n",
    "\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "setup(\n",
    "    name=\"taxifare_trainer\",\n",
    "    version=\"0.1\",\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description=\"Taxifare model training application.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd taxifare\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp taxifare/dist/taxifare_trainer-0.1.tar.gz gs://${BUCKET}/taxifare/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hyperparameter Tuning Job on Vertex AI\n",
    "\n",
    "Hyperparameter tuning takes advantage of the processing infrastructure of Google Cloud to test different hyperparameter configurations when training your model. It can give you optimized values for hyperparameters, which maximizes your model's predictive accuracy.\n",
    "\n",
    "\n",
    "### Setup CustomJob\n",
    "To leverage that capability, we first define a CustomJob object, just as you would for a normal custom training job on Vertex AI. For more details, please refer to the [custom training notebook](./1_training_at_scale_vertex.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_TO_TRAIN_ON = 50000\n",
    "NUM_EVALS = 20\n",
    "NNSIZE = \"32 8\"\n",
    "\n",
    "base_path = f\"gs://{BUCKET}/taxifare\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "args = [\n",
    "    \"--eval_data_path\",\n",
    "    f\"{base_path}/data/taxi-valid*\",\n",
    "    \"--train_data_path\",\n",
    "    f\"{base_path}/data/taxi-train*\",\n",
    "    \"--num_examples_to_train_on\",\n",
    "    f\"{NUM_EXAMPLES_TO_TRAIN_ON}\",\n",
    "    \"--num_evals\",\n",
    "    f\"{NUM_EVALS}\",\n",
    "    \"--nnsize\",\n",
    "    f\"{NNSIZE}\",\n",
    "]\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": None,\n",
    "            \"accelerator_count\": None,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest\",\n",
    "            \"package_uris\": [f\"{base_path}/taxifare_trainer-0.1.tar.gz\"],\n",
    "            \"python_module\": \"trainer.task\",\n",
    "            \"args\": args,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "custom_job = aiplatform.CustomJob(\n",
    "    display_name=\"custom_job\",\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=f\"{base_path}/staging\",\n",
    "    base_output_dir=f\"{base_path}/trained_model_{timestamp}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HyperparameterTuningJob\n",
    "\n",
    "Now, let's define some specs for the hyperparameter tuning job.\n",
    "When you configure a hyperparameter tuning job, you must specify the following details:\n",
    "\n",
    "#### `metric_spec`\n",
    "the metrics you want to use to optimize for. You can specify in a dictionary `{<metrics tag name>: <goal>}`.\n",
    "\n",
    "The metrics name have to be corresponding to the tag name you report in your training application.\n",
    "\n",
    "For the goal, You can set the goal as eigher `'maximize'` (e.g., accuracy) or `'minimize'` (e.g., loss value).\n",
    "\n",
    "#### `parameter_spec`\n",
    "In a ParameterSpec object, you specify the hyperparameter data type as an instance of a parameter value specification. The following table lists the supported parameter value specifications.\n",
    "\n",
    "|Type|Data type|Value ranges|Value data|\n",
    "|--|--|--|--|\n",
    "|DoubleValueSpec|DOUBLE|minValue & maxValue|Floating-point values|\n",
    "|IntegerValueSpec|INTEGER|minValue & maxValue|Integer values|\n",
    "|CategoricalValueSpec|CATEGORICAL|categoricalValues|List of category strings|\n",
    "|DiscreteValueSpec|DISCRETE|discreteValues|List of values in ascending order|\n",
    "\n",
    "\n",
    "Also, you can specify that scaling for each hyperparameter. The available scaling types are:\n",
    "\n",
    "|Scale Type|Description|Alias|\n",
    "|--|--|--|\n",
    "|UNIT_LINEAR_SCALE|Scales the feasible space linearly| `'linear'`|\n",
    "|UNIT_LOG_SCALE|Scales the feasible space logarithmically 0 through 1. The entire feasible space must be strictly positive.| `'log'`|\n",
    "|UNIT_REVERSE_LOG_SCALE| Scales the feasible space \"reverse\" logarithmically 0 through 1. The result is that values close to the top of the feasible space are spread out more than points near the bottom. The entire feasible space must be strictly positive.| `'reverse_log'`|\n",
    "\n",
    "#### `max_trial_count`\n",
    "\n",
    "Decide how many trials you want to allow the service to run and set the maxTrialCount value in the HyperparameterTuningJob object.\n",
    "\n",
    "Increasing the number of trials generally yields better results, but it is not always so. Usually, there is a point of diminishing returns after which additional trials have little or no effect on the accuracy.\n",
    "\n",
    "#### `Parallel trials`\n",
    "You can specify how many trials can run in parallel by setting parallelTrialCount in the HyperparameterTuningJob.\n",
    "\n",
    "Running parallel trials has the benefit of reducing the time the training job takes (real timeâ€”the total processing time required is not typically changed). \n",
    "\n",
    "**However, running in parallel can reduce the effectiveness of the tuning job overall when you use Google Vizier**. That is because Google Vizier uses the results of previous trials to inform the values to assign to the hyperparameters of subsequent trials. When running in parallel, some trials start without having the benefit of the results of any trials still running.\n",
    "\n",
    "\n",
    "Refer to [the document](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview#hyperparameters) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Complete the TODOs below. \n",
    "\n",
    " - Specify the hyperparameter tuning metric tag and goal\n",
    " - Specify the hypertuning configuration for the learning rate, the batch size and the number of buckets using one of the available [hyperparameter types](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview#data-types). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=f\"taxifare_{timestamp}\",\n",
    "    custom_job=custom_job,\n",
    "    metric_spec={...}, # TODO\n",
    "    parameter_spec={\n",
    "        'lr': hpt.DoubleParameterSpec(...), # TODO\n",
    "        'nbuckets': hpt.IntegerParameterSpec(...) # TODO,\n",
    "        'batch_size': ... # TODO\n",
    "    },\n",
    "    max_trial_count=8,\n",
    "    parallel_trial_count=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the configuration is complete, let's submit a job and wait for it to finish. You can visit the console page using the URL link provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_job.run(sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "source": [
    "**Note:** Here is the equivalent `gcloud ai` command where you can provide the config in yaml file for reference.\n",
    "\n",
    "```bash\n",
    "# Output directory and job name\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "BASE_OUTPUT_DIR=gs://${BUCKET}/taxifare_$TIMESTAMP\n",
    "JOB_NAME=taxifare_$TIMESTAMP\n",
    "echo ${BASE_OUTPUT_DIR} ${REGION} ${JOB_NAME}\n",
    "\n",
    "# Vertex AI machines to use for training\n",
    "PYTHON_PACKAGE_URI=\"gs://${BUCKET}/taxifare/taxifare_trainer-0.1.tar.gz\"\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest\"\n",
    "PYTHON_MODULE=\"trainer.task\"\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=15\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=100\n",
    "NUM_EVALS=10\n",
    "NBUCKETS=10\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "\n",
    "echo > ./config.yaml \"displayName: $JOB_NAME\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: val_rmse\n",
    "    goal: MINIMIZE\n",
    "  parameters:\n",
    "  - parameterId: lr\n",
    "    doubleValueSpec:\n",
    "      minValue: 0.0001\n",
    "      maxValue: 0.1\n",
    "    scaleType: UNIT_LOG_SCALE\n",
    "  - parameterId: nbuckets\n",
    "    integerValueSpec:\n",
    "      minValue: 10\n",
    "      maxValue: 25\n",
    "    scaleType: UNIT_LINEAR_SCALE\n",
    "  - parameterId: batch_size\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 15\n",
    "      - 30\n",
    "      - 50\n",
    "    scaleType: UNIT_LINEAR_SCALE\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  baseOutputDirectory:\n",
    "    outputUriPrefix: $BASE_OUTPUT_DIR\n",
    "  workerPoolSpecs:\n",
    "  - machineSpec:\n",
    "      machineType: $MACHINE_TYPE\n",
    "    pythonPackageSpec:\n",
    "      args:\n",
    "      - --train_data_path=$TRAIN_DATA_PATH\n",
    "      - --eval_data_path=$EVAL_DATA_PATH\n",
    "      - --batch_size=$BATCH_SIZE\n",
    "      - --num_examples_to_train_on=$NUM_EXAMPLES_TO_TRAIN_ON\n",
    "      - --num_evals=$NUM_EVALS\n",
    "      - --nbuckets=$NBUCKETS\n",
    "      - --lr=$LR\n",
    "      - --nnsize=$NNSIZE\n",
    "      executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "      packageUris:\n",
    "      - $PYTHON_PACKAGE_URI\n",
    "      pythonModule: $PYTHON_MODULE\n",
    "    replicaCount: $REPLICA_COUNT\"\n",
    "\n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=config.yaml \\\n",
    "    --max-trial-count=10 \\\n",
    "    --parallel-trial-count=2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open TensorBoard\n",
    "We can use TensorBoard for Hyperparameter tuning jobs to compare loss curves of each trial.\n",
    "\n",
    "You might not see any data for a bit until the job begins. Check the job status on the console, and then return here to click the refresh button in the top right to update TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {base_path}/trained_model_{timestamp} --port 8002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

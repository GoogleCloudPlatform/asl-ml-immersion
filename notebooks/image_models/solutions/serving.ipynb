{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e35ea3a-590f-4c90-9c48-862bd106e964",
   "metadata": {},
   "source": [
    "# Serving Models via TF Serving\n",
    "## Learning Objectives\n",
    "1. Learn how to export Keras models in SavedModel format\n",
    "2. Learn how to load and use SavedModel\n",
    "3. Learn how to customize signatures using TensorFlow\n",
    "4. Learn how to deploy models to Vertex AI\n",
    "5. Learn how to use a deployed model in online and batch prediction\n",
    "\n",
    "In this lab, you will learn how to serve models after training. <br>\n",
    "\n",
    "Serving machine learning models requires infrastructure. Vertex AI makes this simple by providing autoscaling services that reduce setup and maintenance effort.\n",
    "\n",
    "To use Vertex AI, we will look at how to export a Keras model in SavedModel format and deploy it into Vertex AI. Along the way, you learn about signatures, how to customize them, and how to get predictions out of a deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91e398-20db-4ea5-a2c2-1a964390b5e7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7bdc0b-b191-41f3-b401-73d221cbeae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set `PATH` to include the directory containing saved_model_cli\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cbc70-5260-4232-ae05-7843b874c3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "import keras\n",
    "import keras_hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fd1a7-19f6-4cfb-bae5-2c744b5c1a68",
   "metadata": {},
   "source": [
    "## Build and Train a Model\n",
    "Model training is not a focus in this lab, so let's create a simple Mobilenet-based model and use transfer learning to train quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad79e4-b229-4939-a7cf-14d4eb38c8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT + \"-flowers\"\n",
    "FILE_DIR = f\"gs://{BUCKET}/data\"\n",
    "\n",
    "CLASSES = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073ea2d-b5cc-4e55-a86b-caac289459c9",
   "metadata": {},
   "source": [
    "If you haven't run [create_tfrecords_at_scale.ipynb](https://github.com/GoogleCloudPlatform/asl-ml-immersion/blob/master/notebooks/image_models/solutions/create_tfrecords_at_scale.ipynb) notebook, please uncomment the cell below and copy the data from `gs://asl-public` bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff3e9e-8522-44df-8d44-08cd78f0d058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !gsutil mb gs://{BUCKET}\n",
    "# !gsutil cp gs://asl-public/data/flowers/tfrecords/* {FILE_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1798282-a8fe-4a7d-9499-7a8ef57622bd",
   "metadata": {},
   "source": [
    "This dataset contains images of flowers that have been serialized to TFRecords. Use the tf.data API to read and parse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9fef6-4627-42e7-ad52-005ef0b7ca36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil ls {FILE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b825b7d-2ccd-425b-802c-c2401aaf9208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_PATTERN = FILE_DIR + \"/train*\"\n",
    "EVAL_PATTERN = FILE_DIR + \"/eval*\"\n",
    "\n",
    "\n",
    "def parse_example(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    example[\"image\"] = tf.image.resize(\n",
    "        example[\"image\"], [IMG_HEIGHT, IMG_WIDTH]\n",
    "    )\n",
    "    return example[\"image\"], example[\"label\"]\n",
    "\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.TFRecordDataset(tf.io.gfile.glob(TRAIN_PATTERN))\n",
    "    .map(parse_example)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "eval_ds = (\n",
    "    tf.data.TFRecordDataset(tf.io.gfile.glob(EVAL_PATTERN))\n",
    "    .map(parse_example)\n",
    "    .batch(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904b0b0-3e34-43e1-a643-9fa3058c240e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone = keras_hub.models.Backbone.from_preset(\n",
    "    \"mobilenet_v3_large_100_imagenet_21k\",\n",
    ")\n",
    "\n",
    "transfer_model = tf.keras.Sequential(\n",
    "    [\n",
    "        keras.Input(\n",
    "            shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), name=\"mobilenet_input\"\n",
    "        ),\n",
    "        keras.layers.Rescaling(scale=1.0 / 255),\n",
    "        backbone,\n",
    "        keras.layers.GlobalMaxPooling2D(),\n",
    "        keras.layers.Dropout(rate=0.2),\n",
    "        keras.layers.Dense(\n",
    "            len(CLASSES),\n",
    "            activation=\"softmax\",\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transfer_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b4fb7-1812-4241-82e4-64873489d40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transfer_model.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=eval_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7bed7-b254-4eea-8c48-fb4e8f27b899",
   "metadata": {},
   "source": [
    "## Export & load SavedModel\n",
    "Now we have a trained model. In this section, we will export the model in SavedModel format.\n",
    "\n",
    "Also, we will look at how to extend our model with an additional serving function. Additional serving functions allow you to provide preprocessing and/or postprocessing logic to a model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc6f76-ad38-4d39-be73-1e1fb2a8e87f",
   "metadata": {},
   "source": [
    "### Export in SavedModel \n",
    "A SavedModel contains a complete model program, including trained parameters  (i.e., tf.Variables) and the computation graph. We don't need the original model building code to run a model exported with SavedModel, making it useful for sharing or deploying with LiteRT, TensorFlow.js, TensorFlow Serving.\n",
    "\n",
    "You can save and load a model in the SavedModel format using the following APIs:\n",
    "\n",
    "- Keras Model API: Keras supports SavedModel format export via [keras.Model.export](https://keras.io/api/models/model_saving_apis/export/) method using `format=\"tf_saved_model\"` option. It also supports \"onnx\" format.\n",
    "- Keras ExportAchive: [ExportArchive](https://keras.io/api/models/model_saving_apis/export/#exportarchive-class) module supports more finer control like configuring different serving endpoints as well as their signatures.\n",
    "\n",
    "First, let's export our trained model to a SavedModel with a Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39fc25-30a2-47c7-b395-de7c94be310b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(\"export\", ignore_errors=True)\n",
    "os.mkdir(\"export\")\n",
    "# For a normal model serialization you can use .save() with .keras extension\n",
    "transfer_model.save(\"export/flowers_model.keras\")\n",
    "\n",
    "transfer_model.export(\n",
    "    \"export/saved_model\", format=\"tf_saved_model\", verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f6bb4-4070-4aef-ad08-dec47ee5abf2",
   "metadata": {},
   "source": [
    "Let's take a look at the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3523e375-1b30-47c9-9634-399b10749acf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls export/saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9b9d9-f6ec-4e04-a0ad-f3f027baa2c2",
   "metadata": {},
   "source": [
    "We can see multiple files in the directory.\n",
    "\n",
    "- `saved_model.pb` is the SavedModel main file which contains the actual TensorFlow program, or model, and a set of named signatures, each identifying a function that accepts tensor inputs and produces tensor outputs.\n",
    "- `keras_metadata.pb` file is created only with tf.keras.Model.save() function. It contains metadata regarding the Keras model.\n",
    "- `variables` directory contains all the variables of the model.\n",
    "- `assets` directory contains arbitrary files, called assets, that are needed for SavedModel. For example, a vocabulary file used to initialize a lookup table. Upon loading, the assets and the serialized functions that depend on them will refer to the correct file paths inside the SavedModel directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e28749-7773-41c3-b995-52a31555e34e",
   "metadata": {},
   "source": [
    "### Investigate a SavedModel with `saved_model_cli` command\n",
    "\n",
    "If you installed TensorFlow through a pre-built TensorFlow binary, then the SavedModel CLI is already installed on your system at pathname `bin/saved_model_cli`.\n",
    "\n",
    "The SavedModel CLI supports the following two commands on a SavedModel:\n",
    "\n",
    "- `show`, which shows the computations available from a SavedModel.\n",
    "- `run`, which runs a computation from a SavedModel.\n",
    "\n",
    "Here let's investigate the SavedModel with `saved_model_cli show` command specifying the default tag (`serve`) and signature (`serving_default`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c6dc1-0cca-475e-b4d4-29db6beeec27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/saved_model --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a884a2-dcbd-4361-bc2b-447a4f99a9bf",
   "metadata": {},
   "source": [
    "Now we can see the concrete function's descriptions.\n",
    "\n",
    "- Its input shape is a batch(`-1`) of  224x224 images with 3 channels, named `'mobilenet_input'`.\n",
    "- Its output shape is a batched 5 float values (that represent probabilities of 5 flowers), named `'output_*'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8111b3-5286-4647-9842-190f351ab29e",
   "metadata": {},
   "source": [
    "### load and predict\n",
    "\n",
    "Once exported as SavedModel, we can load and use the model in a program.\n",
    "\n",
    "If it is a Python program, you can just call `keras.models.load_model` and load the `.keras.` file instead of SavedModel.\n",
    "\n",
    "Also, in the prediction phase, we sometimes (like in the Web API case) cannot expect that our model always receives preprocessed TFRecords data or batched `(224, 224, 3)` Tensors that we used in the training phase.<br>\n",
    "Let's say the model receives file paths to image data. Then we need to add preprocessing operations to handle the image paths before calling the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12801dfb-aea1-4b97-8214-74db2551ca06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"gs://asl-public/data/flowers/jpegs/10172567486_2748826a8b.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10386503264_e05387e1f7_m.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10391248763_1d16681106_n.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10712722853_5632165b04.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10778387133_9141024b10.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/112334842_3ecf7585dd.jpg\",\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(img_bytes):\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=IMG_CHANNELS)\n",
    "    img = keras.ops.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_from_jpegfile(filename):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = preprocess(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "serving_model = keras.models.load_model(\"export/flowers_model.keras\")\n",
    "input_images = [read_from_jpegfile(f) for f in filenames]\n",
    "\n",
    "f, ax = plt.subplots(1, 6, figsize=(15, 15))\n",
    "for idx, img in enumerate(input_images):\n",
    "    ax[idx].imshow(img.numpy())\n",
    "    batch_image = keras.ops.expand_dims(img, axis=0)\n",
    "    batch_pred = serving_model.predict(batch_image)\n",
    "    pred = batch_pred[0]\n",
    "    pred_label_index = keras.ops.argmax(pred).numpy()\n",
    "    pred_label = CLASSES[pred_label_index]\n",
    "    prob = pred[pred_label_index]\n",
    "    ax[idx].set_title(f\"{pred_label} ({prob:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04613d35-f175-40ac-a771-4a649cac3a0f",
   "metadata": {},
   "source": [
    "### Defining Additional Serving Function\n",
    "\n",
    "So do we need to write this boiler-plate preprocessing code each time?<br>\n",
    "The answer is no! By adding a custom serving signature, we can incorporate these additional preprocessing or postprocessing functions into the SavedModel itself.\n",
    "\n",
    "Once compiled into a SavedModel, the same graph can be executed in various environments, such as on edge devices, within C++ code, or in Javascript programs.\n",
    "\n",
    "Let's assume we deploy this model to a web server and provide prediction via a web API.\n",
    "\n",
    "What kind of signature would be easiest for API clients to use? Instead of asking them to send us tensors of the image contents, we can simply ask them for a GCS JPEG file path, for example. <br>\n",
    "And instead of returning a tensor of 5 probabilities, we can send back easy-to-understand information extracted from the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10226025-2295-436e-a998-ef49f5b92129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess(pred):\n",
    "    top_prob = keras.ops.max(pred, axis=[1])\n",
    "    pred_label_index = keras.ops.argmax(pred, axis=1)\n",
    "    pred_label = keras.ops.take(\n",
    "        keras.ops.convert_to_tensor(CLASSES), pred_label_index\n",
    "    )\n",
    "\n",
    "    # custom output\n",
    "    return {\n",
    "        \"probability\": top_prob,\n",
    "        \"flower_type_int\": pred_label_index,\n",
    "        \"flower_type_str\": pred_label,\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_from_filename(filenames):\n",
    "\n",
    "    # custom pre-process\n",
    "    input_images = keras.ops.map(read_from_jpegfile, filenames)\n",
    "\n",
    "    # model\n",
    "    batch_pred = transfer_model(input_images)  # same as model.predict()\n",
    "\n",
    "    # custom post-process\n",
    "    processed = postprocess(batch_pred)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fd534-17a9-40ea-a3cc-f4f285d496bd",
   "metadata": {},
   "source": [
    "Additionally, let's define another serving function that can receive and preprocess base64 encoded image data.<br>\n",
    "This is useful when we want to send raw image data in online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b5547-83bc-4186-bff6-97c34126295a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_from_b64(img_bytes):\n",
    "\n",
    "    # custom pre-process\n",
    "    input_images = keras.ops.map(preprocess, img_bytes)\n",
    "\n",
    "    # model\n",
    "    batch_pred = transfer_model(input_images)\n",
    "\n",
    "    # custom post-process\n",
    "    processed = postprocess(batch_pred)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59d4cd-6ce5-45d2-abfd-eca92457a8b2",
   "metadata": {},
   "source": [
    "Here, let's export these graphs in the SavedModel format under different key names using `add_endpoint()`.\n",
    "- `serving_default` is the default key name for SavedModel. We override this key with `predict_from_filename` serving function.\n",
    "- We add a new serving signature key `predict_from_b64` with `predict_from_b64` that takes an image byte string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18802d7-4a8a-4f3f-bb0c-4ab5daef0a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "export_archive = keras.export.ExportArchive()\n",
    "\n",
    "export_archive.track(transfer_model)\n",
    "\n",
    "export_archive.add_endpoint(\n",
    "    name=\"serving_default\",\n",
    "    fn=predict_from_filename,\n",
    "    # this function receives 1 string value (filename).\n",
    "    input_signature=[keras.InputSpec(shape=(None,), dtype=\"string\")],\n",
    ")\n",
    "\n",
    "export_archive.add_endpoint(\n",
    "    name=\"predict_base64\",\n",
    "    fn=predict_from_b64,\n",
    "    # this function receives 1 string value (byte string).\n",
    "    input_signature=[keras.InputSpec(shape=(None,), dtype=\"string\")],\n",
    ")\n",
    "\n",
    "export_archive.write_out(\"export/flowers_model_with_signature\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de45e28-3bd3-4f32-9faf-238dd839d0f3",
   "metadata": {},
   "source": [
    "Let's take a look at the new SavedModel metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff6efc-7a4b-4be7-9416-811e59105023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/flowers_model_with_signature --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27db260-09f7-438b-93c2-443372c84919",
   "metadata": {},
   "source": [
    "Now we can see our new `serving_default` gets file paths and return dictionaries with three keys (`flower_type_int`, `flower_type_str`, and `probability`).\n",
    "\n",
    "Let's try to load and use this SavedModel.<br>\n",
    "In Keras, you can load a compiled SavedModelusing `TFSMLayer` layer. (In TensorFlow, you can use `tf.saved_model.load` to do the same)\n",
    "\n",
    "Notice that now we don't need to write additional preprocessing or postprocessing codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9810ae4-6dc9-48ab-82f4-605d42886cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serving_fn = keras.layers.TFSMLayer(\n",
    "    \"export/flowers_model_with_signature\", call_endpoint=\"serving_default\"\n",
    ")\n",
    "\n",
    "pred = serving_fn(keras.ops.convert_to_tensor(filenames))\n",
    "\n",
    "# print custom outputs\n",
    "for k in pred.keys():\n",
    "    print(f\"{k:15}: {pred[k].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258c940-161b-4d20-870e-042a3b09adaa",
   "metadata": {},
   "source": [
    "These outputs look more useful for API clients than a vector of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f76d9-7f25-431d-9600-93e115eacea6",
   "metadata": {},
   "source": [
    "And let's check `predict_base64` signature as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c3707-8354-4f85-9ec8-5638da6da2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/flowers_model_with_signature --tag_set serve --signature_def predict_base64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c69ef5-73b6-4fe1-87de-9b9207ea97d7",
   "metadata": {},
   "source": [
    "## Vertex AI Prediction\n",
    "\n",
    "Now our model is ready for deployment!\n",
    "\n",
    "In this notebook, we deploy our model to the scalable Vertex AI service.\n",
    "Vertex AI supports both Batch Prediction and Online Prediction. \n",
    "\n",
    "First, let's upload the SavedModel to Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2061728-976a-4811-bd3d-b718feb76282",
   "metadata": {},
   "source": [
    "### Upload model to Vertex AI Prediction service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69a247-619e-4f13-aac5-c5bda7ee2c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "MODEL_DISPLAYNAME = f\"flower_classifier-{TIMESTAMP}\"\n",
    "\n",
    "print(f\"MODEL_DISPLAYNAME: {MODEL_DISPLAYNAME}\")\n",
    "\n",
    "# from https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.2-17:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9785c2-fb5a-4cd9-8b9e-5deba5d635dd",
   "metadata": {},
   "source": [
    "We upload the SavedModel to a GCS bucket at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae34dd1-0a73-43c3-aa62-0a5acd03a558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp -R export/flowers_model_with_signature gs://{BUCKET}/{MODEL_DISPLAYNAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629d5c4-7eed-4b53-9792-69249ebcfbef",
   "metadata": {},
   "source": [
    "We can use Python SDK to upload models.\n",
    "\n",
    "Here we are specifying `display_name`, `artifact_uri`, which is the path of SavedModel, and `serving_container_image_uri`, which is a container environment on which our model runs (pre-build container is selected in this case, but you can use a custom container if needed).\n",
    "\n",
    "For more detail, please refer to [the SDK document](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_upload)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8bf4d-6cee-4bfc-b927-a85879c5dd7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uploaded_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAYNAME,\n",
    "    artifact_uri=f\"gs://{BUCKET}/{MODEL_DISPLAYNAME}\",\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03382fbf-d5a5-48a9-8718-86f284d21f58",
   "metadata": {},
   "source": [
    "After uploading it, you can check your model on the console UI by clicking Vertex AI -> Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be32d9-b537-4694-aca6-baf3467c2b25",
   "metadata": {},
   "source": [
    "### Batch Prediction\n",
    "\n",
    "In batch prediction, we can pass a large dataset to our model and predict as a batch.<br>\n",
    "\n",
    "#### Create a prediction file\n",
    "[Batch Prediction](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions) service accepts JSON Lines, TF Records, CSV, or simple text file list format.\n",
    "\n",
    "Here we create a simple dataset containing many image file paths in [JSON Lines](https://jsonlines.org/) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3c974-fe22-4c43-9939-5dcbb2b5a418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = !gsutil ls -r gs://asl-public/data/flowers/jpegs/*.jpg\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d126fbb-57c5-4959-8094-7ba3a6a8d4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON_FILE = \"batch_prediction.jsonl\"\n",
    "\n",
    "with open(JSON_FILE, \"w\") as f:\n",
    "    for file in files:\n",
    "        f.write(json.dumps({\"filenames\": file}) + \"\\n\")\n",
    "\n",
    "!head {JSON_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b570f68-55af-478a-9583-818fdb5a7071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp {JSON_FILE} {FILE_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa689f69-960f-449d-b859-444361111187",
   "metadata": {},
   "source": [
    "#### Send a Batch Prediction Job\n",
    "Let's call a batch prediction job with [`aiplatform.batch_predict()`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_batch_predict) function.\n",
    "\n",
    "Note that we can specify machine type and accelerator as needed.<br>\n",
    "This is very useful when we want to process a large amount of data in a limited time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28827b2b-952b-4005-b473-b5aab86a0c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "JOB_DISPLAY_NAME = \"flower_classification_batch\"\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "batch_pred_job = uploaded_model.batch_predict(\n",
    "    job_display_name=JOB_DISPLAY_NAME,\n",
    "    gcs_source=f\"{FILE_DIR}/{JSON_FILE}\",\n",
    "    gcs_destination_prefix=f\"{FILE_DIR}/batch_prediction_result/{TIMESTAMP}\",\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6bbf35-a067-481f-8ae1-0686022ced3f",
   "metadata": {},
   "source": [
    "**Notice it takes around 20 minutes. Please wait until that or move forward to the Online Prediction section and return to the next cell later. You can check the status on Vertex AI -> Batch Predictions page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d4087-1f06-4b9e-8f78-faf011eec740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if batch_pred_job.output_info:\n",
    "    output_dir = batch_pred_job.output_info.gcs_output_directory\n",
    "    results = !gsutil cat {output_dir}/prediction.results*\n",
    "    for r in results[:5]:\n",
    "        r = json.loads(r)\n",
    "        print(f\"filename       : {r['instance']['filenames']}\")\n",
    "        for k in r[\"prediction\"].keys():\n",
    "            print(f\"{k:15}: {r['prediction'][k]}\")\n",
    "        print(\"*\" * 30)\n",
    "else:\n",
    "    print(f\"This job is still running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e051da5-d8ed-41fa-bac4-dba2fe2bae79",
   "metadata": {},
   "source": [
    "### Online Prediction\n",
    "\n",
    "In the Online Prediction option, you can create a dedicated endpoint for your model, and use it as a web API.\n",
    "\n",
    "Let's create an endpoint and link your model to it by [`aiplatform.Model.deploy`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_deploy) function. Here you can also specify the machine type and the accelerators.\n",
    "\n",
    "**The command below takes around 10 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240afcaf-b590-460d-83f0-bee6b4b76651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = uploaded_model.deploy(\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5e103-f36e-464f-92ba-8f9af9d17caf",
   "metadata": {},
   "source": [
    "After the deployment, we can simply call the endpoint and retrieve the result. <br>\n",
    "You can check the endpoint details by visiting the Vertex AI -> Endpoints page.\n",
    "\n",
    "Here we stick with the [Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_predict), but note that you can call the endpoint from any environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee891ca7-7c0d-445a-aede-fbea146ca5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instances = [{\"filenames\": f} for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5efa7-e289-4c57-9e4b-fcf8e62c07dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = endpoint.predict(instances=instances)\n",
    "\n",
    "# print custom outputs\n",
    "for p in pred.predictions:\n",
    "    for k in p.keys():\n",
    "        print(f\"{k:15}: {p[k]}\")\n",
    "    print(\"*\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52abb6-4dc8-4712-8e7a-115e47d3cf82",
   "metadata": {},
   "source": [
    "### Online Prediction with raw images\n",
    "\n",
    "Next, let's call `predict_base64` signature and pass raw image data. <br>\n",
    "In order to submit raw image data via API, you must Base64 encode the data and encapsulate it in a JSON object having b64 as the key as follows:\n",
    "\n",
    "```python\n",
    "{ \"b64\": <base64 encoded string> }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348836b1-48ca-4516-85df-ab8a1d1f2d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download a sample image from GCS.\n",
    "!gsutil cp gs://asl-public/data/flowers/jpegs/10172567486_2748826a8b.jpg sample.jpg\n",
    "\n",
    "\n",
    "def b64encode(filename):\n",
    "    with open(filename, \"rb\") as ifp:\n",
    "        img_bytes = ifp.read()\n",
    "        return base64.b64encode(img_bytes).decode()\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"signature_name\": \"predict_base64\",\n",
    "    \"instances\": [{\"img_bytes\": {\"b64\": b64encode(\"./sample.jpg\")}}],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e498d3-a0ab-45ad-9eef-a9d9eb618851",
   "metadata": {},
   "source": [
    "Since Python SDK `endpoint.predict` supports only `serving_default` signature, here let's define an API call directly with general Python `request` module, and use `rawPredict` API to spefify other signatures instead.\n",
    "\n",
    "In order to do so, we need to define an authorization token and wrap it in a request header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8789d1-49c7-49e3-8e8d-3a7ff9b02887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token = (\n",
    "    GoogleCredentials.get_application_default().get_access_token().access_token\n",
    ")\n",
    "headers = {\"Authorization\": \"Bearer \" + token}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e2c95-83fa-42ea-8ef1-31152989d5f1",
   "metadata": {},
   "source": [
    "The endpoint URL is `https://<region>-aiplatform.googleapis.com/v1/projects/<project id>/locations/<region>/endpoints/<endpoint id>:rawPredict`. <br>\n",
    "Let's define accordingly and send the encoded raw image to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c5622-1faa-4595-80c9-c05802a8fae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api = \"https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/endpoints/{}:rawPredict\".format(\n",
    "    REGION, PROJECT, REGION, endpoint.name\n",
    ")\n",
    "\n",
    "response = requests.post(api, json=data, headers=headers)\n",
    "json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4329b92-80fe-474b-ad3f-aa060727eeb6",
   "metadata": {},
   "source": [
    "Now we could get the result from the Vertex AI online prediction service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e032c-16c0-4143-9578-bf71af10b74e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We learned how to:\n",
    "- export and load a SavedModel\n",
    "- customize the serving function to control a SavedModel behavior\n",
    "- deploy a SavedModel to Vertex AI\n",
    "- Use deployed model both for Batch and Online predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0908e0-478f-4833-9bcc-8fe91bd2c5fb",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefffa4a-cedd-4554-919a-e709d7a84948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

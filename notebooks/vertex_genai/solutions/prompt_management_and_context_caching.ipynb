{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251219ff-472d-4cd5-99ed-192f811a7bdb",
   "metadata": {},
   "source": [
    "# Prompt Management and Context Caching with Gemini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70e5bb-c551-47bc-b386-60b2426b9beb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1.  Learn how to use Vertex AI SDK to manage the lifecycle of prompt templates.\n",
    "2.  Learn how to define, save, load and manage the prompts directly within Python code.\n",
    "3.  Understand the concept of context caching and its benefits when working with large language models.\n",
    "4.  Learn how to use the Vertex AI SDK to create and utilize cached content with Gemini models.\n",
    "5.  Compare the performance of using cached content versus generating content from scratch, highlighting the speed and cost advantages.\n",
    "\n",
    "## Overview\n",
    "This notebook explores two key aspects of working with generative AI on Google Cloud. The first part focuses on Vertex AI's prompt management capabilities, explaining how to programmatically create, version, and organize prompt templates using the Vertex AI SDK. The second part introduces the Gemini API's context caching feature, designed to optimize requests with large, consistent initial contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919d988-e8f3-4935-98b4-f4b576de81c5",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57172993-318c-4a69-91fd-56e4accd6fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import vertexai\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from vertexai.preview import prompts\n",
    "from vertexai.preview.prompts import Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81084c80-7378-4c5c-994b-b0632911ce99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "MODEL = \"gemini-2.0-flash-001\"\n",
    "\n",
    "client = genai.Client(vertexai=True, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06e100-5083-4ebc-9e49-0ce0a8a41a2c",
   "metadata": {},
   "source": [
    "## Prompt Management\n",
    "\n",
    "Vertex AI offers prompt management through its user interface, Vertex AI Studio, and programmatically via the Vertex AI SDK. This section focuses on the latter method, demonstrating how to leverage the `vertexai.preview.prompts` module to define, save, and manage prompts specifically for Gemini text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebb728-254c-4e4c-8b1f-b8a68827e1ed",
   "metadata": {},
   "source": [
    "### The Prompt class\n",
    "\n",
    "To effectively manage prompts, we will use the [Prompt class](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.preview.prompts#prompt). This class represent a prompt object, encapsulates the prompt data, variables, generation configuration, and other relevant information.\n",
    "\n",
    "Consider managing a social media page that features two-sentence stories with two main characters. The [Prompt class](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.preview.prompts#prompt) can define a reusable prompt template, enabling the generation of multiple stories with varied character pairings from a single structure. \n",
    "\n",
    "Let's construct the prompt object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674d29f-154e-44fd-8c4d-ddb84d9463a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize vertexai\n",
    "vertexai.init(project=PROJECT, location=\"us-central1\")\n",
    "\n",
    "# Create local Prompt\n",
    "prompt = Prompt(\n",
    "    prompt_name=\"story-writer\",\n",
    "    prompt_data=\"Generate a story with 2 main characters: {A} and {B}.\",\n",
    "    model_name=MODEL,\n",
    "    system_instruction=\"You are a story writer. Write a short story in 2 sentences. Don't replace the words in the variables with their synnonyms.\",\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307ae9e-d1b7-496c-b4ae-cab4d6120158",
   "metadata": {},
   "source": [
    "After the creation of a Prompt object, the prompt data and properties representing various configurations can be used to generate content. Let's generate content for different variable sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798223b-106c-424f-adef-197d268ad38a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content = prompt.assemble_contents(A=\"cat\", B=\"dog\")\n",
    "# OR with dict: prompt.assemble_contents(**{\"A\":\"cat\", \"B\":\"dog\"})\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f8bc0-9072-4590-8806-2b9e5953d01c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = prompt.generate_content(contents=content)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb88194-2240-45b2-a28d-4a69495d770b",
   "metadata": {},
   "source": [
    "### Save, load and update a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fe1f9-005c-48be-816e-a536e594038a",
   "metadata": {},
   "source": [
    "We can use the `vertexai.preview.prompts.create_version()` method to save a prompt online, making it accessible in the Google Cloud console. This method takes a Prompt object and creates a new version in the online store, returning an updated Prompt object linked to this resource. Remember that changes to a Prompt object are only saved online when `create_version()` is explicitly called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd31ffb-b708-47be-9c20-5c223d51a1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_v1 = prompts.create_version(prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb46078-b42f-43b0-975d-17a2ff20654d",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can go to Google Cloud Console to check your newly created prompt in [Prompt Management](https://console.cloud.google.com/vertex-ai/studio/saved-prompts). You can also retrieve a saved prompt from the online resource using the `vertexai.preview.prompts.get()` method. Simply provide the prompt's unique ID to this function, and it will return the associated Prompt object, as demonstrated in the following code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf98bc-6c93-418d-ba80-59cbd4e31915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_prompt = prompts.get(prompt_id=prompt_v1.prompt_id)\n",
    "\n",
    "loaded_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0808931-f454-45c6-8873-17d786b4a1eb",
   "metadata": {},
   "source": [
    "After retrieving a prompt using `get()`, you can modify its attributes and save these modifications as a new version. For instance, setting the new content to prompt_data updates the prompt locally—these changes are saved online only when create_version() is invoked. Because the prompt is associated with a prompt resource, `create_version()` generates a new version under the same prompt_id and returns a new `Prompt` object linked to the online resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4fac2-19e3-4656-bd41-756a6b2a59db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_prompt.prompt_data = (\n",
    "    \"Write a story with {A} as the protagonist and {B} as the antagonist.\"\n",
    ")\n",
    "\n",
    "prompt_v2 = prompts.create_version(prompt=loaded_prompt)\n",
    "\n",
    "prompt_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f30157-46fd-4a61-87bd-a0e80281604b",
   "metadata": {},
   "source": [
    "### List prompts and prompt versions\n",
    "\n",
    "To see the display names and prompt IDs of all prompts saved in the current Google Cloud project, use the `list_prompts()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d973d8-ecea-4f9f-be1a-c1253375b811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts_metadata = prompts.list()\n",
    "\n",
    "prompts_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc08941-827f-4afa-abc6-c4f3204d2c89",
   "metadata": {},
   "source": [
    "After checking the prompt list, you can specify the index to retriave a specific prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b1350-c92f-4ff4-8fa8-df8f23a38f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_prompt = prompts.get(prompt_id=prompts_metadata[0].prompt_id)\n",
    "\n",
    "retrieved_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27776abf-a694-458b-b4cb-4b48dd88d2d6",
   "metadata": {},
   "source": [
    "To see the display names and version IDs of all prompt versions saved within the prompt, use the `list_versions()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d1022-26b3-44eb-af7e-0897c34b91dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_versions_metadata = prompts.list_versions(prompt_id=prompt_v1.prompt_id)\n",
    "\n",
    "prompt_versions_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268db51-97e7-4826-b141-652d96c53e7a",
   "metadata": {},
   "source": [
    "### Restore a prompt version\n",
    "\n",
    "Prompt resources keep a history of saved versions. To revert to a previous version, use the `restore_version()` method, which makes that older version the latest one. This method returns metadata you can use with `get()` to retrieve the newly restored version.\n",
    "\n",
    "For instance, the following code restores the prompt content to version id 1, the original version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b87721-ffa4-4e55-8325-1be5fd7eeec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_version_metadata = prompts.restore_version(\n",
    "    prompt_id=prompt_v1.prompt_id, version_id=\"1\"\n",
    ")\n",
    "\n",
    "restored_prompt = prompts.get(prompt_id=prompt_version_metadata.prompt_id)\n",
    "\n",
    "restored_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ea7a9-b17c-457c-ba41-716227bdd475",
   "metadata": {},
   "source": [
    "### Delete a prompt\n",
    "\n",
    "To delete the online resource associated with a prompt ID, use the `delete()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1713b-5b9d-4cde-ba9e-a5070c9c22c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts.delete(prompt_id=prompt_v1.prompt_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a329286-4be9-45cc-b3e5-c0e9d1496fdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Context caching\n",
    "\n",
    "The second section of this notebook demonstrates how to use context caching with Gemini models in Vertex AI. \n",
    "\n",
    "Context caching allows you to store the processed content, such as research papers, long videos or audios along with system instructions, so you don't have to re-process it every time. <br>\n",
    "When you query the model, it can leverage the stored context, leading to faster response times and reduced resource consumption. This is particularly useful when working with large documents or when using the same context across multiple queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6270e-9506-4c43-bf52-d557431f842f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d81b19-c67c-40cd-9ddb-ebbd71547fb1",
   "metadata": {},
   "source": [
    "Here we define the contents variable as a list of `Part` objects, each containing a reference to a research paper in PDF format stored in Google Cloud Storage.<br>\n",
    "These are the papers that will be used for context caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473eb6f0-0ebf-419b-b638-5132faccd2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert researcher. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "Now look at these research papers, and answer the following questions.\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    types.Part.from_uri(\n",
    "        file_uri=\"gs://asl-public/data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    types.Part.from_uri(\n",
    "        file_uri=\"gs://asl-public/data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79a0b6-d13c-44ad-968b-c2dd41cf614c",
   "metadata": {},
   "source": [
    "### Create context caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe710d-c4fe-4700-81cf-0229dac3e7fa",
   "metadata": {},
   "source": [
    "Let's create the cached content. It uses `client.caches.create` to set up a cache with specified parameters. The parameters are:\n",
    "\n",
    "*   `model`: Specifies the Gemini model to use (\"gemini-2.0-flash-001\" in this case).\n",
    "*   `config`: Basic configuration, which includes:\n",
    "    *   `system_instruction`: Sets the instructions for how the model should behave.\n",
    "    *   `contents`: The actual documents or other data you want to store in the cache.\n",
    "    *   `ttl`: The time-to-live of the cache (60 minutes in this case), after which the cache will expire.\n",
    "    *   `display_name`: A name for easy identification.\n",
    "\n",
    "The output of this cell is the unique identifier `cached_content.name` that is used to retrieve cached content later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96b7c7-652b-490c-96cb-a94ff9538ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cached_content = client.caches.create(\n",
    "    model=MODEL,\n",
    "    config=types.CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=contents,\n",
    "        ttl=\"3600s\",\n",
    "        display_name=\"example-cache\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27cd75-ada2-483a-9b18-2bbed9d57da5",
   "metadata": {},
   "source": [
    "Let's take a look at the created context cache!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af9180-0442-4efb-a5f4-9a3beccbf9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cache in client.caches.list():\n",
    "    print(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf99fa3-6123-4564-9f46-d9f8485e26eb",
   "metadata": {},
   "source": [
    "### Generate without cached context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeae236-de34-4459-939c-50733a9a3201",
   "metadata": {},
   "source": [
    "For comparison, let's first generate the answer **without** cached content and note the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8be69-cdbe-4003-abcf-ff3c6d5226da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL, contents=contents + [\"What are the papers about?\"]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86991481-6c6a-49c3-bb6f-3660c49f5f78",
   "metadata": {},
   "source": [
    "### Generate with cached context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6ffa7-3e79-43f4-a929-9faf0fec77d4",
   "metadata": {},
   "source": [
    "Now let's use the cached content to generate answers. The `cached_content` parameters refers to the created cached content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1603d54-50be-4d44-bf26-9eabb500e12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents=\"What are the papers about?\",\n",
    "    config=types.GenerateContentConfig(cached_content=cached_content.name),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35205f25-30d4-4015-86c0-47ea55816801",
   "metadata": {},
   "source": [
    "The output clearly demonstrates a substantial decrease in processing time. \n",
    "\n",
    "This performance gain amplifying as the volume of contextual information increases. By storing and reusing processed context, we achieve significant gains in efficiency, especially with larger contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7362ad8-8d9c-482a-bf56-482a7824ba22",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

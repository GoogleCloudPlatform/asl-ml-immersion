{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "mHF9VCProKJN",
    "tags": []
   },
   "source": [
    "# AI Explanations: Deploying an Explainable Image Model with Vertex AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "hZzRVxNtH-zG"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This lab shows how to train a classification model on image data and deploy it to Vertex AI to serve predictions with explanations (feature attributions). In this lab you will:\n",
    "* Explore the dataset\n",
    "* Build and train a custom image classification model with Vertex AI\n",
    "* Deploy the model to an endpoint\n",
    "* Serve predictions with explanations\n",
    "* Visualize feature attributions from Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "rgLXkyHEvTVD",
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyxoF-iqqD1t"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the libraries for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from google.cloud import aiplatform\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT  # defaults to PROJECT\n",
    "REGION = \"us-central1\"  # Replace with your REGION\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "GCS_PATTERN = \"gs://asl-public/data/flowers-xai/*.tfrec\"\n",
    "DATA_PATH = f\"gs://{BUCKET}/flowers/data\"\n",
    "OUTDIR = f\"gs://{BUCKET}/flowers/model_{TIMESTAMP}\"\n",
    "\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"DATA_PATH\"] = DATA_PATH\n",
    "os.environ[\"OUTDIR\"] = OUTDIR\n",
    "os.environ[\"TIMESTAMP\"] = TIMESTAMP\n",
    "print(f\"Project: {PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "editable": true,
    "id": "fsmCk2dwJnLZ"
   },
   "source": [
    "Run the following cell to create your Cloud Storage bucket if it does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "editable": true,
    "id": "160PRO3aJqLD"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo -e \"Bucket gs://${BUCKET} already exists.\"\n",
    "    \n",
    "else\n",
    "   echo \"Creating a new GCS bucket.\"\n",
    "   gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "   echo -e \"\\nHere are your current buckets:\"\n",
    "   gsutil ls\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "The dataset used for this tutorial is the [flowers dataset](https://www.tensorflow.org/datasets/catalog/tf_flowers) from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). This section shows how to shuffle, split, and copy the files to your GCS bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, split, and copy the dataset to your GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_PATH = DATA_PATH + \"/training\"\n",
    "EVAL_DATA_PATH = DATA_PATH + \"/validation\"\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Split data files between training and validation\n",
    "filenames = tf.io.gfile.glob(GCS_PATTERN)\n",
    "random.shuffle(filenames)\n",
    "split = int(len(filenames) * VALIDATION_SPLIT)\n",
    "training_filenames = filenames[split:]\n",
    "validation_filenames = filenames[:split]\n",
    "\n",
    "# Copy training files to GCS\n",
    "for file in training_filenames:\n",
    "    !gsutil -m cp $file $TRAINING_DATA_PATH/\n",
    "\n",
    "# Copy eval files to GCS\n",
    "for file in validation_filenames:\n",
    "    !gsutil -m cp $file $EVAL_DATA_PATH/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands. You should see a number of .tfrec files in your GCS bucket at both gs://{BUCKET}/flowers/data/training and gs://{BUCKET}/flowers/data/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -l $TRAINING_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -l $EVAL_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ingest functions and visualize some of the examples\n",
    "Define and execute helper functions to plot the images and corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [192, 192]\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Do not change, maps to the labels in the data\n",
    "CLASSES = [\n",
    "    \"daisy\",\n",
    "    \"dandelion\",\n",
    "    \"roses\",\n",
    "    \"sunflowers\",\n",
    "    \"tulips\",\n",
    "]\n",
    "\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature(\n",
    "            [], tf.string\n",
    "        ),  # tf.string means bytestring\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
    "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n",
    "    image = (\n",
    "        tf.cast(image, tf.float32) / 255.0\n",
    "    )  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    one_hot_class = tf.sparse.to_dense(example[\"one_hot_class\"])\n",
    "    one_hot_class = tf.reshape(one_hot_class, [5])\n",
    "    return image, one_hot_class\n",
    "\n",
    "\n",
    "# Load tfrecords into tf.data.Dataset\n",
    "def load_dataset(gcs_pattern):\n",
    "    filenames = tf.io.gfile.glob(gcs_pattern + \"/*\")\n",
    "    ds = tf.data.TFRecordDataset(filenames).map(read_tfrecord)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Converts N examples in dataset to numpy arrays\n",
    "def dataset_to_numpy(dataset, N):\n",
    "    numpy_images = []\n",
    "    numpy_labels = []\n",
    "\n",
    "    for images, labels in dataset.take(N):\n",
    "        numpy_images.append(images.numpy())\n",
    "        numpy_labels.append(labels.numpy())\n",
    "\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "\n",
    "def display_one_image(image, title, subplot):\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    plt.title(title, fontsize=16)\n",
    "    return subplot + 1\n",
    "\n",
    "\n",
    "def display_9_images_from_dataset(dataset):\n",
    "    subplot = 331\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    images, labels = dataset_to_numpy(dataset, 9)\n",
    "    for i, image in enumerate(images):\n",
    "        title = CLASSES[np.argmax(labels[i], axis=-1)]\n",
    "        subplot = display_one_image(image, title, subplot)\n",
    "        if i >= 8:\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Display 9 examples from the dataset\n",
    "ds = load_dataset(gcs_pattern=TRAINING_DATA_PATH)\n",
    "display_9_images_from_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build training pipeline\n",
    "In this section you will build an application with keras to train an image classification model on Vertex AI Custom Training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory for the training application and an __ init __.py file (this is required for a Python application but it can be empty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "mkdir -p flowers/trainer\n",
    "touch flowers/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "iN69d4D9Flrh"
   },
   "source": [
    "### Create training application in train.py\n",
    "\n",
    "This code contains the training logic. Here you build an application to ingest data from GCS and train an image classification model using [mobileNet](https://tfhub.dev/google/imagenet/mobilenet_v2_100_192/feature_vector/5) as a feature extractor, then sending it's output feature vector through a tf.keras.dense layer with 5 units and softmax activation (because there are 5 possible labels). Also, use the `fire` library which enables arguments to `train_and_evaluate` to be passed via the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile flowers/trainer/train.py\n",
    "import datetime\n",
    "import fire\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "IMAGE_SIZE = [192, 192]\n",
    "\n",
    "\n",
    "def read_tfrecord(example):\n",
    "\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature(\n",
    "            [], tf.string\n",
    "        ),  # tf.string means bytestring\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
    "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n",
    "    image = (\n",
    "        tf.cast(image, tf.float32) / 255.0\n",
    "    )  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(\n",
    "        image, [*IMAGE_SIZE, 3]\n",
    "    )\n",
    "    one_hot_class = tf.sparse.to_dense(example[\"one_hot_class\"])\n",
    "    one_hot_class = tf.reshape(one_hot_class, [5])\n",
    "    return image, one_hot_class\n",
    "\n",
    "\n",
    "def load_dataset(gcs_pattern, batch_size=32, training=True):\n",
    "    filenames = tf.io.gfile.glob(gcs_pattern)\n",
    "    ds = tf.data.TFRecordDataset(filenames).map(\n",
    "        read_tfrecord).batch(batch_size)\n",
    "    if training:\n",
    "        return ds.repeat()\n",
    "    else:\n",
    "        return ds\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    # MobileNet model for feature extraction\n",
    "    mobilenet_v2 = 'https://tfhub.dev/google/imagenet/'\\\n",
    "        'mobilenet_v2_100_192/feature_vector/5'\n",
    "    feature_extractor_layer = hub.KerasLayer(\n",
    "        mobilenet_v2,\n",
    "        input_shape=[*IMAGE_SIZE, 3],\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    # Instantiate model\n",
    "    model = tf.keras.Sequential([\n",
    "        feature_extractor_layer,\n",
    "        tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(train_data_path,\n",
    "                       eval_data_path,\n",
    "                       output_dir,\n",
    "                       batch_size,\n",
    "                       num_epochs,\n",
    "                       train_examples):\n",
    "\n",
    "    model = build_model()\n",
    "    train_ds = load_dataset(gcs_pattern=train_data_path,\n",
    "                            batch_size=batch_size)\n",
    "    eval_ds = load_dataset(gcs_pattern=eval_data_path,\n",
    "                           training=False)\n",
    "    num_batches = batch_size * num_epochs\n",
    "    steps_per_epoch = train_examples // num_batches\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=eval_ds,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "    )\n",
    "\n",
    "    model.save(output_dir)\n",
    "\n",
    "    print(\"Exported trained model to {}\".format(output_dir))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_and_evaluate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test training application locally \n",
    "It's always a good idea to test out a training application locally (with only a few training steps) to make sure the code runs as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR_LOCAL=local_test_training\n",
    "rm -rf ${OUTDIR_LOCAL}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/flowers\n",
    "python3 -m trainer.train \\\n",
    "    --train_data_path=gs://${BUCKET}/flowers/data/training/*.tfrec \\\n",
    "    --eval_data_path=gs://${BUCKET}/flowers/data/validation/*.tfrec \\\n",
    "    --output_dir=${OUTDIR_LOCAL} \\\n",
    "    --batch_size=1 \\\n",
    "    --num_epochs=1 \\\n",
    "    --train_examples=10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package code as source distribution\n",
    "Now that you have validated your model training code, we need to package our code as a source distribution in order to submit a custom training job to Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile flowers/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='flowers_trainer',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    install_requires=['fire==0.4.0', 'tensorflow-hub==0.12.0'],\n",
    "    description='Flowers image classifier training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd flowers\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the package in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp flowers/dist/flowers_trainer-0.1.tar.gz gs://${BUCKET}/flowers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit to the Cloud we use [`gcloud custom-jobs create`](https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create) and simply specify some additional parameters for the Vertex AI Training Service:\n",
    "- display-name: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- region: Cloud region to train in. See [here](https://cloud.google.com/vertex-ai/docs/general/locations) for supported Vertex AI Training Service regions\n",
    "\n",
    "You might have earlier seen `gcloud ai custom-jobs create` executed with the `worker pool spec` and pass-through Python arguments specified directly in the command call, here we will use a YAML file, this will make it easier to transition to hyperparameter tuning.\n",
    "\n",
    "Through the `args:` argument we add in the passed-through arguments for our `task.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "JOB_NAME=flowers_${TIMESTAMP}\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/flowers/flowers_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11.py310:latest\"\n",
    "PYTHON_MODULE=trainer.train\n",
    "\n",
    "echo > ./config.yaml \\\n",
    "\"workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-standard-8\n",
    "  replicaCount: 1\n",
    "  pythonPackageSpec:\n",
    "    executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "    packageUris: $PYTHON_PACKAGE_URI\n",
    "    pythonModule: $PYTHON_MODULE\n",
    "    args:\n",
    "    - --train_data_path=gs://${BUCKET}/flowers/data/training/*.tfrec\n",
    "    - --eval_data_path=gs://${BUCKET}/flowers/data/validation/*.tfrec\n",
    "    - --output_dir=$OUTDIR\n",
    "    - --num_epochs=15\n",
    "    - --train_examples=15000\n",
    "    - --batch_size=32\n",
    "    \"\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --config=config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** Model training will take 5 minutes or so. You have to wait for training to finish before moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "Su2qu-4CW-YH"
   },
   "source": [
    "### Serving function for image data\n",
    "\n",
    "To pass images to the prediction service, you encode the compressed (e.g., JPEG) image bytes into base 64 -- which makes the content safe from modification while transmitting binary data over the network. Since this deployed model expects input data as raw (uncompressed) bytes, you need to ensure that the base 64 encoded data gets converted back to raw bytes before it is passed as input to the deployed model.\n",
    "\n",
    "To resolve this, define a serving function (`serving_fn`) and attach it to the model as a preprocessing step. Add a `@tf.function` decorator so the serving function is fused to the underlying model (instead of upstream on a CPU).\n",
    "\n",
    "When you send a prediction or explanation request, the content of the request is base 64 decoded into a Tensorflow string (`tf.string`), which is passed to the serving function (`serving_fn`). The serving function preprocesses the `tf.string` into raw (uncompressed) numpy bytes (`preprocess_fn`) to match the input requirements of the model:\n",
    "- `io.decode_jpeg`- Decompresses the JPG image which is returned as a Tensorflow tensor with three channels (RGB).\n",
    "- `image.convert_image_dtype` - Changes integer pixel values to float 32.\n",
    "- `image.resize` - Resizes the image to match the input shape for the model.\n",
    "- `resized / 255.0` - Rescales (normalization) the pixel data between 0 and 1.\n",
    "\n",
    "At this point, the data can be passed to the model (`m_call`).\n",
    "\n",
    "#### XAI Signatures\n",
    "\n",
    "When the serving function is saved back with the underlying model (`tf.saved_model.save`), you specify the input layer of the serving function as the signature `serving_default`.\n",
    "\n",
    "For XAI image models, you need to save two additional signatures from the serving function:\n",
    "\n",
    "- `xai_preprocess`: The preprocessing function in the serving function.\n",
    "- `xai_model`: The concrete function for calling the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model into memory. **NOTE** This directory will not exist if your model has not finished training. Please wait for training to complete before moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = tf.keras.models.load_model(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCRETE_INPUT = \"numpy_inputs\"\n",
    "\n",
    "\n",
    "def _preprocess(bytes_input):\n",
    "    decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
    "    decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
    "    resized = tf.image.resize(decoded, size=(192, 192))\n",
    "    return resized\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def preprocess_fn(bytes_inputs):\n",
    "    decoded_images = tf.map_fn(\n",
    "        _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n",
    "    )\n",
    "    return {\n",
    "        CONCRETE_INPUT: decoded_images\n",
    "    }  # User needs to make sure the key matches model's input\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def serving_fn(bytes_inputs):\n",
    "    images = preprocess_fn(bytes_inputs)\n",
    "    prob = m_call(**images)\n",
    "    return prob\n",
    "\n",
    "\n",
    "# the function that sends data through the model itself and returns\n",
    "# the output probabilities\n",
    "m_call = tf.function(local_model.call).get_concrete_function(\n",
    "    [\n",
    "        tf.TensorSpec(\n",
    "            shape=[None, 192, 192, 3], dtype=tf.float32, name=CONCRETE_INPUT\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "tf.saved_model.save(\n",
    "    local_model,\n",
    "    OUTDIR,\n",
    "    signatures={\n",
    "        \"serving_default\": serving_fn,\n",
    "        # Required for XAI\n",
    "        \"xai_preprocess\": preprocess_fn,\n",
    "        \"xai_model\": m_call,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the serving function signature\n",
    "\n",
    "You can get the signatures of your model's input and output layers by reloading the model into memory, and querying it for the signatures corresponding to each layer.\n",
    "\n",
    "When making a prediction request, you need to route the request to the serving function instead of the model, so you need to know the input layer name of the serving function -- which you will use later when you make a prediction request.\n",
    "\n",
    "You also need to know the name of the serving function's input and output layer for constructing the explanation metadata -- which is discussed subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(OUTDIR)\n",
    "\n",
    "serving_input = list(\n",
    "    loaded.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",
    ")[0]\n",
    "print(\"Serving function input:\", serving_input)\n",
    "serving_output = list(\n",
    "    loaded.signatures[\"serving_default\"].structured_outputs.keys()\n",
    ")[0]\n",
    "print(\"Serving function output:\", serving_output)\n",
    "\n",
    "input_name = local_model.input.name\n",
    "print(\"Model input name:\", input_name)\n",
    "output_name = local_model.output.name\n",
    "print(\"Model output name:\", output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = aiplatform.explain.ExplanationParameters(\n",
    "    {\"integrated_gradients_attribution\": {\"step_count\": 50}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the model\n",
    "\n",
    "Next, upload your model to a `Model` resource using `Model.upload()` method, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Model` resource.\n",
    "- `artifact`: The Cloud Storage location of the trained model artifacts.\n",
    "- `serving_container_image_uri`: The serving container image.\n",
    "- `sync`: Whether to execute the upload asynchronously or synchronously.\n",
    "- `explanation_parameters`: Parameters to configure explaining for `Model`'s predictions.\n",
    "- `explanation_metadata`: Metadata describing the `Model`'s input and output for explanation.\n",
    "\n",
    "If the `upload()` method is run asynchronously, you can subsequently block until completion with the `wait()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"flower_classifier_v1\"\n",
    "INPUT_METADATA = {\"input_tensor_name\": CONCRETE_INPUT, \"modality\": \"image\"}\n",
    "OUTPUT_METADATA = {\"output_tensor_name\": serving_output}\n",
    "\n",
    "input_metadata = aiplatform.explain.ExplanationMetadata.InputMetadata(\n",
    "    INPUT_METADATA\n",
    ")\n",
    "output_metadata = aiplatform.explain.ExplanationMetadata.OutputMetadata(\n",
    "    OUTPUT_METADATA\n",
    ")\n",
    "\n",
    "metadata = aiplatform.explain.ExplanationMetadata(\n",
    "    inputs={\"image\": input_metadata}, outputs={\"class\": output_metadata}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** This can take a few minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT, staging_bucket=BUCKET)\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    artifact_uri=OUTDIR,\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\",\n",
    "    explanation_parameters=parameters,\n",
    "    explanation_metadata=metadata,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "Next, deploy your model for online prediction. To deploy the model, you invoke the `deploy` method, with the following parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
    "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "If only one model, then specify as { \"0\": 100 }, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "If there are existing models on the endpoint, for which the traffic will be split, then use model_id to specify as { \"0\": percent, model_id: percent, ... }, where model_id is the model id of an existing model to the deployed endpoint. The percents must add up to 100.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** This can take a few minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=MODEL_NAME,\n",
    "    traffic_split={\"0\": 100},\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the request content\n",
    "You are going to send the flower image as compressed JPG image, instead of the raw uncompressed bytes:\n",
    "\n",
    "- `mpimg.imsave`: Write the uncompressed image to disk as a compressed JPEG image.\n",
    "- `tf.io.read_file`: Read the compressed JPG images back into memory as raw bytes.\n",
    "- `base64.b64encode`: Encode the raw bytes into a base 64 encoded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = load_dataset(EVAL_DATA_PATH)\n",
    "x_test, y_test = dataset_to_numpy(eval_ds, 5)\n",
    "\n",
    "# Single image from eval dataset\n",
    "test_image = x_test[0]\n",
    "\n",
    "# Write image out as jpg\n",
    "mpimg.imsave(\"tmp.jpg\", test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the JPG image and encode it with base64 to send to the model endpoint. Send the encoded image to the endpoint with `endpoint.explain`. Then you can parse the response for the prediction and explanation. Full documentation on endpoint.explain can be found [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/explain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image and base64 encode\n",
    "bytes = tf.io.read_file(\"tmp.jpg\")\n",
    "b64str = base64.b64encode(bytes.numpy()).decode(\"utf-8\")\n",
    "\n",
    "instances_list = [{serving_input: {\"b64\": b64str}}]\n",
    "\n",
    "# Get prediction with explanation\n",
    "response = endpoint.explain(instances_list)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature attributions from Integrated Gradients. \n",
    "Query the response to get predictions and feature attributions. Use Matplotlib to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CLASSES = [\n",
    "    \"daisy\",\n",
    "    \"dandelion\",\n",
    "    \"roses\",\n",
    "    \"sunflowers\",\n",
    "    \"tulips\",\n",
    "]\n",
    "\n",
    "# Parse prediction\n",
    "for prediction in response.predictions:\n",
    "    label_index = np.argmax(prediction)\n",
    "    class_name = CLASSES[label_index]\n",
    "    confidence_score = prediction[label_index]\n",
    "    print(\n",
    "        \"Predicted class: \"\n",
    "        + class_name\n",
    "        + \"\\n\"\n",
    "        + \"Confidence score: \"\n",
    "        + str(confidence_score)\n",
    "    )\n",
    "\n",
    "    image = base64.b64decode(b64str)\n",
    "    image = BytesIO(image)\n",
    "    img = mpimg.imread(image, format=\"JPG\")\n",
    "\n",
    "# Parse explanation\n",
    "for explanation in response.explanations:\n",
    "    attributions = dict(explanation.attributions[0].feature_attributions)\n",
    "    xai_label_index = explanation.attributions[0].output_index[0]\n",
    "    xai_class_name = CLASSES[xai_label_index]\n",
    "    xai_b64str = attributions[\"image\"][\"b64_jpeg\"]\n",
    "    xai_image = base64.b64decode(xai_b64str)\n",
    "    xai_image = io.BytesIO(xai_image)\n",
    "    xai_img = mpimg.imread(xai_image, format=\"JPG\")\n",
    "\n",
    "# Plot image, feature attribution mask, and overlayed image\n",
    "fig = plt.figure(figsize=(13, 18))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.title(\"Input Image\")\n",
    "plt.imshow(img)\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.title(\"Feature Attribution Mask\")\n",
    "plt.imshow(xai_img)\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.title(\"Overlayed Attribution Mask\")\n",
    "plt.imshow(img)\n",
    "plt.imshow(xai_img, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "To learn more about AI Explanations, check out the resources here.\n",
    "\n",
    "* [Vertex AI Explanations documentation](https://cloud.google.com/vertex-ai/docs/explainable-ai)\n",
    "* [Integrated gradients paper](https://arxiv.org/abs/1703.01365)\n",
    "* [XRAI paper](https://arxiv.org/abs/1906.02825)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ai-explanations-image.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

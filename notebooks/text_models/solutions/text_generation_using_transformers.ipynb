{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFoVN0Fd48kp"
   },
   "source": [
    "# Generating Text Using a Transformer Decoder-Only Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNbexFYv48kt"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this example, we will use KerasNLP to build a scaled down generative model using a Transformer Decoder. A generative model allows you to generate sophisticated text from a prompt. In this lab, we will be building a model that only uses the decoder from the Transformer stack. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models. Some examples of decoder-only models are [Transformer XL](https://arxiv.org/pdf/1901.02860.pdf), [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [CTRL](https://arxiv.org/pdf/1909.05858.pdf), etc.\n",
    "\n",
    "We will train the model on the [simplebooks-92](https://arxiv.org/abs/1911.12391) corpus,which is a dataset made from several novels. This dataset was created from 1,573 Gutenberg books. It is a good dataset for this example since it has a small vocabulary and high word frequency, which is beneficial when training a generative model with few parameters.\n",
    "\n",
    "This notebook demonstrates how to use KerasNLP tokenization, layers and metrics to simplify the training process, and then show how to generate output text using the KerasNLP sampling utilities.\n",
    "\n",
    "### Learning Objectives\n",
    "- Learn how to train a prepare a dataset for generative models using wordpiece tokenizer\n",
    "- Learn how to use Keras NLP to build a generative model\n",
    "- Learn different inference techniques to output text from a prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbtWRiXS48kt"
   },
   "source": [
    "## Setup\n",
    "\n",
    "In order to run this notebook, you will need `keras_nlp`. KerasNLP is a natural language processing library that works natively with TensorFlow, JAX, or PyTorch. Keras NLP offers transformer layers that are extremely helpful to build the generative model in this notebook.\n",
    "\n",
    "Uncomment the cell below if you don't have keras_nlp already installed. You may need to restart the kernel once it has been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QxR27aY5Ghe",
    "outputId": "a31a6553-979c-47ec-aadc-4d829f99d80b"
   },
   "outputs": [],
   "source": [
    "#!pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPz25USa48ku",
    "outputId": "aa8bfff2-3da8-45da-eed5-70034e1172d8"
   },
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you start\n",
    "Please ensure you have a GPU (1 x NVIDIA Tesla T4 should be enough) attached to your notebook instance to ensure that the training doesn't take too long.\n",
    "\n",
    "To check if you have a GPU attached you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should output \"Num GPUs Available: 1\" if you have one GPU attached\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqWg-fxm48kv"
   },
   "source": [
    "## Settings & Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to initialize some hyperparameters here. `SEQ_LEN` here, defines that maximum length of a sentence in our dataset, we will use this as the length in our `WordPieceTokenizer`. We are also define, `MIN_TRAINING_SEQ_LEN` to clean up the dataset with any sentence that is too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Wwy7GtU48kv"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 128\n",
    "MIN_TRAINING_SEQ_LEN = 450\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 256\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 5000  # Limits parameters in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYcb6lTq48kw"
   },
   "source": [
    "## Load the Data\n",
    "\n",
    "We will be using the SimpleBooks dataset for this notebook. The SimpleBooks dataset consists of 1,573 Gutenberg books and a small vocabulary size to word-level tokens ratio. It has a vocabulary size of ~98k. This size makes it easier to fit a small transformer model. In this block we will be using [TensorFlow's data API](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2EWTmjh48kw",
    "outputId": "4a6bcd0b-6c00-42b3-da90-f8f22b799381"
   },
   "outputs": [],
   "source": [
    "keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/asl-public/text/data/simplebooks.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "data_dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
    "\n",
    "# Load simplebooks-92 train set and filter out short lines using MIN_TRAINING_SEQ_LEN\n",
    "raw_train_ds = (\n",
    "    tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/train.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(buffer_size=256)\n",
    ")\n",
    "\n",
    "# Load simplebooks-92 validation set and filter out short lines using MIN_TRAINING_SEQ_LEN\n",
    "raw_val_ds = (\n",
    "    tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/valid.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN8zvDM_48kx"
   },
   "source": [
    "## Train the Tokenizer\n",
    "\n",
    "We train the tokenizer using Keras NLP's [compute_word_piece_vocabulary](https://keras.io/api/keras_nlp/tokenizers/compute_word_piece_vocabulary/) from the training dataset for a vocabulary size of `VOCAB_SIZE`, which is a tuned hyperparameter. We want to limit the vocabulary as much as possible, since it has a large effect on the number of model parameters. We also don't want to include *too few* words, or there would be too many out-of-vocabulary (OOV) sub-words. In addition, three tokens are reserved in the vocabulary:\n",
    "\n",
    "- `\"[PAD]\"` for padding sequences to `SEQ_LEN`. This token has index 0 in both `reserved_tokens` and `vocab`, since `WordPieceTokenizer` (and other layers) consider `0`/`vocab[0]` as the default padding.\n",
    "- `\"[UNK]\"` for OOV sub-words, which should match the default `oov_token=\"[UNK]\"` in\n",
    "`WordPieceTokenizer`.\n",
    "- `\"[BOS]\"` stands for beginning of sentence, but here technically it is a token representing the beginning of each line of training data.\n",
    "\n",
    "This cell takes ~5-10 mins to execute because it is computing the word piece vocabulary on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cF4Unid048kx"
   },
   "outputs": [],
   "source": [
    "# Train tokenizer vocabulary\n",
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    raw_train_ds,\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    lowercase=True,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhlKbkvt48kx"
   },
   "source": [
    "## Load Tokenizer\n",
    "\n",
    "We use the vocabulary data to initialize [keras_nlp.tokenizers.WordPieceTokenizer](https://keras.io/api/keras_nlp/tokenizers/word_piece_tokenizer/). WordPieceTokenizer is an efficient implementation of the WordPiece algorithm used by BERT and other models. It will strip, lower-case and do other irreversible preprocessing operations. Given a vocabulary and input sentence, the WordPiece tokenizer will convert the sentence into an array of IDs and pad the sentence to the `SEQ_LEN` defined. For example, \n",
    "\n",
    "```\n",
    "vocab = [\"[UNK]\", \"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\", \".\"]\n",
    "inputs = \"The quick brown fox.\"\n",
    "SEQ_LEN = 10\n",
    "```\n",
    "\n",
    "When passed to the `WordPieceTokenizer` will return\n",
    "```\n",
    "array([1, 2, 3, 4, 5, 6, 7,0,0,0], dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTz3xEmh48kx"
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INSvSBnB48ky"
   },
   "source": [
    "## Tokenize Data\n",
    "\n",
    "We preprocess the dataset by tokenizing and splitting it into `features` and `labels`. Since this is a language modeling task. The goal will be to predict a \"label sequence\" of \"next words\" from a \"features sequence\" of \"previous words\". In order to obtain the \"feature\" we shift the original sentence to the right using the `[BOS]` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U77FMjaY48ky"
   },
   "outputs": [],
   "source": [
    "# packer adds a start token\n",
    "start_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs):\n",
    "    outputs = tokenizer(inputs)\n",
    "    features = start_packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Tokenize and split into train and label sequences.\n",
    "train_ds = raw_train_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = raw_val_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0XiTjui48ky"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "We create our scaled-down transformer-decoder-based generative text model model with the following layers:\n",
    "\n",
    "- One `keras_nlp.layers.TokenAndPositionEmbedding` layer, which combines the embedding for the token and its position. This is diffrent from a traditional embedding layer because it creates trainable positional embedding instead of the fixed sinusoidal embedding.\n",
    "- Multiple `keras_nlp.layers.TransformerDecoder` layers created using a loop. \n",
    "- One final dense linear layer.\n",
    "\n",
    "**Note:** You can take a look at the [source code](https://github.com/keras-team/keras-nlp/blob/v0.6.1/keras_nlp/layers/modeling/transformer_decoder.py#L31) of this layer to see the different components that go into this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKHCA6ej48ky"
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "# Embedding layer\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "# Transformer decoder layers\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "    )\n",
    "    x = decoder_layer(x)  # Giving one argument only skips cross-attention\n",
    "# Output layer\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# set up the loss metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwtZicZo48ky"
   },
   "source": [
    "Let's take a look at our model summary - a large majority of the\n",
    "parameters are in the `token_and_position_embedding` and the output `dense` layer!\n",
    "This means that the vocabulary size (`VOCAB_SIZE`) has a large effect on the size of the model,\n",
    "while the number of Transformer decoder layers (`NUM_LAYERS`) doesn't affect it as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJg1dbyZ48ky"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2F9s5tyz48kz"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our model, let's train it with the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StJs0vL048kz"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1  # increase the number of epochs for better results\n",
    "model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reWRkSNf48kz"
   },
   "source": [
    "## Inference\n",
    "\n",
    "With our trained model, we can test it out to gauge its performance. To do this we can seed our model with an input sequence starting with the `\"[BOS]\"` token, and progressively sample the model by making predictions for each subsequent token in a loop.\n",
    "\n",
    "To start let us build a prompt with the same shape as our model inputs, containing only the `\"[BOS]\"` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NL15KPUZ48kz"
   },
   "outputs": [],
   "source": [
    "# The \"packer\" layers adds the [BOS] token for us.\n",
    "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWu0fC2H48kz"
   },
   "source": [
    "We will use the `keras_nlp.samplers` module for inference, which requires a callback function wrapping the model we just trained. This wrapper calls the model and returns the logit predictions for the current token we are generating.\n",
    "\n",
    "**Note:** There are two pieces of more advanced functionality available when defining your callback. The first is the ability to take in a `cache` of states computed in previous generation steps, which can be used to speed up generation. The second is the ability to output the final dense \"hidden state\" of each generated token. This is used by `keras_nlp.samplers.ContrastiveSampler`, which avoids repetition by penalizing repeated hidden states. Both are optional, and we will ignore them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPLJM88348kz"
   },
   "outputs": [],
   "source": [
    "def next(prompt, cache, index):\n",
    "    logits = model(prompt)[:, index - 1, :]\n",
    "    # Ignore hidden states for now; only needed for contrastive search.\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JZHgird48k0"
   },
   "source": [
    "Creating the wrapper function is the most complex part of using these functions. Now that\n",
    "it's done, let's test out the different utilities, starting with greedy search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMUdU93R48k0"
   },
   "source": [
    "### Greedy search\n",
    "\n",
    "We greedily pick the most probable token at each timestep. In other words, we get the\n",
    "argmax of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kY-WPGTL48k0"
   },
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.GreedySampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,  # Start sampling immediately after the [BOS] token.\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak1-pDMr48k0"
   },
   "source": [
    "As you can see, greedy search starts out making some sense, but quickly starts repeating itself. This is a common problem with text generation that can be fixed by some of the probabilistic text generation utilities shown later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrLyz92l48k0"
   },
   "source": [
    "### Beam search\n",
    "\n",
    "At a high-level, beam search keeps track of the `num_beams` most probable sequences at\n",
    "each timestep, and predicts the best next token from all sequences. It is an improvement\n",
    "over greedy search since it stores more possibilities. However, it is less efficient than\n",
    "greedy search since it has to compute and store multiple potential sequences.\n",
    "\n",
    "**Note:** beam search with `num_beams=1` is identical to greedy search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oI7AQ9-h48k0"
   },
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Beam search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-azZoy0b48k0"
   },
   "source": [
    "Similar to greedy search, beam search quickly starts repeating itself, since it is still\n",
    "a deterministic method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYA14uMc48k0"
   },
   "source": [
    "### Random search\n",
    "\n",
    "Random search is our first probabilistic method. At each time step, it samples the next\n",
    "token using the softmax probabilities provided by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDpsTuRm48k0"
   },
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.RandomSampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Random search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWSg0ON648k1"
   },
   "source": [
    "Voil√†, no repetitions! However, with random search, we may see some nonsensical words\n",
    "appearing since any word in the vocabulary has a chance of appearing with this sampling\n",
    "method. This is fixed by our next search utility, top-k search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHxKUOWn48k1"
   },
   "source": [
    "### Top-K search\n",
    "\n",
    "Similar to random search, we sample the next token from the probability distribution\n",
    "provided by the model. The only difference is that here, we select out the top `k` most\n",
    "probable tokens, and distribute the probability mass over them before sampling. This way,\n",
    "we won't be sampling from low probability tokens, and hence we would have less\n",
    "nonsensical words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hna2w_e448k1"
   },
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHesxfg748k1"
   },
   "source": [
    "### Top-P search\n",
    "\n",
    "Even with the top-k search, there is something to improve upon. With top-k search, the\n",
    "number `k` is fixed, which means it selects the same number of tokens for any probability\n",
    "distribution. Consider two scenarios, one where the probability mass is concentrated over\n",
    "2 words and another where the probability mass is evenly concentrated across 10. Should\n",
    "we choose `k=2` or `k=10`? There is no one size that fits all `k` here.\n",
    "\n",
    "This is where top-p search comes in! Instead of choosing a `k`, we choose a probability\n",
    "`p` that we want the probabilities of the top tokens to sum up to. This way, we can\n",
    "dynamically adjust the `k` based on the probability distribution. By setting `p=0.9`, if\n",
    "90% of the probability mass is concentrated on the top 2 tokens, we can filter out the\n",
    "top 2 tokens to sample from. If instead the 90% is distributed over 10 tokens, it will\n",
    "similarly filter out the top 10 tokens to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gwfY-YB48k1"
   },
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-P search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnss68kt48k1"
   },
   "source": [
    "### Using callbacks for text generation\n",
    "\n",
    "We can also wrap the utilities in a callback, which allows you to print out a prediction sequence for every epoch of the model. This is extremely useful to see if the model is improving after each epoch. Here is an example of a callback for top-k search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMw0jBN148k6"
   },
   "outputs": [],
   "source": [
    "class TopKTextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.sampler = keras_nlp.samplers.TopKSampler(k)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        output_tokens = self.sampler(\n",
    "            next=next,\n",
    "            prompt=prompt_tokens,\n",
    "            index=1,\n",
    "        )\n",
    "        txt = tokenizer.detokenize(output_tokens)\n",
    "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
    "\n",
    "\n",
    "text_generation_callback = TopKTextGenerator(k=10)\n",
    "# Dummy training loop to demonstrate callback.\n",
    "model.fit(\n",
    "    train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVdJAlz348k6"
   },
   "source": [
    "## Acknowledgment\n",
    "This notebook is based on a [Keras tutorial by Jesse Chan](https://keras.io/examples/generative/text_generation_gpt/#train-the-tokenizer). The transformer decoder layer is based on the the research paper by Google, [Attention Is All You Need, Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "text_generation_gpt",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

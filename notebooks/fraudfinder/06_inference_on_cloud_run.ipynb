{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d1c73d-9e36-45ec-9a6f-f49c23f90241",
   "metadata": {
    "id": "JAPoU8Sm5E6e",
    "tags": []
   },
   "source": [
    "# Fraudfinder - Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ffa73-2e9f-44a9-b88e-3d5eb156bcc6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This series of labs are updated upon [FraudFinder](https://github.com/googlecloudplatform/fraudfinder) repository which builds a end-to-end real-time fraud detection system on Google Cloud. Throughout the FraudFinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model.\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this notebook, we'll focus on deploying our trained fraud detection model as a real-time inference service. We'll be using a serverless architecture, which allows us to build and run applications without having to manage the underlying infrastructure. This means we can focus on our code and let Google Cloud handle the scaling, availability, and maintenance.\n",
    "\n",
    "Our architecture will be event-driven, leveraging Pub/Sub to trigger our inference service in response to new transaction events. This is a common pattern for real-time systems, as it allows for loose coupling between components and can handle high volumes of data.\n",
    "\n",
    "To accomplish this, we will create a Cloud Run app to perform model inference on the endpoint deployed in the previous notebooks. This Cloud Run app will be triggered by a Pub/Sub subscriber for live transactions, perform a look-up on feature values from the Vertex AI Feature Store, and send the prediction request to the Vertex AI endpoint. You can then view the resulting prediction-response logs in BigQuery.\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/)**: For hosting our trained model on an endpoint and for using the Feature Store to retrieve feature values for inference.\n",
    "- **[BigQuery](https://cloud.google.com/bigquery/)**: For storing and analyzing the prediction logs from our model.\n",
    "- **[Cloud Run](https://cloud.google.com/run)**: A serverless compute platform that will host our inference application. It automatically scales up and down, and you only pay for the resources you use.\n",
    "- **[Pub/Sub](https://cloud.google.com/pubsub/)**: A real-time messaging service that will be used to trigger our Cloud Run application whenever a new transaction occurs.\n",
    "\n",
    "The steps we will take in this notebook are:\n",
    "\n",
    "1.  **Build and deploy a Cloud Run app for model inference**: We'll containerize our inference code using Docker and deploy it to Cloud Run.\n",
    "2.  **Create and use a Pub/Sub push subscription to invoke the Cloud Run model inference app**: We'll set up a trigger so that our Cloud Run service is called for each new transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e3e1e-69d0-451f-a092-960fc88d22c9",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Load config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f883f-eebc-4607-8009-6cc34a2d7c1e",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfceee97-a7b9-4323-ba10-6171a02362c2",
   "metadata": {},
   "source": [
    "###Â Define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa00a6-7682-44a1-87ad-0613b1fd984b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4b059-0018-4466-8bc0-8482e70ee21c",
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb54a14-d9f0-4cb2-814c-4662739d8b9d",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2729ca-f3b0-43e1-b30c-e9daa26a592f",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030e8ec-6b29-4477-a92c-122775ed9925",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build and deploy a Cloud Run app for model inference\n",
    "\n",
    "To formalize the process of prediction, you will use a Cloud Run app that takes in live transactions as a trigger, then fetches feature values from Vertex AI Feature Store, then sends the prediction payload to an endpoint. To clarify, to invoke the Cloud Run app, you will create a Pub/Sub push subscription that reads live transactions from the public Pub/Sub topic to invoke the Cloud Run app.\n",
    "\n",
    "[Cloud Run](https://cloud.google.com/run) is a serverless compute platform that enables you to deploy containers that can be executed every time it is triggered. \n",
    "\n",
    "### The Cloud Run Application\n",
    "\n",
    "Before we build and deploy our application, let's take a closer look at the code that will be running on Cloud Run. The application is a simple Flask web server that receives Pub/Sub messages, processes them, and sends a prediction request to our Vertex AI model.\n",
    "\n",
    "The application code is located in the `notebooks/fraudfinder/scripts/cloud_run_model_inference/` directory and consists of three main files:\n",
    "\n",
    "*   [`main.py`](./scripts/cloud_run_model_inference/main.py): This is the main application file. It contains a Flask web server with a single endpoint that listens for POST requests. When a request is received, the application:\n",
    "    1.  Parses the incoming Pub/Sub message to extract the transaction data.\n",
    "    2.  Retrieves additional features from the Vertex AI Feature Store.\n",
    "    3.  Constructs a prediction request with the combined features.\n",
    "    4.  Sends the request to the Vertex AI Endpoint.\n",
    "    5.  Logs the prediction result.\n",
    "*   `Dockerfile`: This file defines the container image for our application. It specifies the base Python image, copies the application code, installs the required dependencies from `requirements.txt`, and defines the command to start the Flask web server using `gunicorn`.\n",
    "*   `requirements.txt`: This file lists the Python dependencies that our application needs to run, including `Flask`, `gunicorn`, and the `google-cloud-aiplatform` library.\n",
    "\n",
    "By containerizing our application, we can ensure that it runs consistently across different environments and can be easily deployed to Cloud Run.\n",
    "\n",
    "#### Steps to build and deploy the Cloud Run app\n",
    "\n",
    "To deploy a Cloud Run app, you must:\n",
    "1. Build a Docker container with your code\n",
    "2. Deploy your container to Cloud Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e547a9bc-7fd9-4d05-8bd1-96034e79eced",
   "metadata": {},
   "source": [
    "### 1. Build a Docker container with your code\n",
    "\n",
    "The container code has been prepared for you in the `cloud_run_model_inference/` folder. The first step is to build our Docker container and push it to Google Container Registry (GCR). GCR is a private Docker registry where we can store our container images.\n",
    "\n",
    "We'll use the `gcloud builds submit` command to do this. This command will:\n",
    "\n",
    "1.  Compress the application code in the `scripts/cloud_run_model_inference` directory.\n",
    "2.  Upload the code to a Cloud Storage bucket.\n",
    "3.  Initiate a build using Cloud Build, which will execute the instructions in our `Dockerfile`.\n",
    "4.  Tag the resulting image with the name `gcr.io/$PROJECT_ID/cloud_run_model_inference`.\n",
    "5.  Push the tagged image to Google Container Registry.\n",
    "\n",
    "This entire process is automated, making it easy to build and publish our container images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a955b72-17eb-4ee4-a2a5-ca6cdbe8df26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2022 Google, LLC.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "# [START cloudrun_pubsub_dockerfile]\n",
      "# [START run_pubsub_dockerfile]\n",
      "\n",
      "# Use the official Python image.\n",
      "# https://hub.docker.com/_/python\n",
      "FROM python:3.10\n",
      "\n",
      "# Allow statements and log messages to immediately appear in the Cloud Run logs\n",
      "ENV PYTHONUNBUFFERED True\n",
      "\n",
      "# Copy application dependency manifests to the container image.\n",
      "# Copying this separately prevents re-running pip install on every code change.\n",
      "COPY requirements.txt ./\n",
      "\n",
      "# Install production dependencies.\n",
      "RUN pip install -r requirements.txt\n",
      "\n",
      "# Copy local code to the container image.\n",
      "ENV APP_HOME /app\n",
      "WORKDIR $APP_HOME\n",
      "COPY . ./\n",
      "\n",
      "# Run the web service on container startup.\n",
      "# Use gunicorn webserver with one worker process and 8 threads.\n",
      "# For environments with multiple CPU cores, increase the number of workers\n",
      "# to be equal to the cores available.\n",
      "# Timeout is set to 0 to disable the timeouts of the workers to allow Cloud Run to handle instance scaling.\n",
      "CMD exec gunicorn --bind :$PORT --workers 1 --threads 8 --timeout 0 main:app\n",
      "\n",
      "# [END run_pubsub_dockerfile]\n",
      "# [END cloudrun_pubsub_dockerfile]\n"
     ]
    }
   ],
   "source": [
    "!cat ./scripts/cloud_run_model_inference/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6213f-195e-4e27-bf6d-6db48d7064ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit ./scripts/cloud_run_model_inference --tag gcr.io/$PROJECT_ID/cloud_run_model_inference --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25b5bd-9463-4544-b072-757c0acad108",
   "metadata": {},
   "source": [
    "### 2. Deploy your container to Cloud Run\n",
    "\n",
    "With your container built on Container Registry, you can now deploy it to Cloud Run. To do so, you will need some environment variables to make sure your Cloud Run app knows which Vertex AI endpoint to use.\n",
    "\n",
    "First, let's retrieve the endpoint ID of our deployed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa8e82-0b39-4f40-a059-ef8d056a13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your Vertex AI endpoint name\n",
    "endpoints = vertex_ai.Endpoint.list(\n",
    "    filter=f\"display_name={ENDPOINT_NAME}\",  # optional: filter by specific endpoint name\n",
    "    order_by=\"update_time\",\n",
    ")\n",
    "\n",
    "ENDPOINT_ID = endpoints[-1].name\n",
    "print(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7dc061-1298-4055-a33f-c07dd525977d",
   "metadata": {},
   "source": [
    "Now we can deploy our container to Cloud Run using the `gcloud run deploy` command. This command will:\n",
    "\n",
    "1.  Create a new Cloud Run service named `cloud-run-model-inference-app`.\n",
    "2.  Use the container image we just built and pushed to GCR.\n",
    "3.  Set the `--no-allow-unauthenticated` flag to ensure that our service can only be invoked by authenticated requests.\n",
    "4.  Set the `--region` flag to the same region as our other resources.\n",
    "5.  Use the `--update-env-vars` flag to set the necessary environment variables that our application needs to connect to the Vertex AI Feature Store and Endpoint.\n",
    "\n",
    "Once deployed, you can check the status of your service on the [Cloud Run console](https://console.cloud.google.com/run).\n",
    "\n",
    "Note that if you try to visit the Service URL (which may look like http://cloud-run-model-inference-app-XXXXXX-a.run.app), you should expect to see `Error: Forbidden\n",
    "Your client does not have permission to get URL / from this server`, which is normal, as you don't want the public internet to invoke your app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81ca0c-b11e-46e1-8b5e-2243732ab9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud run deploy cloud-run-model-inference-app \\\n",
    "--image gcr.io/{PROJECT_ID}/cloud_run_model_inference \\\n",
    "--no-allow-unauthenticated \\\n",
    "--region $REGION \\\n",
    "--update-env-vars FEATURESTORE_ID=$FEATURESTORE_ID,ENDPOINT_ID=$ENDPOINT_ID,PROJECT_ID=$PROJECT_ID,REGION=$REGION \\\n",
    "--quiet --verbosity=none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f682ea-003d-42c6-a8e9-3fbfb80eb3e8",
   "metadata": {},
   "source": [
    "You have now deployed a Cloud Run app to do model inference. However, it is not currently triggered by anything. In the next section, you will connect your Cloud Run app to the live transactions so you can continuously trigger your model inference app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154b978-c5f9-4503-963c-17df78ed9d2f",
   "metadata": {},
   "source": [
    "## Create and use a Pub/Sub push subscription to invoke the Cloud Run model inference app\n",
    "\n",
    "Now that our Cloud Run service is deployed, we need a way to trigger it whenever a new transaction occurs. For this, we'll use a Pub/Sub push subscription. This will create a subscription to the `ff-tx` topic, and for each message that is published to the topic, Pub/Sub will send a POST request to our Cloud Run service's endpoint.\n",
    "\n",
    "However, since we've configured our Cloud Run service to not allow unauthenticated requests, we need to provide a way for Pub/Sub to securely invoke our service. We'll do this by creating a dedicated service account and granting it the necessary permissions to invoke our Cloud Run service. This is a much more secure approach than allowing unauthenticated access, as it ensures that only Pub/Sub can trigger our service.\n",
    "\n",
    "#### There are a few steps needed:\n",
    "1. Create a service account that can invoke your Cloud Run app with appropriate IAM policies\n",
    "2. Create the Pub/Sub subscription from the live transactions to invoke the Cloud Run app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b5985-1ad6-44bf-b080-ad883fb038b0",
   "metadata": {},
   "source": [
    "### 1. Create a service account that can invoke your Cloud Run app with appropriate IAM policies\n",
    "\n",
    "First, we'll create a new service account named `cloud-run-invoker`. This service account will be used by our Pub/Sub subscription to authenticate with our Cloud Run service. We'll then grant this service account the `run.invoker` role, which gives it permission to invoke the Cloud Run service.\n",
    "\n",
    "Finally, we'll grant the Google-managed Pub/Sub service account the `iam.serviceAccountTokenCreator` role. This is a crucial step that allows the Pub/Sub service to create authentication tokens on behalf of our `cloud-run-invoker` service account. When Pub/Sub sends a message to our Cloud Run service, it will include one of these tokens in the request header, which Cloud Run will then use to verify the identity of the caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968aa44-0ab4-43bb-96ef-5c9a62767d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a service account\n",
    "!gcloud iam service-accounts create cloud-run-invoker --display-name \"Cloud Run Pub/Sub Invoker\"\n",
    "\n",
    "# Retrieve your project number\n",
    "PROJECT_NUMBER = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
    "\n",
    "# Bind the service account with an IAM policy to invoke the Cloud Run app\n",
    "!gcloud run services add-iam-policy-binding cloud-run-model-inference-app \\\n",
    "   --member=serviceAccount:cloud-run-invoker@{PROJECT_ID}.iam.gserviceaccount.com \\\n",
    "   --role=roles/run.invoker \\\n",
    "   --region=us-central1\n",
    "\n",
    "# Add another IAM policy to the service account to provide authentication needed to invoke Cloud Run\n",
    "!gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "     --member=serviceAccount:service-{PROJECT_NUMBER}@gcp-sa-pubsub.iam.gserviceaccount.com \\\n",
    "     --role=roles/iam.serviceAccountTokenCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299c429-60be-45b4-937a-25b048b2ec73",
   "metadata": {},
   "source": [
    "### 2. Create the Pub/Sub subscription from the live transactions to invoke the Cloud Run app\n",
    "\n",
    "With the service account created and configured, we can now create the Pub/Sub push subscription. This subscription will connect the live transaction topic (`ff-tx`) to our Cloud Run service. \n",
    "\n",
    "When we create the subscription, we'll specify:\n",
    "\n",
    "*   The name of the topic to subscribe to.\n",
    "*   The push endpoint, which is the URL of our Cloud Run service.\n",
    "*   The service account to use for authentication.\n",
    "\n",
    "This tells Pub/Sub to send an HTTP POST request to our service's URL for every message that arrives on the topic, using the specified service account to authenticate the request. This setup ensures that our service is securely and reliably triggered for each new transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad286fd5-2a11-4cd2-8b53-ca7ad01d9fc4",
   "metadata": {},
   "source": [
    "To create the Pub/Sub push subscription, you will first need to retrieve your Cloud Run service URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18d705-b62d-4ba5-8569-0ab10726d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the service URL programmatically\n",
    "SERVICE_URL = !gcloud run services describe cloud-run-model-inference-app \\\n",
    "  --platform managed \\\n",
    "  --region $REGION \\\n",
    "  --format \"value(status.url)\"\n",
    "SERVICE_URL = SERVICE_URL[0]\n",
    "\n",
    "print(SERVICE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6839d5-6f11-4de5-be2c-0a426fdd2387",
   "metadata": {},
   "source": [
    "Now you can create your Pub/Sub push subscription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2e721-cadc-44dc-93f4-149027e8a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub subscriptions create push-live-tx-to-cloudrun --topic projects/cymbal-fraudfinder/topics/ff-tx \\\n",
    "   --ack-deadline=600 \\\n",
    "   --push-endpoint=$SERVICE_URL \\\n",
    "   --push-auth-service-account=cloud-run-invoker@{PROJECT_ID}.iam.gserviceaccount.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df358d43-83f4-4155-8002-23636ba625cb",
   "metadata": {},
   "source": [
    "Once created, you can do some checks to make sure everything worked successfully:\n",
    "- On the [Pub/Sub page](https://console.cloud.google.com/cloudpubsub/subscription/list), inspect your new Pub/Sub subscription `push-live-tx-to-cloudrun`\n",
    "- On the [Cloud Run logs page](https://console.cloud.google.com/run/detail/us-central1/cloud-run-model-inference-app/logs), check the logs of your Cloud Run app to confirm that you see model prediction requests and responses"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

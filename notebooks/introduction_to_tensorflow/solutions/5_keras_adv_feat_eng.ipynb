{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering in Keras \n",
    "\n",
    "**Learning Objectives**\n",
    "1. Define and adopt stateful preprocessing layers.\n",
    "2. Apply custom transformations with `Lambda` layers.\n",
    "\n",
    "## Overview\n",
    "In this notebook, we use Keras to build a taxifare price prediction model and utilize feature engineering to improve the fare amount prediction for NYC taxi cab rides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Start by importing the necessary libraries for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import (\n",
    "    CategoryEncoding,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Discretization,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    HashedCrossing,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data \n",
    "\n",
    "We will use the taxifare dataset, using the CSV files that we created in the first notebook of this sequence. Those files have been saved into `../data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../data/taxi-*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.data to read the CSV files\n",
    "\n",
    "We wrote these functions for reading data from the CSV files above in the [previous notebook](2_dataset_api.ipynb).\n",
    "\n",
    "The `tf.data` API efficiently loads and preprocesses data. \n",
    "- `parse_csv`: Parses a CSV row into features and a label. Features are returned as a tuple for Functional API compatibility with multiple inputs.\n",
    "- `create_dataset`: Builds a `tf.data.Dataset` from CSV files, including mapping `parse_csv`, repeating, shuffling (for training), and batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(row):\n",
    "    ds = tf.strings.split(row, \",\")\n",
    "    # Label: fare_amount\n",
    "    label = tf.strings.to_number(ds[0])\n",
    "    # Feature: pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "    feature = tf.strings.to_number(ds[2:6])  # use some features only\n",
    "    # Passing feature in tuple so that we can handle them separately.\n",
    "    return (feature[0], feature[1], feature[2], feature[3]), label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size, mode=\"eval\"):\n",
    "    ds = tf.data.TextLineDataset(pattern)\n",
    "    ds = ds.map(parse_csv).repeat()\n",
    "    if mode == \"train\":\n",
    "        ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Advanced Feature Engineering\n",
    "Next, we'll try to improve performance by adding more feature engineering:\n",
    "1.  **Normalization:** Applied to coordinates before distance calculation and other processing.\n",
    "2.  **Euclidean Distance:** Calculated using a `Lambda` layer.\n",
    "\n",
    "\n",
    "### Setup Feature Normalization with `Normalization` Layer\n",
    "\n",
    "The `keras.layers.Normalization` layer standardizes features by scaling them to have zero mean and unit variance.\n",
    "\n",
    "Since it requires some states (mean and variance), we'll need to either:\n",
    "1. Precompute the state values and instantiate the layer with it.\n",
    "```python\n",
    "keras.layers.Normalization(mean=..., variance=...)\n",
    "```\n",
    "2. Or, compute the values using the `adapt()` method.\n",
    "\n",
    "Here let's take a look at the latter option.\n",
    "\n",
    "We first load data to compute these statistics. Here we retrieve the latitude and longitude columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_lat_lon(row):\n",
    "    columns = tf.strings.split(row, \",\")\n",
    "    # latitude idx: 3 and 5, longitude idx: 2 and 4\n",
    "    lat_strings = tf.gather(columns, [3, 5])\n",
    "    lon_strings = tf.gather(columns, [2, 4])\n",
    "    lat_features = tf.strings.to_number(lat_strings)\n",
    "    lon_features = tf.strings.to_number(lon_strings)\n",
    "    return lat_features, lon_features\n",
    "\n",
    "\n",
    "# Create a dataset from the files\n",
    "ds = tf.data.Dataset.list_files(\"../data/taxi-train.csv\")\n",
    "ds = ds.flat_map(tf.data.TextLineDataset)\n",
    "lat_lon_ds = ds.map(parse_lat_lon).batch(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we instantiate `Normalization` layers (`lat_scaler`, `lon_scaler`) and use their `adapt()` method on the loaded latitude and longitude values to learn the mean and variance.\n",
    "These adapted layers can then be used in the model to apply the learned normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lat_scaler = keras.layers.Normalization(axis=None)\n",
    "lon_scaler = keras.layers.Normalization(axis=None)\n",
    "\n",
    "lat_values, lon_values = next(iter(lat_lon_ds))\n",
    "\n",
    "lat_scaler.adapt(lat_values)\n",
    "lon_scaler.adapt(lon_values)\n",
    "\n",
    "print(\"Computed statistics for latitude:\")\n",
    "print(f\"mean: {lat_scaler.mean}, variance: {lat_scaler.variance}\")\n",
    "print(f\"+++++\")\n",
    "print(\"Computed statistics for longitude:\")\n",
    "print(f\"mean: {lon_scaler.mean}, variance: {lon_scaler.variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Input Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLS = [\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "]\n",
    "\n",
    "# input layer is all float\n",
    "inputs = {\n",
    "    colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "    for colname in INPUT_COLS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Feature: Euclidean Distance with a `Lambda` Layer\n",
    "\n",
    "The `euclidean` function calculates straight-line distance. We'll use a [`keras.layers.Lambda` layer](https://keras.io/api/layers/core_layers/lambda/) later to wrap this function, allowing its direct integration into our Keras model for feature engineering. This keeps preprocessing bundled with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff * londiff + latdiff * latdiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing, Normalization, and Lambda Layer Integration\n",
    "\n",
    "Applying the feature engineering steps:\n",
    "1. **Bucket Boundaries:** Now adjusted for normalized data (range `[-5, 5]`).\n",
    "2. **Normalization:** Raw coordinates are scaled using the adapted `lon_scaler` and `lat_scaler`.\n",
    "3. **Lambda Layer:** `euclidean` function calculates distance on these *normalized* coordinates.\n",
    "4. **Discretization:** Normalized coordinates are bucketized.\n",
    "5. **Feature Crossing & Embedding:** Applied to the (now normalized and discretized) features.\n",
    "6. **Concatenate:** The `euclidean_distance` and the final `pd_embed` are combined to be fed into the DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBUCKETS = 16\n",
    "\n",
    "latbuckets = np.linspace(start=-5, stop=5, num=NBUCKETS).tolist()\n",
    "lonbuckets = np.linspace(start=-5, stop=5, num=NBUCKETS).tolist()\n",
    "\n",
    "# Normalize longitude\n",
    "scaled_plon = lon_scaler(inputs[\"pickup_longitude\"])\n",
    "scaled_dlon = lon_scaler(inputs[\"dropoff_longitude\"])\n",
    "\n",
    "# Normalize latitude\n",
    "scaled_plat = lat_scaler(inputs[\"pickup_latitude\"])\n",
    "scaled_dlat = lat_scaler(inputs[\"dropoff_latitude\"])\n",
    "\n",
    "# Lambda layer for the custom euclidean function\n",
    "euclidean_distance = Lambda(euclidean, name=\"euclidean\")(\n",
    "    [scaled_plon, scaled_plat, scaled_dlon, scaled_dlat]\n",
    ")\n",
    "\n",
    "# Discretization\n",
    "plon = Discretization(lonbuckets, name=\"plon_bkt\")(scaled_plon)\n",
    "plat = Discretization(latbuckets, name=\"plat_bkt\")(scaled_plat)\n",
    "dlon = Discretization(lonbuckets, name=\"dlon_bkt\")(scaled_dlon)\n",
    "dlat = Discretization(latbuckets, name=\"dlat_bkt\")(scaled_dlat)\n",
    "\n",
    "\n",
    "# Feature Cross with HashedCrossing layer\n",
    "p_fc = HashedCrossing(num_bins=(NBUCKETS + 1) ** 2, name=\"p_fc\")((plon, plat))\n",
    "d_fc = HashedCrossing(num_bins=(NBUCKETS + 1) ** 2, name=\"d_fc\")((dlon, dlat))\n",
    "pd_fc = HashedCrossing(num_bins=(NBUCKETS + 1) ** 4, name=\"pd_fc\")((p_fc, d_fc))\n",
    "\n",
    "# Embedding with Embedding layer\n",
    "pd_embed = Flatten()(\n",
    "    Embedding(input_dim=(NBUCKETS + 1) ** 4, output_dim=10, name=\"pd_embed\")(\n",
    "        pd_fc\n",
    "    )\n",
    ")\n",
    "\n",
    "deep = Concatenate()(\n",
    "    [\n",
    "        scaled_plon,\n",
    "        scaled_dlon,\n",
    "        scaled_plat,\n",
    "        scaled_dlat,\n",
    "        euclidean_distance,\n",
    "        pd_embed,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the DNN Layers\n",
    "\n",
    "The concatenated `euclidean_distance` and `pd_embed` tensor is passed through `Dense` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_hidden_units = [32, 8]\n",
    "\n",
    "# Add hidden Dense layers\n",
    "for i, num_nodes in enumerate(dnn_hidden_units, start=1):\n",
    "    deep = Dense(num_nodes, activation=\"relu\", name=f\"hidden_{i}\")(deep)\n",
    "\n",
    "# final output is a linear activation because this is regression\n",
    "output = Dense(1, activation=\"linear\", name=\"fare\")(deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and Compile the Engineered Model\n",
    "\n",
    "Define the Keras Model with the original inputs and the final engineered output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[RootMeanSquaredError()],\n",
    "    run_eagerly=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model architecture has changed now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_layer_names=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Engineered Model\n",
    "\n",
    "Train the new model using the same setup as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 10  # training dataset will repeat, wrap around\n",
    "NUM_EVALS = 10  # how many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds = create_dataset(\n",
    "    pattern=\"../data/taxi-train.csv\", batch_size=BATCH_SIZE, mode=\"train\"\n",
    ")\n",
    "\n",
    "evalds = create_dataset(\n",
    "    pattern=\"../data/taxi-valid.csv\", batch_size=BATCH_SIZE, mode=\"eval\"\n",
    ").take(NUM_EVAL_EXAMPLES // BATCH_SIZE)\n",
    "\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "history = model.fit(\n",
    "    trainds,\n",
    "    validation_data=evalds,\n",
    "    epochs=NUM_EVALS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's visualize the DNN model layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_COLS = [\"root_mean_squared_error\", \"val_root_mean_squared_error\"]\n",
    "\n",
    "pd.DataFrame(history.history)[RMSE_COLS].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's a prediction with this new model with engineered features on the example we had above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "    {\n",
    "        \"pickup_longitude\": tf.convert_to_tensor([-73.982683]),\n",
    "        \"pickup_latitude\": tf.convert_to_tensor([40.742104]),\n",
    "        \"dropoff_longitude\": tf.convert_to_tensor([-73.983766]),\n",
    "        \"dropoff_latitude\": tf.convert_to_tensor([40.755174]),\n",
    "    },\n",
    "    steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Copyright 2025 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8a4i3PAFVQd",
    "tags": []
   },
   "source": [
    "# Dataflow NER Pipeline with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%pip install --quiet scikit-learn\n",
    "%pip install --quiet transformers[torch]\n",
    "%pip install --quiet seqeval\n",
    "%pip install --quiet tensorflow\n",
    "%pip install --quiet tf-keras\n",
    "%pip install --quiet torch --quiet\n",
    "%pip install --quiet datasets --quiet\n",
    "%pip install --quiet evaluate --quiet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "PROJECT_ID = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "%env GOOGLE_CLOUD_PROJECT={PROJECT_ID}\n",
    "BUCKET_NAME=f'dataflow_demo_{PROJECT_ID}'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use the exact model you plan to train (bert-base, roberta, etc.)\n",
    "MODEL_NAME = \"google-bert/bert-base-multilingual-cased\"\n",
    "DATASET = \"./train_bert_ner_1k.txt\"\n",
    "gcs_bucket = \"gs://bert-finetuning-ner-demo\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=os.getenv(\"PROJECT_ID\"),\n",
    "    location=os.getenv(\"LOCATION\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "def upload_to_gcs(model_dir, bucket_name):\n",
    "    \"\"\"\n",
    "    Uploads a fine-tuned model directory to Google Cloud Storage (GCS).\n",
    "    \"\"\"\n",
    "    bucket_name = bucket_name.replace(\"gs://\", \"\")  # Remove gs:// prefix if present\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    for root, _, files in os.walk(model_dir):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            gcs_path = os.path.relpath(local_path, model_dir)\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "            print(f\"Uploaded {local_path} to gs://{bucket_name}/{gcs_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6oQl6ywSI4Ux"
   },
   "source": [
    "####\n",
    "# Inference pipeline V1 based on Vertex AI online Endpoint\n",
    "####\n",
    "#Setting Google Cloud env variables\n",
    "%env PROJECT_ID=oleksandr-demo\n",
    "%env LOCATION=us-central1\n",
    "%env CONTAINER_URI=us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-pytorch-inference-cu121.2-2.transformers.4-44.ubuntu2204.py311"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W-3qACl2I9gj"
   },
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "display_name=\"bert-ner-demo-endpoint-v3\"\n",
    "# List endpoints with a filter\n",
    "# We use the filter string to search specifically for the display_name\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f'display_name=\"{display_name}\"',\n",
    "    order_by=\"create_time desc\" # Optional: Get the most recently created one if duplicates exist\n",
    ")\n",
    "deployed_model = endpoints[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference pipeline based on Apache Beam\n",
    "Can be orchestrated by using Cloud Composer or Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vfwfqzBnKeL2"
   },
   "source": [
    "import json\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "from apache_beam import Create\n",
    "from typing import Tuple, Iterable\n",
    "from google.cloud import storage\n",
    "from apache_beam.ml.inference import RunInference\n",
    "from apache_beam.ml.inference.base import PredictionResult, KeyedModelHandler, ModelHandler\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFacePipelineModelHandler, PipelineTask\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam as beam\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, pipeline\n",
    "\n",
    "class BertNerPipelineHandler(ModelHandler):\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.pipeline = None\n",
    "        self.local_folder = \"./tmp_ner_finetuned\"\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the Transformer pipeline model.\"\"\"\n",
    "        # Load the fine-tuned model and tokenizer\n",
    "        if self.pipeline is None:\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(self.model_path)\n",
    "            blobs = bucket.list_blobs(prefix=\"\")\n",
    "            if not os.path.exists(self.local_folder):\n",
    "                os.makedirs(self.local_folder )\n",
    "            for blob in blobs:\n",
    "                file_path = os.path.join(self.local_folder, os.path.basename(blob.name))\n",
    "                blob.download_to_filename(file_path)\n",
    "            tokenizer = BertTokenizerFast.from_pretrained(self.local_folder)\n",
    "            model = BertForTokenClassification.from_pretrained(self.local_folder)\n",
    "            self.pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")\n",
    "        return self.pipeline\n",
    "\n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        \"\"\"Runs inference on a batch of inputs.\"\"\"\n",
    "        predictions = model(batch)\n",
    "        return [PredictionResult(x, y) for x, y in zip(batch, predictions)]\n",
    "\n",
    "    def get_num_bytes(self, batch):\n",
    "        \"\"\"Returns the estimated size of the batch.\"\"\"\n",
    "        return sum(len(str(x).encode('utf-8')) for x in batch)\n",
    "\n",
    "model_handler = BertNerPipelineHandler(\"bert-finetuning-ner-demo\")\n",
    "\n",
    "def mask_sensetive_data(predictions):\n",
    "    text = predictions.example\n",
    "    redacted_text = text\n",
    "\n",
    "    for mask in predictions.inference:\n",
    "        text_to_mask = text[int(mask['start']): int(mask['end'])]\n",
    "        if len(text_to_mask) > 0:\n",
    "            redacted_text = redacted_text.replace(text_to_mask, f\"[{mask['entity_group']}]\")\n",
    "    return {\"text\": text, \"redacted_text\": redacted_text}\n",
    "\n",
    "\n",
    "input_json_file = \"gs://bert-ner-demo-io-storage/generated_CoNLL_train_data.json\"\n",
    "OUTPUT_GCS_BUCKET = \"gs://bert-ner-demo-io-storage/output/\"\n",
    "TEMP_LOCATION=\"gs://bert-ner-demo-io-storage/temp/\"\n",
    "STAGING_LOCATION=\"gs://bert-ner-demo-io-storage/staging/\"\n",
    "PROJECT_ID=\"oleksandr-demo\"\n",
    "LOCATION=\"us-central1\"\n",
    "\n",
    "# Set up Beam PipelineOptions for Dataflow\n",
    "# pipeline_options = PipelineOptions(\n",
    "#     runner=\"DirectRunner\",\n",
    "#     project=PROJECT_ID,\n",
    "#     region=LOCATION,\n",
    "#     temp_location=TEMP_LOCATION,\n",
    "#     staging_location=STAGING_LOCATION,\n",
    "#     job_name=\"bert-demo-ner-inference\",\n",
    "#     save_main_session=True,\n",
    "# )\n",
    "\n",
    "p = beam.Pipeline(options=PipelineOptions())\n",
    "\n",
    "elements=[\n",
    "    \"Send the invoice to 123 Main Street, New York. Contact John Connnor at johnconnor@example.com.\",\n",
    "    \"My name is Jessica Williams.\",\n",
    "]\n",
    "\n",
    "pp = (p\n",
    "      # | 'Init' >> beam.Create([input_json_file])\n",
    "      # | \"Read File Content\" >> beam.io.fileio.ReadMatches()\n",
    "      # | \"Extract Content\" >> beam.Map(lambda file: file.read_utf8())\n",
    "      # | \"Parse JSON\" >> beam.Map(json.loads)\n",
    "      # | \"To Records\" >> beam.FlatMap()\n",
    "      # | \"Extract text\" >> beam.Map(lambda x: x[\"text\"])\n",
    "      | \"Create elements\" >> Create(elements)\n",
    "      | \"RunInference\" >> RunInference(model_handler)\n",
    "      | \"Mask Data\" >> beam.Map(mask_sensetive_data)\n",
    "      | \"Format Output\" >> beam.Map(json.dumps)\n",
    "      | \"Write to GCS\" >> beam.io.WriteToText(OUTPUT_GCS_BUCKET))\n",
    "\n",
    "p.run().wait_until_finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "from apache_beam import Create\n",
    "from typing import Tuple, Iterable\n",
    "from google.cloud import storage\n",
    "from apache_beam.ml.inference import RunInference\n",
    "from apache_beam.ml.inference.base import PredictionResult, KeyedModelHandler, ModelHandler\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFacePipelineModelHandler, PipelineTask\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam as beam\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, pipeline\n",
    "\n",
    "class BertNerPipelineHandler(ModelHandler):\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.pipeline = None\n",
    "        self.local_folder = \"./tmp_ner_finetuned\"\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the Transformer pipeline model.\"\"\"\n",
    "        # Load the fine-tuned model and tokenizer\n",
    "        if self.pipeline is None:\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(self.model_path)\n",
    "            blobs = bucket.list_blobs(prefix=\"\")\n",
    "            if not os.path.exists(self.local_folder):\n",
    "                os.makedirs(self.local_folder )\n",
    "            for blob in blobs:\n",
    "                file_path = os.path.join(self.local_folder, os.path.basename(blob.name))\n",
    "                blob.download_to_filename(file_path)\n",
    "            tokenizer = BertTokenizerFast.from_pretrained(self.local_folder)\n",
    "            model = BertForTokenClassification.from_pretrained(self.local_folder)\n",
    "            self.pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")\n",
    "        return self.pipeline\n",
    "\n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        \"\"\"Runs inference on a batch of inputs.\"\"\"\n",
    "        predictions = model(batch)\n",
    "        return [PredictionResult(x, y) for x, y in zip(batch, predictions)]\n",
    "\n",
    "    def get_num_bytes(self, batch):\n",
    "        \"\"\"Returns the estimated size of the batch.\"\"\"\n",
    "        return sum(len(str(x).encode('utf-8')) for x in batch)\n",
    "\n",
    "model_handler = BertNerPipelineHandler(\"bert-finetuning-ner-demo\")\n",
    "\n",
    "def mask_sensetive_data(predictions):\n",
    "    text = predictions.example\n",
    "    redacted_text = text\n",
    "\n",
    "    for mask in predictions.inference:\n",
    "        text_to_mask = text[int(mask['start']): int(mask['end'])]\n",
    "        if len(text_to_mask) > 0:\n",
    "            redacted_text = redacted_text.replace(text_to_mask, f\"[{mask['entity_group']}]\")\n",
    "    return {\"text\": text, \"redacted_text\": redacted_text}\n",
    "\n",
    "\n",
    "def extract_entity(predictions):\n",
    "    text = predictions.example\n",
    "    redacted_text = text\n",
    "    entity_dict = {}\n",
    "    for mask in predictions.inference:\n",
    "        text_to_mask = text[int(mask['start']): int(mask['end'])]\n",
    "        if len(text_to_mask) > 0:\n",
    "            if mask['entity_group'] not in entity_dict:\n",
    "                entity_dict[mask['entity_group']] = []\n",
    "            entity_dict[mask['entity_group']].append(text_to_mask)\n",
    "            \n",
    "    return {\"text\": text, \"extracted\": entity_dict}\n",
    "\n",
    "\n",
    "DATAFLOW_BUCKET=f\"bert-ner-demo-io-storage-{PROJECT_ID}\" \n",
    "OUTPUT_GCS_BUCKET = f\"gs://{DATAFLOW_BUCKET}/output/\"\n",
    "TEMP_LOCATION=f\"gs://{DATAFLOW_BUCKET}/temp/\"\n",
    "STAGING_LOCATION=f\"gs://{DATAFLOW_BUCKET}/staging/\"\n",
    "\n",
    "# Set up Beam PipelineOptions for Dataflow\n",
    "# pipeline_options = PipelineOptions(\n",
    "#     runner=\"DirectRunner\",\n",
    "#     project=PROJECT_ID,\n",
    "#     region=LOCATION,\n",
    "#     temp_location=TEMP_LOCATION,\n",
    "#     staging_location=STAGING_LOCATION,\n",
    "#     job_name=\"bert-demo-ner-inference\",\n",
    "#     save_main_session=True,\n",
    "# )\n",
    "\n",
    "p = beam.Pipeline(options=PipelineOptions())\n",
    "\n",
    "elements=[\n",
    "    \"Send the invoice to 123 Main Street, New York. Contact John Connnor at johnconnor@example.com.\",\n",
    "    \"My name is Jessica Williams.\",\n",
    "]\n",
    "\n",
    "pp = (p\n",
    "      # | 'Init' >> beam.Create([input_json_file])\n",
    "      # | \"Read File Content\" >> beam.io.fileio.ReadMatches()\n",
    "      # | \"Extract Content\" >> beam.Map(lambda file: file.read_utf8())\n",
    "      # | \"Parse JSON\" >> beam.Map(json.loads)\n",
    "      # | \"To Records\" >> beam.FlatMap()\n",
    "      # | \"Extract text\" >> beam.Map(lambda x: x[\"text\"])\n",
    "      | \"Create elements\" >> Create(elements)\n",
    "      | \"RunInference\" >> RunInference(model_handler)\n",
    "      | \"Postprocess\" >> beam.Map(extract_entity)\n",
    "      | \"Format Output\" >> beam.Map(json.dumps)\n",
    "      | \"Write to GCS\" >> beam.io.WriteToText(OUTPUT_GCS_BUCKET))\n",
    "\n",
    "p.run().wait_until_finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Apache Beam 2.69.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.69.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

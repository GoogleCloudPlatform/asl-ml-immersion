{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Gemini for multi-modal prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkHPv2myT2cx"
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "\n",
    "1. Learn how to generate text from text prompts using the Gemini\n",
    "2. Learn how to explore various features and configuration options \n",
    "3. Learn how to generate text from image(s) and text prompts with Gemini\n",
    "4. Learn how to generate text from video and text prompts with Gemini\n",
    "5. Learn how to use all modalities at once using Gemini\n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Gemini\n",
    "In this notebook, you will learn how to use the Google Gen AI SDK for Python to interact with the Gemini.\n",
    "Gemini is a family (e.g., Gemini Pro, Gemini Flash, Gemini Flash-Lite) of generative AI models developed by Google DeepMind that is designed for multimodal use cases.  \n",
    "\n",
    "You can interact with Gemini in many ways, including:\n",
    "- The [Vertex AI Studio](https://cloud.google.com/generative-ai-studio) for quick testing and command generation\n",
    "- cURL commands\n",
    "- The Google Gen AI SDK.\n",
    "\n",
    "### Google Gen AI SDK\n",
    "[Google Gen AI Python SDK](https://github.com/googleapis/python-genai) provides an interface for developers to integrate Google's generative models including Gemini into their Python applications. It supports the [Gemini Developer API](https://ai.google.dev/gemini-api/docs) and [Vertex AI APIs](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/overview).\n",
    "\n",
    "\n",
    "This notebook focuses on using the **Gen AI SDK** to call the Vertex AI Gemini APIs.\n",
    "\n",
    "For more information, see the [Generative AI on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview) documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXHfaVS66_01"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lslYAvw37JGQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import io\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig, Part\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "## Setup\n",
    "### Specify Gemini version\n",
    "\n",
    "The Gemini model is designed to handle natural language tasks, multiturn text and code chat, and code generation. Here we use `gemini-2.0-flash-001` version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2998506fe6d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = \"gemini-2.0-flash-001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIl7R_jBUsaC"
   },
   "source": [
    "### Set up Gen AI SDK client\n",
    "Let's instantiate the Gen AI SDK client. You can call Gemini via either Vertex AI and Gemini Developer API.\n",
    "\n",
    "Here we use Vertex AI by spefifying `vertexai=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text from text prompts\n",
    "**Exercise**\n",
    "\n",
    "Send a text prompt to the model and print the stream of response as it arrives in the cell below. Gemini provides a streaming response mechanism. With this approach, you don't need to wait for the complete response; you can start processing fragments as soon as they're accessible using `client.models.generate_content_stream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAo-UsfZECGF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us8idXnVyQ97",
    "tags": []
   },
   "source": [
    "### Try your own prompts\n",
    "**for example:**\n",
    "\n",
    "- What are the biggest challenges facing the healthcare industry?\n",
    "- What are the latest developments in the automotive industry?\n",
    "- What are the biggest opportunities in retail industry?\n",
    "- (Try your own prompts!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmAZQW1GyQ97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDK4XLdz3Oqv"
   },
   "source": [
    "### Model parameters\n",
    "\n",
    "Every prompt you send to the model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. The `temperature` parameter controls the amount of consistency in the response. With a temperature of 0 you'll consistently get the exact same answer to the same prompt, and as you increase the temperature you'll get more \"creative\" albeit less consistent answers. The `top_p` and `top_k` control the tokens that are considered by the LLM to generate the answer. If you set `top_p` to 0.2 for instance, the model will only consider the most probable token for an answer whose probability sums up to 0.2, while if you set `top_k` to 100, the model will only consider the top 100 most probable tokens.  `candidate_counts` determines the number of candidate answers you want to be generated, while `max_output_tokens` specifies the maximum number of tokens allowed per candidate answer. \n",
    "\n",
    "\n",
    "You can experiment with different model parameters to see how the results change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Create a configuration object setting the `temperature`, `top_p`, `top_k`, and `candidate_count` using `GenerateContentConfig` and pass this configuration object to `generate_content_stream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_2ann-F3WTo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config = None  # TODO\n",
    "\n",
    "responses = client.models.generate_content_stream(None)  # TODO\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga0xM9z9fAnR"
   },
   "source": [
    "### Test chat prompts\n",
    "\n",
    "**Exercise:**<br>\n",
    "The Gemini model supports natural multi-turn conversations and is ideal for text tasks that require back-and-forth interactions. The following examples show how the model responds during a multi-turn conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFbGVflTfBBk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = None # TODO\n",
    "\n",
    "prompt = \"\"\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\n",
    "\n",
    "Suggest another movie I might like.\n",
    "\"\"\"\n",
    "\n",
    "responses = # TODO\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP_z_Oh1J4nk"
   },
   "source": [
    "This follow-up prompt shows how the model responds based on the previous prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCq7JNBKJrI8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Are my favorite movies based on a book series?\"\n",
    "\n",
    "responses = # TODO\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view the chat history:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chat.get_history())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK6TsnYghrQk"
   },
   "source": [
    "## Use Gemini for Multimodal data\n",
    "\n",
    "Gemini is a multimodal model that supports multimodal prompts. You can include text, image(s), and video in your prompt requests and get text or code responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwvfMDEDVyKI"
   },
   "source": [
    "### Define helper functions\n",
    "\n",
    "Define helper functions to load and display images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQS13DI6Pjp6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_byte_image(image_bytes: bytes) -> None:\n",
    "    image = PIL_Image.open(io.BytesIO(image_bytes))\n",
    "    IPython.display.display(image)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    if image_url.startswith(\"gs://\"):\n",
    "        image_url = get_url_from_gcs(image_url)\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def get_url_from_gcs(gcs_uri: str) -> str:\n",
    "    # converts gcs uri to url for image display.\n",
    "    url = \"https://storage.googleapis.com/\" + gcs_uri.replace(\n",
    "        \"gs://\", \"\"\n",
    "    ).replace(\" \", \"%20\")\n",
    "    return url\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list):\n",
    "    \"\"\"\n",
    "    Given contents that would be sent to Gemini,\n",
    "    output the full multimodal prompt for ease of readability.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        if isinstance(content, Part):\n",
    "            if content.inline_data:\n",
    "                mime_type = content.inline_data.mime_type\n",
    "                if mime_type.startswith(\"image\"):\n",
    "                    display_byte_image(content.inline_data.data)\n",
    "            elif content.file_data:\n",
    "                mime_type = content.file_data.mime_type\n",
    "                if mime_type.startswith(\"image\"):\n",
    "                    image_bytes = get_image_bytes_from_url(\n",
    "                        content.file_data.file_uri\n",
    "                    )\n",
    "                    display_byte_image(image_bytes)\n",
    "            elif content.text:\n",
    "                print(content.text)\n",
    "        if isinstance(content, str):\n",
    "            print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wy75sLb-yjNn"
   },
   "source": [
    "### Generate text from text & image(s)\n",
    "#### Generate text from local image and text\n",
    "\n",
    "**Exercise:**<br>\n",
    "Use the `Part.from_bytes` method to load a local file as the image to generate text for.\n",
    "Then create the `contents` object containing an image as well as the prompt needed by `client.models.generate_content_stream` to describe the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzqjpEiryjNo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download an image from Google Cloud Storage\n",
    "local_path = \"image.jpg\"\n",
    "! gsutil cp \"gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg\" {local_path}\n",
    "\n",
    "# Load from local file\n",
    "image = None  # TODO\n",
    "\n",
    "# Prepare contents\n",
    "prompt = None  # TODO\n",
    "contents = None  # TODO\n",
    "\n",
    "responses = None  # TODO\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJvME8gV2nyk"
   },
   "source": [
    "#### Images with URL\n",
    "\n",
    "If your images are stored in [Cloud Storage](https://cloud.google.com/storage/docs) or have direct links, you can specify the URI of the image to include in the prompt. You must also specify the `mime_type` field. The supported MIME types for images include `image/png` and `image/jpeg`.\n",
    "\n",
    "Note that the URI (not to be confused with URL) for a Cloud Storage object should always start with `gs://`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load image from Cloud Storage URI\n",
    "gcs_uri = \"gs://cloud-samples-data/generative-ai/image/boats.jpeg\"\n",
    "\n",
    "# Prepare contents\n",
    "image = Part.from_uri(file_uri=gcs_uri, mime_type=\"image/jpeg\")\n",
    "prompt = \"Describe the scene?\"\n",
    "contents = [image, prompt]\n",
    "\n",
    "responses = client.models.generate_content_stream(\n",
    "    model=MODEL, contents=contents\n",
    ")\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining multiple images and text prompts in JSON format for few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can send more than one image at a time, and also place your images anywhere alongside your text prompt.\n",
    "\n",
    "In the example below, few-shot prompting is performed to have Gemini return the city and landmark in a specific JSON format.<br>\n",
    "We can force the ouput JSON schama by providing pydantic model.\n",
    "\n",
    "**Exercise:**<br>\n",
    "Write the few-shot example prompts `prompt1` and `prompt2` below to help Gemini understand the output format you want the information of the city (\"London\" for image 1 and \"Paris\" for image 2) and landmark (\"Big Ben\" for image 1 and \"Eiffel Tower\" for image 2) for image 3. <br>\n",
    "Use the `LandmarkInfo` model defined below, and `Part.json` to provide the JSON schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the return JSON schema using pydantic\n",
    "class LandmarkInfo(BaseModel):\n",
    "    city: str\n",
    "    landmark: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfU7Qlz1hAEA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load images from Cloud Storage URI\n",
    "image1_url = (\n",
    "    \"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark1.jpg\"\n",
    ")\n",
    "image2_url = (\n",
    "    \"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark2.jpg\"\n",
    ")\n",
    "image3_url = (\n",
    "    \"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg\"\n",
    ")\n",
    "image1 = Part.from_uri(file_uri=image1_url, mime_type=\"image/jpeg\")\n",
    "image2 = Part.from_uri(file_uri=image2_url, mime_type=\"image/jpeg\")\n",
    "image3 = Part.from_uri(file_uri=image3_url, mime_type=\"image/jpeg\")\n",
    "\n",
    "# Prepare prompts\n",
    "prompt1 = None  # TODO\n",
    "prompt2 = None  # TODO\n",
    "\n",
    "# Prepare contents\n",
    "contents = [image1, prompt1, image2, prompt2, image3]\n",
    "\n",
    "# Specify output schema\n",
    "generate_config = GenerateContentConfig(\n",
    "    response_mime_type=None,  # TODO\n",
    "    response_schema=None,  # TODO\n",
    ")\n",
    "\n",
    "responses = client.models.generate_content_stream(\n",
    "    model=MODEL, contents=contents, config=generate_config\n",
    ")\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyjpi1fB7mgj"
   },
   "source": [
    "### Generate text from a video file\n",
    "\n",
    "Specify the Cloud Storage URI of the video to include in the prompt. The bucket that stores the file must be in the same Google Cloud project that's sending the request. You must also specify the `mime_type` field. The supported MIME type for video includes `video/mp4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\"\n",
    "video_uri = f\"gs://{file_path}\"\n",
    "video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "\n",
    "IPython.display.Video(video_url, width=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**<br>\n",
    "Extract the video using `Part` so that it can be concatenate the `contents` passed to Gemini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXX1jLXq7ojB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "What is the profession of the main person?\n",
    "What are the main features of the phone highlighted?\n",
    "Which city was this recorded in?\n",
    "Provide the answer JSON.\n",
    "\"\"\"\n",
    "\n",
    "video = # TODO\n",
    "contents = [prompt, video]\n",
    "\n",
    "generate_config = GenerateContentConfig(\n",
    "    response_mime_type=\"application/json\",\n",
    ")\n",
    "\n",
    "responses = client.models.generate_content_stream(\n",
    "    model=MODEL, contents=contents, config=generate_config\n",
    ")\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### All modalities (images, video, audio, text) at once\n",
    "\n",
    "Gemini is natively multimodal and supports interleaving of data from different modalities, it can support a mix of audio, visual, text, and code inputs in the same input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_path = (\n",
    "    \"cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4\"\n",
    ")\n",
    "video_file_uri = f\"gs://{video_file_path}\"\n",
    "video_file_url = f\"https://storage.googleapis.com/{video_file_path}\"\n",
    "\n",
    "IPython.display.Video(video_file_url, width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_path = \"cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\"\n",
    "image_file_uri = f\"gs://{image_file_path}\"\n",
    "image_file_url = f\"https://storage.googleapis.com/{image_file_path}\"\n",
    "\n",
    "IPython.display.Image(image_file_url, width=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**  \n",
    "Now that we have read in a video and an image file. Write a prompt to look through each frame in the video to answer the about what timestamp in the video is the moment in image happening at.  \n",
    "\n",
    "Once you have crafted your prompt, concatenate the video, image and your prompt together and pass it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = Part.from_uri(file_uri=video_file_uri, mime_type=\"video/mp4\")\n",
    "image_file = Part.from_uri(file_uri=image_file_uri, mime_type=\"image/png\")\n",
    "\n",
    "prompt = \"\"\"\n",
    " TODO\n",
    "\"\"\"\n",
    "\n",
    "contents = #TODO\n",
    "\n",
    "response = client.models.generate_content(model=MODEL, contents=contents)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement \n",
    "\n",
    "This notebook is adapted from a [tutorial](https://github.com/GoogleCloudPlatform/generative-ai/commits/main/gemini/getting-started/intro_gemini_python.ipynb)\n",
    "written by Polong Lin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Copyright 2023 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

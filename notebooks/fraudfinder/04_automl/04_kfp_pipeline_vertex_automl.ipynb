{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Continuous Training with AutoML Vertex Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to use Vertex AutoML pre-built components\n",
    "1. Learn how to build a Vertex AutoML pipeline with these components using BigQuery as a data source\n",
    "1. Learn how to compile, upload, and run the Vertex AutoML pipeline\n",
    "\n",
    "\n",
    "In this lab, you will build, deploy, and run a Vertex AutoML pipeline that orchestrates the **Vertex AutoML AI** services to train, tune, and deploy a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from google.cloud import aiplatform"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT = !(gcloud config get-value project)\n",
    "PROJECT = PROJECT[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow implemented by the pipeline is defined using a Python based Domain Specific Language (DSL). The pipeline's DSL is in the `pipeline_vertex/pipeline_vertex_automl.py` file that we will generate below.\n",
    "\n",
    "The pipeline's DSL has been designed to avoid hardcoding any environment specific settings like file paths or connection strings. These settings are provided to the pipeline code through a set of environment variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building and deploying the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the pipeline to disk:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import json\n",
    "\n",
    "# General\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Vertex Pipelines\n",
    "from typing import NamedTuple\n",
    "\n",
    "import kfp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline variables\n",
    "PIPELINE_NAME = f\"fraud-finder-automl-pipeline-{ID}\"\n",
    "\n",
    "# Feature Store component variables\n",
    "BQ_DATASET = \"tx\"\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{ID}\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "# Dataset component variables\n",
    "DATASET_NAME = f\"fraud_finder_dataset_{ID}\"\n",
    "\n",
    "# Endpoint variables\n",
    "ENDPOINT_NAME = f\"{ENDPOINT_NAME}_automl_pipeline_{ID}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%writefile ./pipeline_vertex/fs_import_component.py\n",
    "# Copyright 2025 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n",
    "# use this file except in compliance with the License. You may obtain a copy of\n",
    "# the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\n",
    "\"\"\"Lightweight component ingest features.\"\"\"\n",
    "from typing import Dict, List, NamedTuple\n",
    "\n",
    "from kfp.dsl import Metrics, Output, component\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform==1.112.0\"],\n",
    ")\n",
    "def ingest_features_gcs(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    bucket_name: str,\n",
    "    feature_store_id: str,\n",
    "    read_instances_uri: str,\n",
    ") -> NamedTuple(\"Outputs\", [(\"snapshot_uri_paths\", List[str]),],):\n",
    "    # Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "    from datetime import datetime\n",
    "    import glob\n",
    "    import urllib\n",
    "    import json\n",
    "    #import logger TODO:\n",
    "    from typing import NamedTuple\n",
    "\n",
    "    # Feature Store\n",
    "    from google.cloud.aiplatform import Featurestore, EntityType, Feature\n",
    "\n",
    "    # Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    api_endpoint = region + \"-aiplatform.googleapis.com\"\n",
    "    bucket = urllib.parse.urlsplit(bucket_name).netloc\n",
    "    export_uri = (\n",
    "        f\"{bucket_name}/data/snapshots/{timestamp}\"  # format as new gsfuse requires\n",
    "    )\n",
    "    export_uri_path = f\"/gcs/{bucket}/data/snapshots/{timestamp}\"\n",
    "    customer_entity = \"customer\"\n",
    "    terminal_entity = \"terminal\"\n",
    "    serving_feature_ids = {customer_entity: [\"*\"], terminal_entity: [\"*\"]}\n",
    "\n",
    "    print(timestamp)\n",
    "    print(bucket)\n",
    "    print(export_uri)\n",
    "    print(export_uri_path)\n",
    "    print(customer_entity)\n",
    "    print(terminal_entity)\n",
    "    print(serving_feature_ids)\n",
    "\n",
    "    # Main -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ## Define the feature store resource path\n",
    "    feature_store_resource_path = (\n",
    "        f\"projects/{project_id}/locations/{region}/featurestores/{feature_store_id}\"\n",
    "    )\n",
    "    print(\"Feature Store: \\t\", feature_store_resource_path)\n",
    "   \n",
    "\n",
    "    ## Run batch job request\n",
    "    try:\n",
    "        ff_feature_store = Featurestore(feature_store_resource_path)\n",
    "        ff_feature_store.batch_serve_to_gcs(\n",
    "            gcs_destination_output_uri_prefix=export_uri,\n",
    "            gcs_destination_type=\"csv\",\n",
    "            serving_feature_ids=serving_feature_ids,\n",
    "            read_instances_uri=read_instances_uri,\n",
    "            pass_through_fields=[\"tx_fraud\", \"tx_amount\"],\n",
    "        )\n",
    "    except Exception as error:\n",
    "         print(error)\n",
    "\n",
    "    # Store metadata\n",
    "    snapshot_pattern = f\"{export_uri_path}/*.csv\"\n",
    "    snapshot_files = glob.glob(snapshot_pattern)\n",
    "    snapshot_files_fmt = [p.replace(\"/gcs/\", \"gs://\") for p in snapshot_files]\n",
    "\n",
    "    component_outputs = NamedTuple(\n",
    "        \"Outputs\",\n",
    "        [\n",
    "            (\"snapshot_uri_paths\", List[str]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(snapshot_pattern)\n",
    "    print(snapshot_files)\n",
    "    print(snapshot_files_fmt)\n",
    "\n",
    "    return component_outputs(snapshot_files_fmt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ./pipeline_vertex/create_load_query_component.py\n",
    "# Copyright 2025 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n",
    "# use this file except in compliance with the License. You may obtain a copy of\n",
    "# the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\n",
    "\"\"\"Lightweight component ingest features.\"\"\"\n",
    "from typing import Dict, List, NamedTuple\n",
    "\n",
    "from kfp.dsl import Metrics, Output, component\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def create_load_query(snapshot_uri_paths: List[str], destination_table: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the BigQuery LOAD DATA query string.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    snapshot_files_string = json.dumps(snapshot_uri_paths)\n",
    "\n",
    "    query_template = f\"\"\"\n",
    "        LOAD DATA OVERWRITE {destination_table} (\n",
    "            tx_fraud INTEGER,\n",
    "            tx_amount FLOAT64,\n",
    "            timestamp TIMESTAMP,\n",
    "            entity_type_customer STRING,\n",
    "            customer_id_nb_tx_30min_window FLOAT64,\n",
    "            customer_id_avg_amount_1day_window FLOAT64,\n",
    "            customer_id_avg_amount_60min_window FLOAT64,\n",
    "            customer_id_nb_tx_7day_window FLOAT64,\n",
    "            customer_id_avg_amount_15min_window FLOAT64,\n",
    "            customer_id_avg_amount_14day_window FLOAT64,\n",
    "            customer_id_nb_tx_1day_window FLOAT64,\n",
    "            customer_id_nb_tx_60min_window FLOAT64,\n",
    "            customer_id_avg_amount_7day_window FLOAT64,\n",
    "            customer_id_nb_tx_14day_window FLOAT64,\n",
    "            customer_id_avg_amount_30min_window FLOAT64,\n",
    "            customer_id_nb_tx_15min_window FLOAT64,\n",
    "            entity_type_terminal STRING,\n",
    "            terminal_id_avg_amount_15min_window FLOAT64,\n",
    "            terminal_id_nb_tx_7day_window FLOAT64,\n",
    "            terminal_id_risk_14day_window FLOAT64,\n",
    "            terminal_id_avg_amount_60min_window FLOAT64,\n",
    "            terminal_id_risk_1day_window FLOAT64,\n",
    "            terminal_id_nb_tx_30min_window FLOAT64,\n",
    "            terminal_id_avg_amount_30min_window FLOAT64,\n",
    "            terminal_id_nb_tx_14day_window FLOAT64,\n",
    "            terminal_id_nb_tx_15min_window FLOAT64,\n",
    "            terminal_id_risk_7day_window FLOAT64,\n",
    "            terminal_id_nb_tx_1day_window FLOAT64,\n",
    "            terminal_id_nb_tx_60min_window FLOAT64\n",
    "        ) FROM FILES (\n",
    "            format = 'CSV',\n",
    "            uris = {snapshot_files_string},\n",
    "            skip_leading_rows = 1\n",
    "        );\n",
    "    \"\"\"\n",
    "    print(query_template)\n",
    "    return query_template.replace('\"', \"'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%writefile ./pipeline_vertex/pipeline_vertex_automl.py\n",
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n",
    "# use this file except in compliance with the License. You may obtain a copy of\n",
    "# the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\n",
    "\"\"\"Kubeflow Covertype Pipeline.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from google_cloud_pipeline_components.v1.automl.training_job import (\n",
    "    AutoMLTabularTrainingJobRunOp,\n",
    ")\n",
    "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    ")\n",
    "from google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\n",
    "\n",
    "from kfp import dsl\n",
    "from fs_import_component import ingest_features_gcs\n",
    "from create_load_query_component import create_load_query\n",
    "\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT = os.getenv(\"PROJECT\")\n",
    "REGION = os.getenv(\"REGION\", \"us-central1\")\n",
    "DATASET_SOURCE = os.getenv(\"DATASET_SOURCE\")\n",
    "PIPELINE_NAME = os.getenv(\"PIPELINE_NAME\", \"fraudfinder\")\n",
    "DISPLAY_NAME = os.getenv(\"MODEL_DISPLAY_NAME\", PIPELINE_NAME)\n",
    "TARGET_COLUMN = os.getenv(\"TARGET_COLUMN\", \"tx_fraud\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "SERVING_MACHINE_TYPE = os.getenv(\"SERVING_MACHINE_TYPE\", \"n1-standard-4\")\n",
    "ID = os.getenv(\"ID\")\n",
    "\n",
    "VERTEX_DATASET_SOURCE=f\"bq://{PROJECT}.{DATASET_SOURCE}\"\n",
    "\n",
    "FEATURESTORE_ID=f\"fraudfinder_{ID}\"\n",
    "# Feature Store component variables\n",
    "BQ_DATASET = \"tx\"\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{ID}\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "bucket_name = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "column_specs = {\n",
    "    'tx_amount': \"numeric\",\n",
    "    'customer_id_avg_amount_14day_window': \"numeric\",\n",
    "    'customer_id_avg_amount_15min_window': \"numeric\",\n",
    "    'customer_id_avg_amount_1day_window': \"numeric\",\n",
    "    'customer_id_avg_amount_30min_window': \"numeric\",\n",
    "    'customer_id_avg_amount_60min_window': \"numeric\",\n",
    "    'customer_id_avg_amount_7day_window': \"numeric\",\n",
    "    'customer_id_nb_tx_14day_window': \"numeric\",\n",
    "    'customer_id_nb_tx_15min_window': \"numeric\",\n",
    "    'customer_id_nb_tx_1day_window': \"numeric\",\n",
    "    'customer_id_nb_tx_30min_window': \"numeric\",\n",
    "    'customer_id_nb_tx_60min_window': \"numeric\",\n",
    "    'customer_id_nb_tx_7day_window': \"numeric\",\n",
    "    'terminal_id_avg_amount_15min_window': \"numeric\",\n",
    "    'terminal_id_avg_amount_30min_window': \"numeric\",\n",
    "    'terminal_id_avg_amount_60min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_14day_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_15min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_1day_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_30min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_60min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_7day_window': \"numeric\",\n",
    "    'terminal_id_risk_14day_window': \"numeric\",\n",
    "    'terminal_id_risk_1day_window': \"numeric\",\n",
    "    'terminal_id_risk_7day_window': \"numeric\"\n",
    "}\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"{PIPELINE_NAME}-vertex-automl-pipeline\",\n",
    "    description=f\"AutoML Vertex Pipeline for {PIPELINE_NAME}\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def create_pipeline():\n",
    "    \n",
    "\n",
    "    #Ingest data from featurestore\n",
    "    ingest_features_op = ingest_features_gcs(\n",
    "        project_id=PROJECT,\n",
    "        region=REGION,\n",
    "        bucket_name=bucket_name,\n",
    "        feature_store_id=FEATURESTORE_ID,\n",
    "        read_instances_uri=READ_INSTANCES_URI,\n",
    "    )\n",
    "    \n",
    "    #Prepare SQL Query for BigQuery Job\n",
    "    bq_load_query_op = create_load_query(\n",
    "        snapshot_uri_paths=ingest_features_op.outputs[\"snapshot_uri_paths\"],\n",
    "        destination_table=DATASET_SOURCE\n",
    "    )\n",
    "\n",
    "    # Use the BigqueryQueryJobOp to ingest training dataset\n",
    "    bq_job_op = BigqueryQueryJobOp(\n",
    "        project=PROJECT,\n",
    "        query=bq_load_query_op.output,\n",
    "        #query_parameters=bq_query_params_list,\n",
    "    )\n",
    "    \n",
    "    #Create Dataset\n",
    "    dataset_create_task = TabularDatasetCreateOp(\n",
    "        project=PROJECT,\n",
    "        display_name=DISPLAY_NAME,\n",
    "        bq_source=VERTEX_DATASET_SOURCE\n",
    "    ).after(bq_job_op)\n",
    "\n",
    "    # Run the AutoML Tabular Training Job\n",
    "    automl_training_task = AutoMLTabularTrainingJobRunOp(\n",
    "        project=PROJECT,\n",
    "        display_name=DISPLAY_NAME,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        dataset=dataset_create_task.outputs[\"dataset\"],\n",
    "        target_column=TARGET_COLUMN,\n",
    "        timestamp_split_column_name='timestamp',\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1,\n",
    "        # Feature list configuration\n",
    "        column_specs=column_specs,\n",
    "        # column_transformations=column_transformations,\n",
    "        # New parameters for budget and early stopping\n",
    "        budget_milli_node_hours=1000,  # 1000 milli-node hours = 1 node hour\n",
    "        disable_early_stopping=False   # Explicitly set to False to enable early stopping\n",
    "    )\n",
    "    \n",
    "    # Create Vertex AI Endpoint\n",
    "    endpoint_create_task = EndpointCreateOp(\n",
    "        project=PROJECT,\n",
    "        display_name=DISPLAY_NAME,\n",
    "    ).after(automl_training_task)\n",
    "\n",
    "    # Deploy model to the Vertex AI Endpoint\n",
    "    model_deploy_task = ModelDeployOp(  # pylint: disable=unused-variable\n",
    "        model=automl_training_task.outputs[\"model\"],\n",
    "        endpoint=endpoint_create_task.outputs[\"endpoint\"],\n",
    "        deployed_model_display_name=DISPLAY_NAME,\n",
    "        dedicated_resources_machine_type=SERVING_MACHINE_TYPE,\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the environment variables that will be passed to the pipeline compiler:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "ARTIFACT_STORE = f\"gs://{PROJECT}-kfp-artifact-store\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATASET_SOURCE = \"tx.train_table_automl_demo\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT={PROJECT}\n",
    "%env REGION={REGION}\n",
    "%env DATASET_SOURCE={DATASET_SOURCE}\n",
    "%env ID={ID}\n",
    "%env BUCKET_NAME={BUCKET_NAME}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that the `ARTIFACT_STORE` has been created, and let us create it if not:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CLI compiler to compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the pipeline from the Python file we generated into a YAML description using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "PIPELINE_YAML = \"fraudfinder_automl_vertex_pipeline.yaml\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!kfp dsl compile --py pipeline_vertex/pipeline_vertex_automl.py --output $PIPELINE_YAML"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can also use the Python SDK to compile the pipeline:\n",
    "\n",
    "```python\n",
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=create_pipeline, \n",
    "    package_path=PIPELINE_YAML,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the pipeline file. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!head {PIPELINE_YAML}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"automl_fraudfinder_kfp_pipeline\",\n",
    "    template_path=PIPELINE_YAML,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa28be6-9dc8-406d-aa45-babbd2df37a3",
   "metadata": {},
   "source": [
    "# Function Calling with Gemini \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Learn about function calling and relevant use cases\n",
    "1. Learn how to implement function calling with Gemini Pro\n",
    "1. Learn patterns for handling function calls in a chat session\n",
    "1. Learn how function calling can be used in different situations and use cases \n",
    "\n",
    "Function calling allows developers to define custom functions and provide these functions to Gemini. While processing a query, Gemini can choose to delegate certain data processing tasks to these functions. Gemini does not call these functions, rather it provides structured data output that includes the name of a selected function and the arguments the function should be called with. You can use this output to perform tasks like invoking external APIs, performing mathematical computations, extracting structured data, and more. You can then provide the function response back to the model, allowing it to complete its answer to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15fc4e-2284-4c1d-bbc1-e7c855e9dd3b",
   "metadata": {},
   "source": [
    "<img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/multimodal/images/function-calling.png\" alt=\"Function Calling\" class=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9e8ee-e3f0-4430-898b-43f3db4c23d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "\n",
    "from google import genai\n",
    "from google.cloud import bigquery\n",
    "from google.genai.types import (\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    GenerateContentResponse,\n",
    "    Part,\n",
    "    Schema,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39feb35-b724-4ec1-baea-2ade785c1758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "\n",
    "MODEL = \"gemini-2.0-flash-001\"\n",
    "\n",
    "client = genai.Client(vertexai=True, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484081e-ac89-48cf-b092-290297943f57",
   "metadata": {},
   "source": [
    "## Chat Session with Function Calling\n",
    "First, lets think about how function calling can be implemented within a chat session. Essentially, when the model returns a function call, instead of returning to the user, we need to invoke a Python function that executes the specified function with the provided arguments, then feeds the result back into the model. This may happen multiple times (e.g. model returns function call -> function call response fed back into model -> model returns another function call -> ... ). \n",
    "\n",
    "Our goal is to create a simple class for example chat sessions, that implements a reasoning loop in its `send_message` method. The class should be instantiated with:\n",
    "1) `tools`: A list of `Tool`s\n",
    "2) `tool_handler_fn`. A Python callable that accepts the function call name (str) and the function call arguments (dict) when invoked. This function should implement the logic of the function call itself and return the result.\n",
    "\n",
    "For example, if we had a tool with function calls for reading from and writing to a database, then we may have a `tool_handler_fn` that looks like:\n",
    "\n",
    "```python\n",
    "def tool_handler_fn(fn_name, fn_args):\n",
    "    \"\"\"This assumes function call read_row has parameter row_id, and function call write_row has parameter row\"\"\"\n",
    "    if fn_name == \"read_row\":\n",
    "        result = db.read_row(fn_args[\"row_id\"])\n",
    "    elif fn_name == \"write_row\":\n",
    "        result = db.write_row(fn_args[\"row\"])\n",
    "    return result \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e76206-c1bf-4351-a2e8-71c9481a9cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChatAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tools: list[Tool],\n",
    "        tool_handler_fn: Callable[[str, dict], Any],\n",
    "        max_iterative_calls: int = 5,\n",
    "    ):\n",
    "        self.tools = tools\n",
    "        self.tool_handler_fn = tool_handler_fn\n",
    "        self.chat_session = client.chats.create(\n",
    "            model=MODEL,\n",
    "            config=GenerateContentConfig(tools=tools),\n",
    "        )\n",
    "        self.max_iterative_calls = 5\n",
    "\n",
    "    def send_message(self, message: str) -> GenerateContentResponse:\n",
    "        response = self.chat_session.send_message(message)\n",
    "        # This is None if a function call was not triggered\n",
    "        fn_calls = response.function_calls\n",
    "\n",
    "        num_calls = 0\n",
    "        # Reasoning loop. If fn_calls is empty then we never enter this\n",
    "        # and simply return the response\n",
    "        while fn_calls:\n",
    "            if num_calls > self.max_iterative_calls:\n",
    "                break\n",
    "\n",
    "            # Handle the function calls\n",
    "            fn_call_responses = []\n",
    "            for fn_call in fn_calls:\n",
    "                response = self.tool_handler_fn(\n",
    "                    fn_call.name, dict(fn_call.args)\n",
    "                )\n",
    "                fn_call_responses.append(\n",
    "                    Part.from_function_response(\n",
    "                        name=fn_call.name,\n",
    "                        response={\n",
    "                            \"content\": response,\n",
    "                        },\n",
    "                    ),\n",
    "                )\n",
    "                num_calls += 1\n",
    "\n",
    "            # Send the function call result back to the model\n",
    "            response = self.chat_session.send_message(fn_call_responses)\n",
    "\n",
    "            # If the response is another function call then we want to\n",
    "            # stay in the reasoning loop and keep calling functions.\n",
    "            fn_calls = response.function_calls\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a82803-7dd7-4d77-b048-b30905252c4c",
   "metadata": {},
   "source": [
    "## Simple API Example\n",
    "Now that we have a way to use function calling in a chat session, let's implement some common use cases. Imagine you want to call an API to get the current weather for a specific location, when a user asks for it. This requires a mechanism to identify that the current weather is being asked for, and also to extract the location required in the API request. \n",
    "\n",
    "With function calling, this is fairly straightforward. Simply define a function declaration with the intent and required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78242dc8-1910-4b76-ae0b-1de2a22b32de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_weather_func = FunctionDeclaration(\n",
    "    name=\"current_weather\",\n",
    "    description=\"Get the current weather at a specified location\",\n",
    "    parameters=Schema(\n",
    "        type=\"OBJECT\",\n",
    "        properties={\n",
    "            \"location\": Schema(\n",
    "                type=\"STRING\",\n",
    "                description=\"Location\",\n",
    "            ),\n",
    "        },\n",
    "        required=[\"location\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Simulate a function that calls a weather API\n",
    "\n",
    "\n",
    "def current_weather(location: str) -> dict:\n",
    "    print(\"Executing current_weather function...\")\n",
    "    api_response = {\n",
    "        \"location\": \"New York City\",\n",
    "        \"temperature\": \"55 degrees (F)\",\n",
    "        \"wind\": \"8 mph\",\n",
    "        \"wind_direction\": \"West\",\n",
    "        \"skies\": \"clear/sunny\",\n",
    "        \"chance_of_rain\": \"0%\",\n",
    "    }\n",
    "    return api_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76fcb03-c63a-4834-b6bc-52c06ec5774c",
   "metadata": {},
   "source": [
    "Instantiate a `Tool` with the single function declaration, and then write a tool handler function to invoke when the model returns a function call. Then instantiate the model with the `Tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f9659-3217-4f7b-8ef8-ab7b47db2f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tools can wrap around one or multiple functions\n",
    "weather_tool = Tool(\n",
    "    function_declarations=[current_weather_func],\n",
    ")\n",
    "\n",
    "# Instantiate chat with weather tool\n",
    "chat = client.chats.create(\n",
    "    model=MODEL, config=GenerateContentConfig(tools=[weather_tool])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749dca7-1d9f-4e13-9ada-5577a1558a0f",
   "metadata": {},
   "source": [
    "Send a chat through the model without using `ChatAgent` to see what the response of a function call looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d56ff-d6b2-403b-8b74-2c3fd56dfbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\"What is the weather like in New York City?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f8dab-7c37-4ed0-9f32-7905b513d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.function_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a5a9f-159c-4c76-b783-5bdf84007a4a",
   "metadata": {},
   "source": [
    "Notice how instead of returning a text response, Gemini returned the function name to call and arguments to call it with. Now implement a function that we can instantiate `ChatAgent` with, that we will pass the function name and arguments to any time Gemini returns a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cbef4-6f11-43b3-8a0c-44251f6e627e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weather_tool_handler_fn(fn_name: str, fn_args: dict) -> dict:\n",
    "    if fn_name == \"current_weather\":\n",
    "        return current_weather(fn_args[\"location\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown function call: {fn_name}\")\n",
    "\n",
    "\n",
    "chat = ChatAgent(tools=[weather_tool], tool_handler_fn=weather_tool_handler_fn)\n",
    "response = chat.send_message(\"What is the weather like in New York City?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35158569-c82b-4b40-acbf-2d24e42c22e3",
   "metadata": {},
   "source": [
    "If we take a look at the chat history, we can see that a function call was returned, our handler function was invoked, which then invoked the Python function simulating an API. The response from that was then sent back into the model and incorporated in its response about the weather!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260763e-a73b-4f76-ac3e-4e3cf7aede1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat.chat_session.get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17572e-18b3-4be9-a2a8-455751271690",
   "metadata": {},
   "source": [
    "## Function calling to perform mathematical operations\n",
    "Function calling can also help in an area that LLMs have long struggled - mathematics. Language models build up deep and insightful representations of natural language, but often lack the ability to (correctly and consistently) perform mathematical operations. We can provide a degree of consistency and accuracy by creating a tool that identifies when a mathematical operation is needed, and calls a function to actually perform that operation. \n",
    "\n",
    "Create function declarations for simple mathematical operations (addition, subtraction, multiplication, division).\n",
    "\n",
    "#### Exercise\n",
    "Implement the four mathematical functions to invoke when a function call is triggered. You need to write functions to add, subtract, multiply and divide numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd781f3b-3751-4684-9908-79ec3bc0395d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = Schema(\n",
    "    type=\"OBJECT\",\n",
    "    properties={\n",
    "        \"first_number\": Schema(\n",
    "            type=\"NUMBER\",\n",
    "            description=\"First number\",\n",
    "        ),\n",
    "        \"second_number\": Schema(\n",
    "            type=\"NUMBER\",\n",
    "            description=\"Second_number number\",\n",
    "        ),\n",
    "    },\n",
    "    required=[\"first_number\", \"second_number\"],\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Create function declarations for core math operations\n",
    "add_two_numbers_func = None\n",
    "subtract_two_numbers_func = None\n",
    "multiply_two_numbers_func = None\n",
    "divide_two_numbers_func = None\n",
    "\n",
    "math_tool = Tool(\n",
    "    function_declarations=[\n",
    "        add_two_numbers_func,\n",
    "        subtract_two_numbers_func,\n",
    "        multiply_two_numbers_func,\n",
    "        divide_two_numbers_func,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca48def-6b12-414d-9ee6-ad2432cd7b0e",
   "metadata": {},
   "source": [
    "Instead of simulating the response from functions, lets actually write the Python functions that we will call with arguments provided when Gemini responds with a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86df3a6-b282-46d1-a706-e70b60a0ea7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define functions for each function declaration used in the math tool\n",
    "add_two_numbers = lambda a, b: a + b\n",
    "subtract_two_numbers = lambda a, b: a - b\n",
    "multiply_two_numbers = lambda a, b: a * b\n",
    "divide_two_numbers = lambda a, b: a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7548d-aa0e-4713-a3de-683156fa4aa1",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "\n",
    "Implement a handler using if else statemnets to route function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c94689-212c-403f-bcaa-27bb72e67440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_math_fn_call(fn_name: str, fn_args: dict) -> Union[int, float]:\n",
    "    \"\"\"Handles math tool function calls.\"\"\"\n",
    "\n",
    "    print(f\"Function calling: {fn_name} with args: {fn_args}\")\n",
    "    a = fn_args[\"first_number\"]\n",
    "    b = fn_args[\"second_number\"]\n",
    "\n",
    "    # TODO: Complete this function to handle different function calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53a1dc-5297-45d2-98f1-d9edf83bdf58",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Exercise:** Instantiate a model, chat agent, and test out some queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de912d28-2a6b-422c-bed0-842661fe737d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ChatAgent(\n",
    "    tools=[None], tool_handler_fn=handle_math_fn_call  # TODO: add the tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83ea2e-8a44-489a-bfb0-6853050952d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\"What is one plus one?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ad122-21e8-4517-b187-333ff4419266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\"Thanks! What is (5 * 5) / (4 + 1)?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86bcaf-6793-48b2-8190-2cb21e407085",
   "metadata": {},
   "source": [
    "Notice how Gemini called more than one function, sequentially and logically, in order to answer the question. Very cool! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4549b-38e9-45ac-bb38-4dadad2fd5d9",
   "metadata": {},
   "source": [
    "## Natural Language to SQL with Database Execution \n",
    "Function calling can be helpful with systems that require SQL generation and execution, and using the response to answer a query. Start by creating a dataset in BigQuery and cloning some public tables into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a98056-225e-4f8a-959f-110be2d71d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "!bq mk --location=\"US\" iowa_liquor_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce29c24-6f68-4f7f-b758-1e4dfe1303f8",
   "metadata": {},
   "source": [
    "Create the table by querying the public data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3ef85-9b49-4c0c-b6b8-db762536b43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE iowa_liquor_sales.sales AS \n",
    "SELECT * FROM `bigquery-public-data.iowa_liquor_sales.sales`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916dcf7-ba93-4eeb-a15e-47ab8a0675e0",
   "metadata": {},
   "source": [
    "Update the schema of your table to include column definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71c54f-0b5e-4a18-b94a-4a57b0ad0ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCHEMA_FILE = \"liquor_sales_schema.json\"\n",
    "!bq show --schema --format=prettyjson bigquery-public-data:iowa_liquor_sales.sales > {SCHEMA_FILE}\n",
    "!bq update {PROJECT}:iowa_liquor_sales.sales {SCHEMA_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78089e35-7e94-4f13-92d7-5a3ba748e983",
   "metadata": {},
   "source": [
    "Create function declarations and instantiate a new `Tool`. The function declarations should be:\n",
    "1) Listing available tables `list_available_tables`\n",
    "2) Retrieving information and schema about a specific table `get_table_info`\n",
    "3) Retrieves information from BigQuery to answer a users question `sql_query`\n",
    "\n",
    "#### Exercise \n",
    "1. Implement the `sql_query_func` function declaration with one parameter \"query\". Provide a description for this query so that it only executes the query if the table is in the project.\n",
    "2. Implement a tool that gathers all the function declarations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a7307-ac28-4140-84f4-457fd2763ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since we only have one table we will just hardcode the response from this function if triggered\n",
    "list_available_tables_func = FunctionDeclaration(\n",
    "    name=\"list_available_tables\",\n",
    "    description=\"Get and list all available BigQuery tables with fully qualified IDs.\",\n",
    "    parameters=Schema(type=\"OBJECT\", properties={}),\n",
    ")\n",
    "\n",
    "get_table_info_func = FunctionDeclaration(\n",
    "    name=\"get_table_info\",\n",
    "    description=\"Get information about a BigQuery table and it's schema so you can better answer user questions.\",\n",
    "    parameters=Schema(\n",
    "        type=\"OBJECT\",\n",
    "        properties={\n",
    "            \"table_id\": Schema(\n",
    "                type=\"STRING\",\n",
    "                description=\"Fully qualified ID of BigQuery table\",\n",
    "            )\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "sql_query_func = FunctionDeclaration(\n",
    "    name=\"sql_query\",\n",
    "    description=\"Get information from data in BigQuery using SQL queries\",\n",
    "    parameters=None,  # TODO: Define one parameter \"query\". Provide a description for this query so that it only executes the query if the table is in the project.\n",
    ")\n",
    "\n",
    "# TODO: Instantiate the query tool using the above function declarations\n",
    "query_tool = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a4a422-a4ba-4d74-8e70-fd093516bddf",
   "metadata": {},
   "source": [
    "Now we need to create Python functions that will be executed when the model returns any of these function calls. The functions should be implement as such:\n",
    "* `list_available_tables` should accept no parameters and simply return the name of the BigQuery table created above: `f\"{PROJECT}/iowa_liquor_sales.sales\"`\n",
    "* `get_table_info` should accept a table_id parameter and use the BigQuery client library to retrieve table information and schema \n",
    "* `sql_query` should accept a query_sting parameter and use the BigQuery client library to execute a sql query and return the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640741b4-c326-44ff-9c3b-c2483cacf540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_available_tables():\n",
    "    return [f\"{PROJECT}.iowa_liquor_sales.sales\"]\n",
    "\n",
    "\n",
    "def get_table_info(table_id: str) -> dict:\n",
    "    \"\"\"Returns dict from BigQuery API with table information\"\"\"\n",
    "    bq_client = bigquery.Client()\n",
    "    return bq_client.get_table(table_id).to_api_repr()\n",
    "\n",
    "\n",
    "def sql_query(query_str: str):\n",
    "    bq_client = bigquery.Client()\n",
    "    try:\n",
    "        # clean up query string a bit\n",
    "        query_str = (\n",
    "            query_str.replace(\"\\\\n\", \"\").replace(\"\\n\", \"\").replace(\"\\\\\", \"\")\n",
    "        )\n",
    "        # print(query_str)\n",
    "        query_job = bq_client.query(query_str)\n",
    "        result = query_job.result()\n",
    "        result = str([dict(x) for x in result])\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error from BigQuery Query API: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b2c1f3-2434-44ec-9935-e8c947c7d58b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Exercise:** Create a Python function to handle function calls returned by the model and invoke the needed logic. Use if else statements to map the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4321e6-7880-4ebb-852c-884d10e308e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_query_fn_call(fn_name: str, fn_args: dict):\n",
    "    \"\"\"Handles query tool function calls.\"\"\"\n",
    "\n",
    "    print(f\"Function calling: {fn_name} with args: {str(fn_args)}\\n\")\n",
    "    # TODO: If-else statements to map the fn_name to the right function call\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f56cc2-7f04-4344-9e56-1873d9f940dc",
   "metadata": {},
   "source": [
    "Instantiate a model, chat agent, and test out some queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b127d-a44d-4005-a89f-e2b5df974813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ChatAgent(tools=[query_tool], tool_handler_fn=handle_query_fn_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b495a24-9dd6-4ca8-b0c8-c6b1127761b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert an initialization prompt before the first chat to help guide model behavior and output style/format\n",
    "\n",
    "init_prompt = \"\"\"\n",
    "    Please give a concise and easy to understand answer to any questions. \n",
    "    Only use information that you learn by querying the BigQuery table. \n",
    "    Do not make up information. Be sure to look at which tables are available \n",
    "    and get the info of any relevant tables before trying to write a query. \n",
    "    \n",
    "    Question:\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Which store has sold the most bottles of all time?\"\n",
    "response = chat.send_message(init_prompt + prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768dd419-6879-461f-bc98-61f576b48c58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    \"Interesting! What is the most popular bottle of all time?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d546ab-1a5b-4a48-b534-eb120f068049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    \"What are the five most popular bottles in polk county?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1c500-9a9b-46e4-8e56-dbd431593d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    \"What vendors have made the most revenue selling liquor?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32be2bc-4cee-43f6-8323-41e1a5e41668",
   "metadata": {},
   "source": [
    "Feel free to execute the generated SQL to verify/validate the responses! An easy way to do this is in a code cell with `%%bigquery` at the top. For example:\n",
    "\n",
    "```\n",
    "%%bigquery\n",
    "SELECT ... \n",
    "FROM ... \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e8f47-54f8-460b-8fd3-3ddff28f2f84",
   "metadata": {},
   "source": [
    "## Function calling for entity extraction \n",
    "In the previous examples we used entity extraction to pass parameters along to another function, API, or client library. However, you might want to only perform the entity extraction step, and stop there without actually invoking anything else. You can think of this functionality as a convenient way to transform unstructured text data into structured fields.\n",
    "\n",
    "For example, we can easily build a log extractor that transforms raw logs into structured data with details about error messages.\n",
    "\n",
    "Start by specifying the function declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e8e13-3295-48b8-aa9e-c42a5ffcd01e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract_log_data_func = FunctionDeclaration(\n",
    "    name=\"extract_log_data\",\n",
    "    description=\"Extracts specific details from errors in log data\",\n",
    "    parameters=Schema(\n",
    "        type=\"OBJECT\",\n",
    "        properties={\n",
    "            \"errors\": Schema(\n",
    "                type=\"ARRAY\",\n",
    "                description=\"Errors\",\n",
    "                items=Schema(\n",
    "                    type=\"OBJECT\",\n",
    "                    description=\"Details of the error\",\n",
    "                    properties={\n",
    "                        \"error_message\": Schema(\n",
    "                            type=\"STRING\",\n",
    "                            description=\"Full error message\",\n",
    "                        ),\n",
    "                        \"error_code\": Schema(\n",
    "                            type=\"STRING\",\n",
    "                            description=\"Error code\",\n",
    "                        ),\n",
    "                        \"error_type\": Schema(\n",
    "                            type=\"STRING\",\n",
    "                            description=\"Error type\",\n",
    "                        ),\n",
    "                    },\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "error_extraction_tool = Tool(\n",
    "    function_declarations=[extract_log_data_func],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb296121-24ba-4cb0-a98d-3d5e808c35f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "[15:43:28] ERROR: Could not process image upload: Unsupported file format. (Error Code: 308)\n",
    "[15:44:10] INFO: Search index updated successfully. \n",
    "[15:45:02] ERROR: Service dependency unavailable (payment gateway). Retrying... (Error Code: 5522) \n",
    "[15:45:33] ERROR: Application crashed due to out-of-memory exception. (Error Code: 9001) \n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    config=GenerateContentConfig(tools=[error_extraction_tool]),\n",
    "    contents=prompt,\n",
    ")\n",
    "function_calls = response.function_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035deb00-eb2c-4045-bf30-444da13c4e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for func_call in function_calls:\n",
    "    for err in dict(func_call.args).get(\"errors\"):\n",
    "        print(dict(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588e750-d149-4760-88a4-01c96b6614a6",
   "metadata": {},
   "source": [
    "Function calling is an incredibly versatile tool! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0551b-300d-4924-bef6-631644e0bed4",
   "metadata": {},
   "source": [
    "## (Optional) Function Calling with LangChain\n",
    "Let's see how we can implement function calling using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bce224-448f-463c-8b1d-3535ef4a6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "chat = ChatVertexAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79c391-7c73-49b8-b5f9-c362aa2694fd",
   "metadata": {},
   "source": [
    "To implement function calling in LangChain, you utilize LangChain `tools`. These tools are defined by schemas, which can be created from various Python constructs: regular functions (including type hints and docstrings), [Pydantic models](https://python.langchain.com/docs/how_to/tool_calling/#pydantic-class), [TypedDict classes](https://python.langchain.com/docs/how_to/tool_calling/#typeddict-class), or dedicated [LangChain Tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#basetool) instances. The following example illustrates how to define a tool using a Python function, each function is decorated with the `@tool` decorator.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef24f9-d648-461b-aa45-184d8d1d44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract second_number from first_number.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"Divide first_number by second_number.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7117bfd-9881-4633-b976-cf66270b2564",
   "metadata": {},
   "source": [
    "In Python function, you can further customize it by adding `@tool` decorator. Please check [the document](https://python.langchain.com/docs/how_to/custom_tools/#tool-decorator) for more details.\n",
    "\n",
    "Now let's bind the tools with `.bind_tools()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2fa43-19ff-48f9-91aa-611956027de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, subtract, multiply, divide]\n",
    "\n",
    "chat_with_tools = chat.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3faa4a0-779b-4a34-881d-e344938ce56c",
   "metadata": {},
   "source": [
    "Now you can invoke the model and see the response from the model.<br>\n",
    "The response contains the `.tool_calls` when function calling is requested by the model.\n",
    "\n",
    "Here we manage the chat history in `message` list, but please consider using LangGraph when building a complex agent system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e225c-4575-46cf-be5b-12e3c3aedcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is (5 * 5) / (4 + 1)?\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "response = chat_with_tools.invoke(messages)\n",
    "messages.append(response)\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e2acc-4adf-4506-bad5-a856d671c336",
   "metadata": {},
   "source": [
    "Let's call each function and appned to the `message`. Please note the response from tools are wrapped in a [`ToolMessage`](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.tool.ToolMessage.html) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70298001-cf6c-4914-86ad-dbbe0880d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool_call in response.tool_calls:\n",
    "    selected_tool = {\n",
    "        \"add\": add,\n",
    "        \"subtract\": subtract,\n",
    "        \"multiply\": multiply,\n",
    "        \"divide\": divide,\n",
    "    }[tool_call[\"name\"]]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2946f7-ca40-48a8-972b-2ff411e5b0b6",
   "metadata": {},
   "source": [
    "Now we provide the responses from functions to the LLM model and call it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68d501-1933-46ef-bf60-170b8803d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_with_tools.invoke(messages)\n",
    "messages.append(response)\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd9040-eba2-41d1-ae4e-0e96a3b179a7",
   "metadata": {},
   "source": [
    "The model responded with another function call. Let's repeat the same process again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506a6f4-59cc-4838-8352-4cf24875bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool_call in response.tool_calls:\n",
    "    selected_tool = {\n",
    "        \"add\": add,\n",
    "        \"subtract\": subtract,\n",
    "        \"multiply\": multiply,\n",
    "        \"divide\": divide,\n",
    "    }[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "chat_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d6bc2c-2c21-4ffb-b5c5-1045b4893f5b",
   "metadata": {},
   "source": [
    "Now we successfully got the final response!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ddcf33-329d-49e1-895f-f28526806ab2",
   "metadata": {},
   "source": [
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450aa40-5443-457f-99e6-cd1f71b34d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240b3a7d-6ed7-422f-aa2e-0117ecbd1a22",
   "metadata": {},
   "source": [
    "## Creating TFRecords for Image Data at Scale\n",
    "In this lab you will create TFRecords for an image classification dataset, and you will develop an Apache Beam pipeline that creates TFRecords for image data, and submit a Dataflow job to create these TFRecords at scale.\n",
    "\n",
    "Learning Objectives:\n",
    "* Learn how to serialize image data to TFRecords at scale\n",
    "* Learn how to extend `beam.DoFn` for creating `tf.Example` protos.\n",
    "* Learn how to use `beam.io.tfrecordio.WriteToTFRecord` to write TFRecords from a Beam pipeline\n",
    "* Learn how to deploy an Apache Beam pipeline to Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f0fe9-1356-4d83-971c-885907cec47f",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf89e2-22ba-4e8e-a92b-7d6c2fa0a702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efcffc-9bfc-4561-966b-3392ef63081f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT + \"-flowers\"\n",
    "REGION = \"us-central1\"\n",
    "CLASSES = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "DATASET_FILE = f\"gs://{BUCKET}/flowers.csv\"\n",
    "\n",
    "%env PROJECT={PROJECT}\n",
    "%env BUCKET={BUCKET}\n",
    "%env REGION={REGION}\n",
    "%env DATASET_FILE={DATASET_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de5815-ca1a-4f53-8f05-fc20a832e968",
   "metadata": {},
   "source": [
    "We will be using GCS to store the data. Run the following cell to create this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0dbb38-3bcd-46bc-b2bc-c31408341eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "\n",
    "if [ -n \"$exists\" ]; then\n",
    "  echo -e \"Bucket gs://${BUCKET} already exists.\"\n",
    "    \n",
    "else\n",
    "   echo \"Creating a new GCS bucket.\"\n",
    "   gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "   echo -e \"\\nHere are your current buckets:\"\n",
    "   gsutil ls\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48cda4-6236-4608-a42e-7fd05d8da6cd",
   "metadata": {},
   "source": [
    "Copy the dataset CSV file to your GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992758a0-305e-41a7-8bbd-2597354ee0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp ../dataset_import_files/flowers.csv {DATASET_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef9a9eb-0151-493c-87ce-68b2434a27b3",
   "metadata": {},
   "source": [
    "#### Look at Dataset Import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13474553-120a-41b2-9745-7563c2c15a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = pd.read_csv(DATASET_FILE)\n",
    "examples.columns = [\"imageUri\", \"label\"]\n",
    "examples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ae8e2-9185-416b-b823-4bd98cc887b9",
   "metadata": {},
   "source": [
    "As you can see, this Image Classification dataset is represented as a CSV file with pattern `Image URI, Label`. The Image URIs point to jpeg images, stored in a publically available GCS bucket. This file will be used as input to a Beam pipeline that creates `.tfrecord` files of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd902e56-95d4-4cb1-acd0-900169fb50a5",
   "metadata": {},
   "source": [
    "#### Look at one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746adcf-e1ee-4685-9076-4b5254bf46e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "single_example = examples.iloc[0]\n",
    "img = tf.io.decode_jpeg(tf.io.read_file(single_example.imageUri))\n",
    "plt.imshow(img)\n",
    "plt.title(single_example.label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69474e-5857-4940-a75c-55956c1af22e",
   "metadata": {},
   "source": [
    "### Use Apache Beam and Dataflow to Create TFRecords at scale\n",
    "With image data, it is common to have very large datasets. In these situations converting images to TFRecords locally isn't feasible. We can leverage the massive scalability of Cloud Dataflow to do this for us.\n",
    "\n",
    "Dataflow is an execution engine for Apache Beam, so we need to create a Beam pipeline to read from a CSV file and serialize the entire dataset to TFRecords. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece27a9-7862-48cd-b9fd-5c3ca4dc85f0",
   "metadata": {},
   "source": [
    "#### Apache Beam Pipeline\n",
    "This Apache Beam pipeline will do the following\n",
    "* Read in rows from a CSV file formatted as `imageURI, label`\n",
    "* Create TF Examples with a custom `beam.DoFn`\n",
    "* Randomly split the data into train and validation with `beam.Partition`\n",
    "* Serialize Examples to TFRecords\n",
    "* Write data out to GCS with `beam.io.tfrecordio.WriteToTFRecord`\n",
    "\n",
    "All of the pipeline code will be written to the same file, but it broken up across multiple cells for simplicity. \n",
    "\n",
    "First, define the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c0a8af-bd0b-4a16-bcfa-5cdd5f959c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p create_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1eb3c-2c8c-4ac0-b947-62ed3cc75d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile create_tfrecords/create_tfrecords.py\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import (\n",
    "    GoogleCloudOptions, \n",
    "    PipelineOptions, \n",
    "    StandardOptions, \n",
    "    SetupOptions\n",
    ")\n",
    "\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "import argparse\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d938c53-8c13-4f15-84b5-835e064eb89d",
   "metadata": {},
   "source": [
    "Define `ParseCsv` (a custom `DoFn`) to use the `str.split()` method to split each row and return a `NamedTuple` of the image URI and label for each example in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b73b7-1332-4e0d-9223-10743a521104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile -a create_tfrecords/create_tfrecords.py\n",
    "\n",
    "# Schema of CSV file\n",
    "class CSVRow(typing.NamedTuple):\n",
    "    image_uri: str\n",
    "    label: str\n",
    "\n",
    "# DoFn to transform CSV rows to PCollection with schema\n",
    "class ParseCsv(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        image_uri, label = element.split(',')\n",
    "        yield CSVRow(\n",
    "            image_uri = image_uri,\n",
    "            label = label\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df983192-ad7b-4b8b-97c0-490fa1da20dd",
   "metadata": {},
   "source": [
    "Define `CreateTFExample` and helper functions to serialize image data to TF Examples. Also, define a function to randomly split data for train/test splits. Note: random splitting is not repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56047b1-651e-40dd-a5fd-32eeedcac399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile -a create_tfrecords/create_tfrecords.py\n",
    "\n",
    "# TFRecord Helper Functions\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from an image.\"\"\"\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n",
    "    )\n",
    "\n",
    "# DoFn to create TF Example's\n",
    "class CreateTFExample(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "        img = tf.io.decode_jpeg(tf.io.read_file(element.image_uri))\n",
    "        \n",
    "        feature = {\n",
    "            \"image\": _image_feature(img), \n",
    "            \"label\": _int64_feature(CLASSES.index(element.label)),\n",
    "        }\n",
    "        \n",
    "        yield tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "def partition_fn(example, num_partitions, train_percent):\n",
    "    if random.random() < train_percent:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4427d05-88e4-483b-9fa5-ad0570a2231d",
   "metadata": {},
   "source": [
    "Define the `run` function. This function will parse command line arguments, set pipeline options, then compose the Beam pipeline itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974af855-91f3-4659-8473-36d241cd6ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile -a create_tfrecords/create_tfrecords.py\n",
    "    \n",
    "# Function to run the Beam pipeline\n",
    "def run():\n",
    "    parser = argparse.ArgumentParser(description='Image Data to TFRecords')\n",
    "\n",
    "    # Google Cloud options\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--job_name', required=True, help='Job name for Dataflow Runner')\n",
    "\n",
    "    # Pipeline-specific options\n",
    "    parser.add_argument('--dataset_file', required=True, help='GCS path to input CSV')\n",
    "    parser.add_argument('--output_dir', required=True, help='GCS output directory')\n",
    "    parser.add_argument('--train_percent', required=True, help='Percentage of training data')\n",
    "    parser.add_argument('--requirements_file', required=True, help='Required Packages')\n",
    "\n",
    "\n",
    "    opts, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options.\n",
    "    options = PipelineOptions(pipeline_opts)\n",
    "\n",
    "    DATASET_FILE = opts.dataset_file\n",
    "    OUTPUT_DIR = opts.output_dir\n",
    "    TRAIN_PERCENT = float(opts.train_percent)\n",
    "    \n",
    "    # Set standard pipeline options.\n",
    "    options.view_as(StandardOptions).streaming = False\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "    options.view_as(SetupOptions).save_main_session = True\n",
    "    options.view_as(SetupOptions).requirements_file = opts.requirements_file\n",
    "    options.view_as(SetupOptions).pickle_library = \"dill\"\n",
    "\n",
    "    # Set Google Cloud specific options.\n",
    "    google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "    google_cloud_options.project = opts.project\n",
    "    google_cloud_options.job_name = opts.job_name\n",
    "    google_cloud_options.staging_location = opts.staging_location\n",
    "    google_cloud_options.region = opts.region\n",
    "\n",
    "\n",
    "    # Instaniate pipeline\n",
    "    if opts.runner=='DataflowRunner':\n",
    "        p = beam.Pipeline(DataflowRunner(), options=options)\n",
    "    else:\n",
    "        p = beam.Pipeline(DirectRunner(), options=options)\n",
    "        \n",
    "    rows = ( p | \"Read CSV\" >> beam.io.ReadFromText(DATASET_FILE)\n",
    "               | \"Parse CSV\" >> beam.ParDo(ParseCsv()))\n",
    "    \n",
    "    train, val = ( rows \n",
    "                   | \"Create TF Examples\" >> beam.ParDo(CreateTFExample()) \n",
    "                   | \"Split Data\" >> beam.Partition(partition_fn, 2, train_percent=TRAIN_PERCENT))\n",
    "    \n",
    "    write_train = ( train \n",
    "                       | \"Serialize Training Examples\" >> beam.Map(lambda x: x.SerializeToString()) \n",
    "                       | \"Write Train\" >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                           f\"{OUTPUT_DIR}/train.tfrecord\", num_shards=10\n",
    "                       )\n",
    "                  )\n",
    "    write_val = ( val \n",
    "                       | \"Serialize Validation Examples\" >> beam.Map(lambda x: x.SerializeToString()) \n",
    "                       | \"Write Validation\" >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                           f\"{OUTPUT_DIR}/eval.tfrecord\", num_shards=3\n",
    "                       )\n",
    "                )\n",
    "    \n",
    "    # Run pipeline\n",
    "    p.run()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799e77e-0f23-453a-96ad-ae02c1dbb0e6",
   "metadata": {},
   "source": [
    "### A deeper look at this pipeline\n",
    "\n",
    "#### Apache Beam Core Concepts\n",
    "`Pcollection`: An immutable collections of values representing data elements.\n",
    "\n",
    "`PTransform`: Represents a data processing operation, or a step, in your pipeline.\n",
    "\n",
    "`ParDo`: A transform for generic parallel processing. A `ParDo` transform considers each element in the input `PCollection`, performs some processing function on that element, and emits elements to an output `PCollection`. The processing function passed to ParDo is a `DoFn` object.\n",
    "\n",
    "`DoFn`: These are what define your pipeline's exact data processing tasks. To create a custom processing task, create a `DoFn` subclass (e.g. `class MyCustomProcessingTask(beam.DoFn)`) and write a method `def process(self, element):` where you provide the actual processing logic. You don't need to manually extract the elements from the input collection; the Beam SDKs handle that for you. Your `process` method should accept an argument `element`, which is the input element, and return an iterable with its output values. You can accomplish this by emitting individual elements with `yield` statements. \n",
    "\n",
    "#### Image data to TFRecord Pipeline\n",
    "1) `beam.io.ReadFromText`: A `PTransform` for reading text files into `str` elements. It returns one element for each line the file. With the input CSV file for our image dataset, it will return a string element `\"{imageUri}, {label}\"` for each row in the CSV.\n",
    "\n",
    "2) `beam.ParDo(ParseCsv())`: A `PTransform` that applies `ParseCsv` (a custom `DoFn`), to each string element in the output `PCollection` from `beam.io.ReadFromText`. The `process` method for `ParseCsv` simply uses the `str.split()` method to split each row and return a `NamedTuple` of the image URI and label for each example in the dataset.\n",
    "\n",
    "3) `beam.ParDo(CreateTFExample())`. A `PTransform` that applies `CreateTFExample()` (a custom `DoFn`), to each element in the output `PCollection` from `beam.ParDo(ParseCsv())`. The `process` method for `CreateTFExample()` first reads and decodes the image from the image URI with `tf.io.read_file` and `tf.io.decode_jpeg`, creates a feature dictionary with our TFRecord helper functions, and returns a `tf.train.Example` proto.\n",
    "\n",
    "4) `beam.Partition(partition_fn, 2, train_percent)`: A `PTransform` that seperates elements in a collection into multiple output collections. The partitioning function provided contains the logic that determines how to seperate the elements into each resulting partition. In our pipeline, our custom `partition_fn` simply splits the input collection into 2 partitions, a training partition if a random number is less than train_percent, or a test partition if a random number is greater than train_percent.\n",
    "\n",
    "Now at this stage of the pipeline, we have 2 `PCollections`; `train` and `val`. Each element in `train` and `val` are `tf.train.Example` protos at this point. For both of these, we need to serialize them and write them to `.tfrecord` format. We will pipe each of these `PCollections` into the following two `PTransforms`\n",
    "1) `beam.Map(lambda x: x.SerializeToString())`: A `PTransform` that applies `.SerializeToString` to each `tf.train.Example` proto element in the `PCollection`.\n",
    "\n",
    "2) `beam.io.tfrecordio.TFRecord(OUTPUT_DIR)`: A `PTransform` that writes the serialized `tf.train.Example` protos to `.tfrecord` format to a given output directory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effceaf2-6ee6-49eb-9b4f-0cac34c98b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a requirements.txt file for DataFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594fc15e-46f2-4101-8bef-3a622575fb52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile create_tfrecords/requirements.txt\n",
    "tensorflow==2.18.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfa20f-c878-4f4e-9852-843a79f71b67",
   "metadata": {},
   "source": [
    "### Run the pipeline with Cloud Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c84e6-378c-4a8c-95f2-ebe9d4decd75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "JOB_NAME = f\"images-to-tfrecords-{TIMESTAMP}\"\n",
    "OUTPUT_DIR = f\"gs://{BUCKET}/data\"\n",
    "STAGING_LOCATION = f\"gs://{BUCKET}/staging\"\n",
    "TRAIN_PERCENT = 0.8\n",
    "\n",
    "%env JOB_NAME = {JOB_NAME}\n",
    "%env OUTPUT_DIR = {OUTPUT_DIR}\n",
    "%env STAGING_LOCATION = {STAGING_LOCATION}\n",
    "%env TRAIN_PERCENT = {TRAIN_PERCENT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54008890-e445-43c9-8e9e-0f9a04e66e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 create_tfrecords/create_tfrecords.py \\\n",
    "    --project=${PROJECT} \\\n",
    "    --region=${REGION} \\\n",
    "    --runner='DataflowRunner' \\\n",
    "    --job_name=${JOB_NAME} \\\n",
    "    --staging_location=${STAGING_LOCATION} \\\n",
    "    --dataset_file=${DATASET_FILE} \\\n",
    "    --output_dir=${OUTPUT_DIR} \\\n",
    "    --train_percent=${TRAIN_PERCENT} \\\n",
    "    --requirements_file='./create_tfrecords/requirements.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8346fb-31d1-4348-90b3-59e4ae4080f8",
   "metadata": {},
   "source": [
    "**NOTE** The Dataflow job will take about 10 minutes to complete. Feel free to watch the progression of the job in the Dataflow UI.\n",
    "The job must be complete before you can move forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad56b94-a1b4-4764-bddd-9db903f96acf",
   "metadata": {},
   "source": [
    "Now the entire dataset has been serialized to TFRecords and stored in GCS. Run the following command to see the TFRecords created. If you get a `CommandException: One of more URLs matched no objects`, the Dataflow job may not be finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e6bb3-8bcd-4cd1-a4f1-3f6dbe06aa95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil ls -l {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007eb44-33b8-4c24-8903-2e4635b36279",
   "metadata": {},
   "source": [
    "Use the `tf.data` API to ingest, parse, and display a couple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5aac98-ecce-474c-b575-bd5b0ec2c363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATTERN = OUTPUT_DIR + \"/train*\"\n",
    "\n",
    "\n",
    "def parse_example(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    return example\n",
    "\n",
    "\n",
    "ds = tf.data.TFRecordDataset(tf.io.gfile.glob(FILE_PATTERN))\n",
    "ds = ds.map(parse_example)\n",
    "\n",
    "for example in ds.take(2):\n",
    "    plt.imshow(example[\"image\"].numpy())\n",
    "    plt.title(CLASSES[example[\"label\"].numpy()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56ae5e-a82d-45a2-824c-736b369b4791",
   "metadata": {},
   "source": [
    "Congrats! You have succesfully deployed a Dataflow pipeline to serialize an entire image dataset to TFRecords at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba27e42-d7fa-4dd8-8cf4-0c8f513862d7",
   "metadata": {},
   "source": [
    "# License\n",
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941bc6b9-9473-47ca-b90a-600ba69332e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

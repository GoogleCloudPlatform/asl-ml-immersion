{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b46970-5565-49b4-ae2a-e915e90b6c2f",
   "metadata": {},
   "source": [
    "# Intro to Retrieval Augmented Generation Systems, LangChain & ChromaDB\n",
    "\n",
    "This notebook walks through building a question/answer system that retrieves information to formulate responses, effectively grounding the LLM with specific information. A pre-trained LLM, or likely even a fine-tuned LLM will not be sufficient (in and of itself) when you want a system that understands specific, possibly private data or information that was not in its training dataset.\n",
    "\n",
    "In this lab you will:\n",
    "* Learn about the different components of a retrieval augmented system\n",
    "* Build a simple retrieval augmented generation system \n",
    "* Use LangChain and ChromaDB to simplify and scale the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e8220-6d91-4938-8188-7b3d986b2845",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6855635-3a1c-4295-acb3-999a634db4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "from google import genai\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74651656-cc3b-48e8-876b-6caefcf121fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-004\"\n",
    "GENERATIVE_MODEL = \"gemini-2.0-flash-001\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fafe53-dee2-4b22-b66a-d43f0b7d6fa4",
   "metadata": {},
   "source": [
    "### Build a simple retrieval augmented generation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168271d5-86a6-451f-ab66-3dbdd902f82a",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this toy example, we want to ground an LLM on information that an off-the-shelf LLM would not know. For example, instructions left for a house sitter that will be watching two pets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d7ff0-426f-4c0b-bb58-2a6177088e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of things we want to ground the LLM on.\n",
    "information = [\n",
    "    \"Estrella is a dog\",\n",
    "    \"Finnegan is a cat\",\n",
    "    \"Finnegan gets fed five times daily. Estrella gets fed three times daily.\",\n",
    "    \"Estrella usually goes on one long walk per day, but needs to go outside every 4-6 hours\",\n",
    "    \"Please play with Finnegan for 30 minutes each day. His favorite toy is the fake mouse!\",\n",
    "]\n",
    "\n",
    "information_df = pd.DataFrame({\"text\": information})\n",
    "information_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfca1b4-0600-44fc-909e-730ab4afcd56",
   "metadata": {},
   "source": [
    "At the core of most retrieval generation systems is a vector database. A vector database stores embedded representations of information. \n",
    "\n",
    "Let's add a column to our information dataframe that is an embedded representation of the text. We will use the [Vertex AI text-embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72475f45-dfb7-47f3-a4bc-20be91658f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3e8c6-f8d5-49b2-8cd4-6d9a54ecd6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "information_df[\"vector\"] = [\n",
    "    x.values\n",
    "    for x in client.models.embed_content(\n",
    "        model=EMBEDDING_MODEL, contents=information\n",
    "    ).embeddings\n",
    "]\n",
    "information_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bab4e4-d701-4cfd-b0fe-71307bd5a1b5",
   "metadata": {},
   "source": [
    "Retrieval systems need a way of finding the most relevant information to answer a given query. This is done with a nearest neighbor (semantic similarity) search. Let's define a function to take in a query (text) input and return a distance metric for each text in our information. We will need to: \n",
    "* Embed the query with the same embedding model used for the information \n",
    "* Computes a distance metric between the query vector and each information vector. We will use cosine similarity, one of the many similarity measures that can be used.\n",
    "* Returns a list of distance metrics between the query vector and each information vector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082758f-f72a-43fd-bda5-3381a5ca9733",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Complete the function below so that it iterates through `information_df` and computes a similarity score between the query embedding and each information embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71088247-87d3-4aeb-a5a9-c8dc3f74f4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_and_compute_distances(query: str):\n",
    "    # Get vector for query string\n",
    "    query_embedding = (\n",
    "        client.models.embed_content(model=EMBEDDING_MODEL, contents=query)\n",
    "        .embeddings[0]\n",
    "        .values\n",
    "    )  # Query embedding\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    # TODO: Compute distances between query vector and all information vectors\n",
    "    # Each element of the returned list should be a dictionary with keys\n",
    "    # `information` and `distance`. To compute distance use\n",
    "    # scipy.spatial.distance.cosine(v1, v2)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa74a6-28e8-429b-9afc-04caf1efa71c",
   "metadata": {},
   "source": [
    "Test this function out on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfed57b-8edb-4a17-963d-e796669f3f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_and_compute_distances(query=\"What type of animal is Estrella?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda4660-ce3c-4c7e-b61b-d682ea43c9ab",
   "metadata": {},
   "source": [
    "Notice that the vector that has the lowest cosine similarity (meaning most similiar) to the vector for \"What type of animal is Estrella?\" is the vector for \"Estrella is a dog\". This highlights the core assumption that underpins retrieval augmented systems: information relevant to answering a question will be close in vector space to the question itself.\n",
    "\n",
    "Now all we have to do is write a function that incorporates the text corresponding to the closest information vectors in a prompt, then send that prompt to an LLM to answer the question with the information.\n",
    "\n",
    "Start by writing a helper function to put together this prompt. `context` will be the relevant information strings (found via nearest neighbor search) and `query` will be the query string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703aa1d-4971-44f5-8915-188584fe859a",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Complete the function below so that it implements a templated prompt, putting together a query string and the relevant context strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6444d77-92a9-4411-b233-c70e687fcfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prompt(query: str, context: list[str]):\n",
    "    # TODO: Write the prompt template\n",
    "    prompt = None\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd9157-94f2-4c34-aa6a-f7979393c9cb",
   "metadata": {},
   "source": [
    "Now put everything together in a function that \n",
    "* Embeds the query\n",
    "* Computes the distance between query vector and all information vectors \n",
    "* Gets the k most relevant information texts by sorting by distance \n",
    "* Uses the k most relevant information texts in a prompt to an LLM along with the query \n",
    "* Returns the LLM response and the information used (citations) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5d0e3-83a3-4aaf-aaf0-11543e00acda",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Complete the function below so that it implements the end to end retrieval chain.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff87db5-0ca3-4c6b-8c9f-cc47f0705010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieval_chain(query: str, k: int = 2):\n",
    "    # TODO: Compute distances for query and all information vectors\n",
    "    distances = None\n",
    "\n",
    "    # TODO: Sort the information from smallest distance to greatest distance\n",
    "    sorted_distances = None\n",
    "\n",
    "    # TODO: Get the text corresponding to the k closest vectors\n",
    "    closest_information_texts = None\n",
    "\n",
    "    # TODO: Incorporate the closest k information texts in a prompt to an LLM\n",
    "    prompt = None\n",
    "\n",
    "    # TODO: Send prompt through LLM\n",
    "    response = None\n",
    "    print(f\"Response: {response.text}\")\n",
    "    print(f\"Information used: {closest_information_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d812fae-17c7-4840-9781-44b18256116b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieval_chain(\"What type of animal is Estrella?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79964f00-2741-4d8f-a3ae-47c810b941e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieval_chain(\"How many times a day do I need to feed Finnegan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24083e3-c49a-4424-8965-0541cf4eb8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieval_chain(\"What stock should I invest in this month?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c3d6c-1a10-4e53-9f30-85c2160e874a",
   "metadata": {},
   "source": [
    "Notice that the prompt is constructed such that if a question is asked that cannot be answered from the information provided, the LLM will not try to answer it.\n",
    "\n",
    "It is also worth noting that we are arbitrarily setting k=2 (including the closest 2 information texts in the prompt). Different use cases require different k's and there is no perfect one-size-fits-all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f361a-ed4a-47bc-91a7-99ca8721a228",
   "metadata": {},
   "source": [
    "### Simplify and Scale with LangChain and Chroma\n",
    "Of course with only 5 examples of grounding information, we could easily include all five in a prompt. In other words, the extra retrieval step to identify *what* is needed in the prompt was unnessesary. Of course in the real world we may have thousands or millions of grounding information examples. Additionally as the number of grounding examples grows, simply computing a distance for every single vector is incredibly innefficient. In other words, production retrieval augmented generation systems require:\n",
    "* Scalable vector databases to store large amounts of information\n",
    "* Efficient ways of performing nearest neighbor searches \n",
    "\n",
    "Of course there are many options for a vectorstore, including managed and scalable offerings like [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview). For simplicity, in this lab we will use [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) as a vectorstore and [Langchain](https://github.com/langchain-ai/langchain) to orchestrate the retrieval system. Langchain will provide classes and methods that help simplify the steps we had to implement ourselves in the toy example above.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6113e3-d700-4e53-a599-3362935190ab",
   "metadata": {},
   "source": [
    "#### Document Loading\n",
    "\n",
    "Langchain provides classes to load data from different sources. Some useful data loaders are [Google Cloud Storage Directory Loader](https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/), [Google Drive Loader](https://python.langchain.com/docs/integrations/document_loaders/google_drive), [Recursive URL Loader](https://python.langchain.com/docs/integrations/document_loaders/recursive_url/), [PDF Loader](https://python.langchain.com/docs/integrations/document_loaders/#pdfs), [JSON Loader](https://python.langchain.com/docs/integrations/document_loaders/json/), [Wikipedia Loader](https://python.langchain.com/docs/integrations/document_loaders/wikipedia/), and [more](https://python.langchain.com/docs/integrations/document_loaders/).\n",
    "\n",
    "In this notebook we will use the Wikipedia loader to create a private knowledge base of wikipedia articles about large language models, but the overall process is similiar regardless of which document loader you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb93ade-e33f-4408-aba3-10f6c05c01eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = WikipediaLoader(query=\"Large Language Models\", load_max_docs=10).load()\n",
    "\n",
    "# Take a look at a single document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077ce8e-fb29-43e7-b7ec-313b043ee84b",
   "metadata": {},
   "source": [
    "#### Split text into chunks \n",
    "Now that we have the documents we will split them into chunks. Each chunk will become one vector in the vector store. To do this we will define a chunk size (number of characters) and a chunk overlap (amount of overlap i.e. sliding window). The perfect chunk size can be difficult to determine. Too large of a chunk size leads to too much information per chunk (individual chunks not specific enough), however too small of a chunk size leads to not enough information per chunk. In both cases, nearest neighbors lookup with a query/question embedding may struggle to retrieve the actually relevant chunks, or fail altogether if the chunks are too large to use as context with an LLM query.\n",
    "\n",
    "In this notebook we will use a chunk size of 800 chacters and a chunk overlap of 400 characters, but feel free to experiment with other sizes! Note: you can specify a custom `length_function` with `RecursiveCharacterTextSplitter` if you want chunk size/overlap to be determined by something other than Python's len function. In addition to `RecursiveCharacterTextSplitter`, [other text splitters](https://python.langchain.com/docs/how_to/#text-splitters) you can consider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8949a8f-2c7e-4220-916e-dd93abcca4a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise\n",
    "In the cell below, instantiate `RecursiveCharacterTextSplitter` to split and chunk the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff137784-b004-4a96-99db-142f9208571e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = None  # TODO: Instantiate recursive text splitter\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Look at the first two chunks\n",
    "chunks[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31316e9-273f-4ca6-a252-8de9f0e9224e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Number of documents: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52830b13-f2fe-4a28-a8a3-d529da45757f",
   "metadata": {},
   "source": [
    "#### Embed Document Chunks \n",
    "Now we need to embed the document chunks and store them in a vectorstore. For this, we can use any text embedding model, however we need to be sure to use the same text embedding model when we embed our queries/questions at prediction time. To make things simple we will use the Gemini API for Embeddings. The langchain library provides a nice wrapper class around the Gemini Embeddings API, VertexAIEmbeddings().\n",
    "\n",
    "Since Vertex AI Vector Search takes awhile (~45 minutes) to create an index, we will use Chroma instead to keep things simple. Of course, in a real-world use case with a large private knowledge-base, you may not be able to fit everything in memory. Langchain has a nice wrapper class for Chroma which allows us to pass in a list of documents, and an embedding class to create the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012aa63d-726d-4393-8293-88776458c40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding = VertexAIEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# set persist directory so the vector store is saved to disk\n",
    "db = Chroma.from_documents(chunks, embedding, persist_directory=\"./vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f28527-2b38-4c51-a739-b56e499383c9",
   "metadata": {},
   "source": [
    "#### Putting it all together \n",
    "\n",
    "Now that everything is in place, we can tie it all together with a langchain chain. A langchain chain simply orchestrates the multiple steps required to use an LLM for a specific use case. In this case the process we will chain together first embeds the query/question, then performs a nearest neighbors lookup to find the relevant chunks, then uses the relevant chunks to formulate a response with an LLM. We will use the Chroma database as our vector store and Gemini as our LLM. Langchain provides a wrapper around Gemini, `VertexAI()`.\n",
    "\n",
    "For this simple Q/A use case we can use langchain's `RetrievalQA` to link together the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21cc77-b734-4be3-a5f4-a66c182a76c0",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the cell below, implement the end to end retrieval chain using LangChain's `RetrievalQA` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b4f33-0fe2-46e8-a898-ade9e26e7510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector store\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10},  # number of nearest neighbors to retrieve\n",
    ")\n",
    "\n",
    "# You can also set temperature, top_p, top_k\n",
    "llm = VertexAI(model_name=GENERATIVE_MODEL, max_output_tokens=1024)\n",
    "\n",
    "# q/a chain\n",
    "qa = None  # TODO: Instantiate retrieval chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d0811-c32c-4059-a7e0-8f8771dd68d2",
   "metadata": {},
   "source": [
    "Now that everything is tied together we can send queries and get answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474abe04-d499-4066-8d41-8ff38d67b932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    response = qa.invoke({\"query\": question})\n",
    "    print(f\"Response: {response['result']}\\n\")\n",
    "\n",
    "    citations = {doc.metadata[\"source\"] for doc in response[\"source_documents\"]}\n",
    "    print(f\"Citations: {citations}\\n\")\n",
    "\n",
    "    # uncomment below to print source chunks used\n",
    "    # print(f\"Source Chunks Used: {response['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cd186-d1c0-4f1f-9496-f1ea35a07429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask_question(\"What technology underpins large language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6926718-6237-415d-8633-fd554528323e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask_question(\"When was the transformer introduced?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67fa985-4a03-4002-bf42-d19fc27c75a3",
   "metadata": {},
   "source": [
    "Congrats! You have now built a toy retrieval augmented generation system from scratch and applied the learnings to build a more real system using a vector database and orchestration with langchain."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training at scale with the Vertex AI Training Service\n",
    "**Learning Objectives:**\n",
    "  1. Learn how to organize your training code into a Python package\n",
    "  1. Train your model using cloud infrastructure via Google Cloud Vertex AI Training Service\n",
    "  1. Learn how to run your training package using Docker containers and push training Docker images on a Docker registry\n",
    "  1. Monitor Cloud job using TensorBoard\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we'll make the jump from training locally, to training in the cloud. We'll take advantage of Google Cloud's [Vertex AI Training Service](https://cloud.google.com/vertex-ai/). \n",
    "\n",
    "Vertex AI Training Service is a managed service that allows the training and deployment of ML models without having to provision or maintain servers. The infrastructure is handled seamlessly by the managed service for us.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in the [student lab notebook](../labs/1_training_at_scale_vertex.ipynb) -- try to complete that notebook first before reviewing this solution notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify your project name and bucket name in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorboard\n",
    "from google import api_core\n",
    "from google.cloud import aiplatform, bigquery\n",
    "\n",
    "%load_ext tensorboard\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following cell as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change below if necessary\n",
    "PROJECT = !gcloud config get-value project  # noqa: E999\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "OUTDIR = f\"gs://{BUCKET}/taxifare/data\"\n",
    "\n",
    "%env PROJECT=$PROJECT\n",
    "%env BUCKET=$BUCKET\n",
    "%env REGION=$REGION\n",
    "%env OUTDIR=$OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm below that the bucket is regional and its region equals to the specified region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls -Lb gs://$BUCKET | grep \"gs://\\|Location\"\n",
    "echo $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set ai/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BigQuery tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not already created a BigQuery dataset for our data, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq = bigquery.Client(project=PROJECT)\n",
    "dataset = bigquery.Dataset(bq.dataset(\"taxifare\"))\n",
    "\n",
    "try:\n",
    "    bq.create_dataset(dataset)\n",
    "    print(\"Dataset created\")\n",
    "except api_core.exceptions.Conflict:\n",
    "    print(\"Dataset already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a table with 1 million examples.\n",
    "\n",
    "This query reflects the best practice of using a hash function (`FARM_FINGERPRINT`) in the `WHERE` and `ORDER BY` clauses to ensure reproducibility while introducing randomness.\n",
    "\n",
    "Note that the order of columns is exactly what was in our CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_training_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    'unused' AS key\n",
    "FROM `nyc-tlc.yellow.trips` as ny_taxi_trips\n",
    "WHERE ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 1000)) = 1\n",
    "AND\n",
    "    trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0\n",
    "ORDER BY FARM_FINGERPRINT(TO_JSON_STRING(ny_taxi_trips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the validation dataset be 1/10 the size of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_valid_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    'unused' AS key\n",
    "FROM `nyc-tlc.yellow.trips` as ny_taxi_trips\n",
    "WHERE ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 10000)) = 2\n",
    "AND\n",
    "    trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0\n",
    "ORDER BY FARM_FINGERPRINT(TO_JSON_STRING(ny_taxi_trips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the tables as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Deleting current contents of $OUTDIR\"\n",
    "gsutil -m -q rm -rf $OUTDIR\n",
    "\n",
    "echo \"Extracting training data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "   --destination_format CSV  \\\n",
    "   --field_delimiter \",\" --noprint_header \\\n",
    "   taxifare.feateng_training_data \\\n",
    "   $OUTDIR/taxi-train-*.csv\n",
    "\n",
    "echo \"Extracting validation data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "   --destination_format CSV  \\\n",
    "   --field_delimiter \",\" --noprint_header \\\n",
    "   taxifare.feateng_valid_data \\\n",
    "   $OUTDIR/taxi-valid-*.csv\n",
    "\n",
    "gsutil ls -l $OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that you have created both the training and validation datasets in Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET/taxifare/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!gsutil cat gs://$BUCKET/taxifare/data/taxi-train-000000000000.csv | head -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!gsutil cat gs://$BUCKET/taxifare/data/taxi-valid-000000000000.csv | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make code compatible with Vertex AI Training Service\n",
    "In order to make our code compatible with Vertex AI Training Service we need to make the following changes:\n",
    "\n",
    "1. Upload data to Google Cloud Storage \n",
    "2. Move code into a trainer Python package\n",
    "4. Submit training job with `gcloud` to train on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move code into a python package\n",
    "\n",
    "The first thing to do is to convert your training code snippets into a regular Python package. \n",
    "\n",
    "A Python package is simply a collection of one or more `.py` files along with an `__init__.py` file to identify the containing directory as a package. The `__init__.py` sometimes contains initialization code but for our purposes an empty file is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the package directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our package directory contains 3 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ./taxifare/trainer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paste existing code into model.py\n",
    "\n",
    "A Python package requires our code to be in a .py file, as opposed to notebook cells. So, we simply copy and paste our existing code we \n",
    "developed in [this notebook](../../introduction_to_tensorflow/solutions/5_custom_feature_engineering.ipynb) into a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #1**: Organizing your training code into a Python package\n",
    "\n",
    "There are two places to fill in TODOs in `model.py`. \n",
    "\n",
    " * in the `build_dnn_model` function, add code to use an optimizer with a custom learning rate.\n",
    " * in the `train_and_evaluate` function, add code to define variables using the `hparams` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile ./taxifare/trainer/model.py\n",
    "\"\"\"Data prep, train and evaluate DNN model.\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import callbacks\n",
    "from keras.layers import (\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Discretization,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    HashedCrossing,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "\n",
    "def parse_csv(row):\n",
    "    ds = tf.strings.split(row, \",\")\n",
    "    # Label: fare_amount\n",
    "    label = tf.strings.to_number(ds[0])\n",
    "    # Feature: pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "    feature = tf.strings.to_number(ds[2:6])  # use some features only\n",
    "    # Passing feature in tuple so that we can handle them separately.\n",
    "    return (feature[0], feature[1], feature[2], feature[3]), label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size, num_repeat, mode=\"eval\"):\n",
    "    ds = tf.data.Dataset.list_files(pattern)\n",
    "    ds = ds.flat_map(tf.data.TextLineDataset)\n",
    "    ds = ds.map(parse_csv)\n",
    "    if mode == \"train\":\n",
    "        ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.repeat(num_repeat).batch(batch_size, drop_remainder=True)\n",
    "    return ds\n",
    "\n",
    "def lat_lon_parser(row, pick_lat):\n",
    "    ds = tf.strings.split(row, \",\")\n",
    "    # latitude idx: 3 and 5, longitude idx: 2 and 4\n",
    "    idx = [3,5] if pick_lat else [2,4]\n",
    "    return tf.strings.to_number(tf.gather(ds, idx))\n",
    "\n",
    "def adapt_normalize(train_data_path):\n",
    "    ds = tf.data.Dataset.list_files(train_data_path)\n",
    "    ds = ds.flat_map(tf.data.TextLineDataset)\n",
    "    lat_values = ds.map(lambda x: lat_lon_parser(x, True)).batch(10000)\n",
    "    lon_values = ds.map(lambda x: lat_lon_parser(x, False)).batch(10000)\n",
    "\n",
    "    lat_scaler = keras.layers.Normalization(axis=None)\n",
    "    lon_scaler = keras.layers.Normalization(axis=None)\n",
    "    lat_scaler.adapt(lat_values)\n",
    "    lon_scaler.adapt(lon_values)\n",
    "\n",
    "    print(\"Computed statistics for latitude:\")\n",
    "    print(f\"mean: {lat_scaler.mean}, variance: {lat_scaler.variance}\")\n",
    "    print(\"+++++\")\n",
    "    print(\"Computed statistics for longitude:\")\n",
    "    print(f\"mean: {lon_scaler.mean}, variance: {lon_scaler.variance}\")\n",
    "\n",
    "    return lat_scaler, lon_scaler\n",
    "\n",
    "\n",
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff * londiff + latdiff * latdiff)\n",
    "\n",
    "\n",
    "def transform(inputs, nbuckets, normalizers):\n",
    "    lat_scaler, lon_scaler = normalizers\n",
    "\n",
    "    # Normalize longitude\n",
    "    scaled_plon = lon_scaler(inputs[\"pickup_longitude\"])\n",
    "    scaled_dlon = lon_scaler(inputs[\"dropoff_longitude\"])\n",
    "\n",
    "    # Normalize latitude\n",
    "    scaled_plat = lat_scaler(inputs[\"pickup_latitude\"])\n",
    "    scaled_dlat = lat_scaler(inputs[\"dropoff_latitude\"])\n",
    "\n",
    "    # Lambda layer for the custom euclidean function\n",
    "    euclidean_distance = Lambda(euclidean, name=\"euclidean\")(\n",
    "        [scaled_plon, scaled_plat, scaled_dlon, scaled_dlat]\n",
    "    )\n",
    "\n",
    "    # Discretization\n",
    "    latbuckets = np.linspace(start=-5, stop=5, num=nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(start=-5, stop=5, num=nbuckets).tolist()\n",
    "\n",
    "    plon = Discretization(lonbuckets, name=\"plon_bkt\")(scaled_plon)\n",
    "    plat = Discretization(latbuckets, name=\"plat_bkt\")(scaled_plat)\n",
    "    dlon = Discretization(lonbuckets, name=\"dlon_bkt\")(scaled_dlon)\n",
    "    dlat = Discretization(latbuckets, name=\"dlat_bkt\")(scaled_dlat)\n",
    "\n",
    "    # Feature Cross with HashedCrossing layer\n",
    "    p_fc = HashedCrossing(num_bins=(nbuckets + 1) ** 2, name=\"p_fc\")((plon, plat))\n",
    "    d_fc = HashedCrossing(num_bins=(nbuckets + 1) ** 2, name=\"d_fc\")((dlon, dlat))\n",
    "    pd_fc = HashedCrossing(num_bins=(nbuckets + 1) ** 4, name=\"pd_fc\")((p_fc, d_fc))\n",
    "\n",
    "    # Embedding with Embedding layer\n",
    "    pd_embed = Flatten()(\n",
    "        Embedding(input_dim=(nbuckets + 1) ** 4, output_dim=10, name=\"pd_embed\")(\n",
    "            pd_fc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    transformed = Concatenate()([\n",
    "        scaled_plon,\n",
    "        scaled_dlon,\n",
    "        scaled_plat,\n",
    "        scaled_dlat,\n",
    "        euclidean_distance, \n",
    "        pd_embed\n",
    "    ])\n",
    "\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    squared_error = tf.keras.ops.square(y_pred[:, 0] - y_true)\n",
    "    return tf.keras.ops.sqrt(tf.keras.ops.mean(squared_error))\n",
    "\n",
    "def build_dnn_model(nbuckets, nnsize, lr, normalizers):\n",
    "    INPUT_COLS = [\n",
    "        \"pickup_longitude\",\n",
    "        \"pickup_latitude\",\n",
    "        \"dropoff_longitude\",\n",
    "        \"dropoff_latitude\",\n",
    "    ]\n",
    "\n",
    "    inputs = {\n",
    "        colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "        for colname in INPUT_COLS\n",
    "    }\n",
    "\n",
    "    # transforms\n",
    "    x = transform(inputs, nbuckets, normalizers)\n",
    "\n",
    "    for layer, nodes in enumerate(nnsize):\n",
    "        x = Dense(nodes, activation=\"relu\", name=f\"h{layer}\")(x)\n",
    "    output = Dense(1, name=\"fare\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "    # TODO 1a: Your code here\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    # TODO 1b: Your code here\n",
    "    nnsize = [int(s) for s in hparams[\"nnsize\"].split()]\n",
    "    eval_data_path = hparams[\"eval_data_path\"]\n",
    "    num_evals = hparams[\"num_evals\"]\n",
    "    num_examples_to_train_on = hparams[\"num_examples_to_train_on\"]\n",
    "    output_dir = hparams[\"output_dir\"]\n",
    "    train_data_path = hparams[\"train_data_path\"]\n",
    "\n",
    "    model_export_path = os.path.join(output_dir, \"model.keras\")\n",
    "    serving_model_export_path = os.path.join(output_dir, \"savedmodel\")\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoint.keras\")\n",
    "    tensorboard_path = os.path.join(output_dir, \"tensorboard\")\n",
    "\n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "\n",
    "    normalizers = adapt_normalize(eval_data_path)\n",
    "\n",
    "    model = build_dnn_model(nbuckets, nnsize, lr, normalizers)\n",
    "    logging.info(model.summary())\n",
    "    \n",
    "    trainds = create_dataset(\n",
    "        pattern=train_data_path, batch_size=batch_size, num_repeat=None, mode=\"train\"\n",
    "    )\n",
    "\n",
    "    evalds = create_dataset(\n",
    "        pattern=eval_data_path, batch_size=batch_size, num_repeat=1, mode=\"eval\"\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "\n",
    "    checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, verbose=1\n",
    "    )\n",
    "    tensorboard_cb = callbacks.TensorBoard(tensorboard_path, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb],\n",
    "    )\n",
    "\n",
    "    # Save the Keras model file.\n",
    "    model.save(model_export_path)\n",
    "    # Exporting the model in savedmodel for serving.\n",
    "    model.export(serving_model_export_path)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Command Line Parser\n",
    "\n",
    "If you look closely above, you'll notice a new function, `train_and_evaluate` that wraps the code that actually trains the model. This allows us to parametrize the training by passing a dictionary of parameters to this function (e.g, `batch_size`, `num_examples_to_train_on`, `train_data_path` etc.)\n",
    "\n",
    "This is useful because the output directory, data paths and number of train steps will be different depending on whether we're training locally or in the cloud. Parametrizing allows us to use the same code for both.\n",
    "\n",
    "We specify these parameters at run time via the command line. Which means we need to add code to parse command line parameters and invoke `train_and_evaluate()` with those params. This is the job of the `task.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile taxifare/trainer/task.py\n",
    "\"\"\"Argument definitions for model training code in `trainer.model`.\"\"\"\n",
    "\n",
    "import argparse\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Batch size for training steps\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location pattern of eval files\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        default=\"32 8\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nbuckets\",\n",
    "        help=\"Number of buckets to divide lat and lon with\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", help=\"learning rate for optimizer\", type=float, default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help=\"Number of times to evaluate model on eval data training.\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help=\"Number of examples to train on.\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location pattern of train files containing eval URLs\",\n",
    "        required=True,\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "\n",
    "    model.train_and_evaluate(hparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trainer module package locally\n",
    "\n",
    "Now we can test our training code locally as follows using the local test data. We'll run a very small training job over a single file with a small batch size and one eval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "EVAL_DATA_PATH=../data/taxi-traffic-valid*\n",
    "TRAIN_DATA_PATH=../data/taxi-traffic-train*\n",
    "OUTPUT_DIR=./taxifare-model\n",
    "\n",
    "test ${OUTPUT_DIR} && rm -rf ${OUTPUT_DIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "\n",
    "python3 -m trainer.task \\\n",
    "--eval_data_path $EVAL_DATA_PATH \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--train_data_path $TRAIN_DATA_PATH \\\n",
    "--batch_size 5 \\\n",
    "--num_examples_to_train_on 100 \\\n",
    "--num_evals 1 \\\n",
    "--nbuckets 10 \\\n",
    "--lr 0.001 \\\n",
    "--nnsize \"32 8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your training package on Vertex AI\n",
    "\n",
    "\n",
    "Once the code works in standalone mode locally, you can run it on the Cloud using Vertex AI.\n",
    "\n",
    "In Vertex AI, ou can provide training code to Vertex AI in one of the following forms:\n",
    "\n",
    "- **A Python training application to use with a prebuilt container**. Create a [Python source distribution](https://packaging.python.org/en/latest/overview/#python-source-distributions) with code that trains an ML model and exports it to Cloud Storage. This training application can use any of the dependencies included in the prebuilt container that you plan to use it with. Use this option if one of the Vertex AI prebuilt containers for training includes all the dependencies that you need for training.\n",
    "\n",
    "- **A custom container image**. Create a Docker container image with code that trains an ML model and exports it to Cloud Storage. Include any dependencies required by your code in the container image.\n",
    "\n",
    "\n",
    "### Method 1: Prebuilt Container\n",
    "First, let's run a cloud training using a prebuild containers.\n",
    "\n",
    "In order to do so, we need to package our code as a source distribution. For this, we can use `setuptools`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile taxifare/setup.py\n",
    "\"\"\"Using `setuptools` to create a source distribution.\"\"\"\n",
    "\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "setup(\n",
    "    name=\"taxifare_trainer\",\n",
    "    version=\"0.1\",\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description=\"Taxifare model training application.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd taxifare\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store our package in the Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp taxifare/dist/taxifare_trainer-0.1.tar.gz gs://${BUCKET}/taxifare/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Custom Job using the Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit this source distribution to the Cloud we use [CustomJob](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_get) module under Vertex AI Python SDK, and specify some parameters for Vertex AI Training service under [worker_pool_spec](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.WorkerPoolSpec), which includes:\n",
    "- [`machine_spec`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.MachineSpec): Specification of a single machine where we run training. Here we can speficy `machine_type`, as well as the [accelerator specifications](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.AcceleratorType).\n",
    "- `python_package_spec`: Here we provide specification about our Python package runtime, including the prebuilt container URL (`executor_image_uri`), our Python package path (`package_uris`), as well as the custom arguments (`args`) we pass to our training application can be defined and provided here.\n",
    "\n",
    "Because this is on the entire dataset, it will take a while. You can monitor the job from the GCP console in the Vertex AI Training section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #2**: Train your model using cloud infrastructure via Google Cloud Vertex AI Training Service\n",
    "Fill in the TODOs in the code below to submit your job for training on Vertex AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EXAMPLES_TO_TRAIN_ON = 500000\n",
    "NUM_EVALS = 100\n",
    "NBUCKETS = 10\n",
    "LR = 0.001\n",
    "NNSIZE = \"32 8\"\n",
    "\n",
    "base_path = f\"gs://{BUCKET}/taxifare\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "args = [\n",
    "    \"--eval_data_path\",\n",
    "    f\"{base_path}/data/taxi-valid*\",\n",
    "    \"--output_dir\",\n",
    "    f\"{base_path}/trained_model_{timestamp}\",\n",
    "    \"--train_data_path\",\n",
    "    f\"{base_path}/data/taxi-train*\",\n",
    "    \"--batch_size\",\n",
    "    f\"{BATCH_SIZE}\",\n",
    "    \"--num_examples_to_train_on\",\n",
    "    f\"{NUM_EXAMPLES_TO_TRAIN_ON}\",\n",
    "    \"--num_evals\",\n",
    "    f\"{NUM_EVALS}\",\n",
    "    \"--nbuckets\",\n",
    "    f\"{NBUCKETS}\",\n",
    "    \"--lr\",\n",
    "    f\"{LR}\",\n",
    "    \"--nnsize\",\n",
    "    f\"{NNSIZE}\",\n",
    "]\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": None,\n",
    "            \"accelerator_count\": None,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest\",\n",
    "            \"package_uris\": [f\"{base_path}/taxifare_trainer-0.1.tar.gz\"],\n",
    "            \"python_module\": \"trainer.task\",\n",
    "            \"args\": args,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "training_job = aiplatform.CustomJob(\n",
    "    # TODO 2: Your code here\n",
    ")\n",
    "\n",
    "training_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can run a custom training via bash command, using [`gcloud ai custom-jobs create`](https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create).\n",
    "\n",
    "Equivalent bash command:\n",
    "```bash\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model_$TIMESTAMP\n",
    "JOB_NAME=taxifare_$TIMESTAMP\n",
    "echo ${OUTDIR} ${REGION} ${JOB_NAME}\n",
    "\n",
    "PYTHON_PACKAGE_URIS=gs://${BUCKET}/taxifare/taxifare_trainer-0.1.tar.gz\n",
    "MACHINE_TYPE=n1-standard-4\n",
    "REPLICA_COUNT=1\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=64\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=500000\n",
    "NUM_EVALS=100\n",
    "NBUCKETS=10\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "WORKER_POOL_SPEC=\"machine-type=$MACHINE_TYPE,\\\n",
    "replica-count=$REPLICA_COUNT,\\\n",
    "executor-image-uri=$PYTHON_PACKAGE_EXECUTOR_IMAGE_URI,\\\n",
    "python-module=$PYTHON_MODULE\"\n",
    "\n",
    "ARGS=\"--eval_data_path=$EVAL_DATA_PATH,\\\n",
    "--output_dir=$OUTDIR,\\\n",
    "--train_data_path=$TRAIN_DATA_PATH,\\\n",
    "--batch_size=$BATCH_SIZE,\\\n",
    "--num_examples_to_train_on=$NUM_EXAMPLES_TO_TRAIN_ON,\\\n",
    "--num_evals=$NUM_EVALS,\\\n",
    "--nbuckets=$NBUCKETS,\\\n",
    "--lr=$LR,\\\n",
    "--nnsize=$NNSIZE\"\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --python-package-uris=$PYTHON_PACKAGE_URIS \\\n",
    "  --worker-pool-spec=$WORKER_POOL_SPEC \\\n",
    "  --args=\"$ARGS\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open TensorBoard\n",
    "Since we specified the `TensorBoard` callback in `model.fit`, the training application saved the intermediate logs for TensorBoard dashboard.\n",
    "\n",
    "While we can [host TensorBoard in Vertex AI Experiments](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-introduction), here, let's simply open from this notebook.\n",
    "\n",
    "You might not see any data for a bit until the job begins. Check the job status on the console, and then return here to click the refresh button in the top right to update TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {base_path}/trained_model_{timestamp} --port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using a custom container\n",
    "\n",
    "Vertex AI Training also supports training in custom containers, allowing users to bring their own Docker containers with any pre-installed ML framework or algorithm to run on Vertex AI Training. \n",
    "\n",
    "In this last section, we'll see how to submit a Cloud training job using a customized Docker image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Containerizing our `./taxifare/trainer` package involves 3 steps:\n",
    "\n",
    "* Writing a Dockerfile in `./taxifare`\n",
    "* Building the Docker image\n",
    "* Pushing it to the Google Cloud Artifact Registry in our GCP project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dockerfile` specifies\n",
    "1. How the container needs to be provisioned so that all the dependencies in our code are satisfied\n",
    "2. Where to copy our trainer Package in the container\n",
    "3. What command to run when the container is run (the `ENTRYPOINT` line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #3**: Running your training package using Docker containers.\n",
    "Fill in the TODOs in the code below for Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile ./taxifare/Dockerfile\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest\n",
    "\n",
    "# TODO 3: Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_REGISTRY_DIR = \"asl-artifact-repo\"\n",
    "IMAGE_NAME = \"taxifare_training_container\"\n",
    "IMAGE_URI = f\"us-docker.pkg.dev/{PROJECT}/{ARTIFACT_REGISTRY_DIR}/{IMAGE_NAME}\"\n",
    "\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "PROJECT_DIR=$(cd ./taxifare && pwd)\n",
    "DOCKERFILE=$PROJECT_DIR/Dockerfile\n",
    "\n",
    "# Authorize docker command for Artifact Registry\n",
    "gcloud auth print-access-token | docker login -u oauth2accesstoken --password-stdin https://us-docker.pkg.dev\n",
    "\n",
    "docker build $PROJECT_DIR -f $DOCKERFILE -t $IMAGE_URI\n",
    "\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Custom Job using the Python SDK\n",
    "\n",
    "As we did above, let's define the worker_pool_spec using Vertex AI Python SDK and run the cloud training.\n",
    "\n",
    "The definition is almost the same, but please note that here we specify `container_spec` instead of `python_package_spec`, which simply includes our custom container image path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EXAMPLES_TO_TRAIN_ON = 500000\n",
    "NUM_EVALS = 100\n",
    "NBUCKETS = 10\n",
    "LR = 0.001\n",
    "NNSIZE = \"32 8\"\n",
    "\n",
    "base_path = f\"gs://{BUCKET}/taxifare\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "args = [\n",
    "    \"--eval_data_path\",\n",
    "    f\"{base_path}/data/taxi-valid*\",\n",
    "    \"--output_dir\",\n",
    "    f\"{base_path}/trained_model_{timestamp}\",\n",
    "    \"--train_data_path\",\n",
    "    f\"{base_path}/data/taxi-train*\",\n",
    "    \"--batch_size\",\n",
    "    f\"{BATCH_SIZE}\",\n",
    "    \"--num_examples_to_train_on\",\n",
    "    f\"{NUM_EXAMPLES_TO_TRAIN_ON}\",\n",
    "    \"--num_evals\",\n",
    "    f\"{NUM_EVALS}\",\n",
    "    \"--nbuckets\",\n",
    "    f\"{NBUCKETS}\",\n",
    "    \"--lr\",\n",
    "    f\"{LR}\",\n",
    "    \"--nnsize\",\n",
    "    f\"{NNSIZE}\",\n",
    "]\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": None,\n",
    "            \"accelerator_count\": None,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        # Now we specify container_spec, instead of python_package_spec\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"args\": args,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "training_job = aiplatform.CustomJob(\n",
    "    display_name=f\"taxifare_container_{timestamp}\",\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=f\"{base_path}/staging\",\n",
    ")\n",
    "\n",
    "training_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "source": [
    "Here is the equivalent bash command to run container training.\n",
    "\n",
    "```bash\n",
    "# Output directory and jobID\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model_$TIMESTAMP\n",
    "JOB_NAME=taxifare_container_$TIMESTAMP\n",
    "echo ${OUTDIR} ${REGION} ${JOB_NAME}\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=64\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=500000\n",
    "NUM_EVALS=100\n",
    "NBUCKETS=10\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# Vertex AI machines to use for training\n",
    "MACHINE_TYPE=n1-standard-4\n",
    "REPLICA_COUNT=1\n",
    "\n",
    "# GCS paths.\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "WORKER_POOL_SPEC=\"machine-type=$MACHINE_TYPE,\\\n",
    "replica-count=$REPLICA_COUNT,\\\n",
    "container-image-uri=$IMAGE_URI\"\n",
    "\n",
    "ARGS=\"--eval_data_path=$EVAL_DATA_PATH,\\\n",
    "--output_dir=$OUTDIR,\\\n",
    "--train_data_path=$TRAIN_DATA_PATH,\\\n",
    "--batch_size=$BATCH_SIZE,\\\n",
    "--num_examples_to_train_on=$NUM_EXAMPLES_TO_TRAIN_ON,\\\n",
    "--num_evals=$NUM_EVALS,\\\n",
    "--nbuckets=$NBUCKETS,\\\n",
    "--lr=$LR,\\\n",
    "--nnsize=$NNSIZE\"\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=$REGION \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --worker-pool-spec=$WORKER_POOL_SPEC \\\n",
    "  --args=\"$ARGS\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open TensorBoard\n",
    "\n",
    "Let's check TensorBoard once more. A different port number will be used this time, as `8080` is occupied by another TensorBoard instance above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {base_path}/trained_model_{timestamp} --port 8081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

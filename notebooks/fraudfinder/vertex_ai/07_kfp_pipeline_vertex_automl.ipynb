{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Continuous Training with AutoML Vertex Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to use Vertex AutoML pre-built components\n",
    "1. Learn how to build a Vertex AutoML pipeline with these components using BigQuery as a data source\n",
    "1. Learn how to compile, upload, and run the Vertex AutoML pipeline\n",
    "\n",
    "\n",
    "In this lab, you will build, deploy, and run a Vertex AutoML pipeline that orchestrates the **Vertex AutoML AI** services to train, tune, and deploy a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT = !(gcloud config get-value project)\n",
    "PROJECT = PROJECT[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:.:/home/jupyter/.local/bin:.:/home/jupyter/.local/bin:.:/home/jupyter/.local/bin:.:/home/jupyter/.local/bin:.\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow implemented by the pipeline is defined using a Python based Domain Specific Language (DSL). The pipeline's DSL is in the `pipeline_vertex/pipeline_vertex_automl.py` file that we will generate below.\n",
    "\n",
    "The pipeline's DSL has been designed to avoid hardcoding any environment specific settings like file paths or connection strings. These settings are provided to the pipeline code through a set of environment variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building and deploying the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the pipeline to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Vertex Pipelines\n",
    "from typing import NamedTuple\n",
    "import kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUCKET_NAME          = \"qwiklabs-asl-02-111b5e486eb8-fraudfinder\"\n",
      "PROJECT              = \"qwiklabs-asl-02-111b5e486eb8\"\n",
      "REGION               = \"us-central1\"\n",
      "ID                   = \"9m3ic\"\n",
      "FEATURESTORE_ID      = \"fraudfinder_9m3ic\"\n",
      "MODEL_NAME           = \"ff_model\"\n",
      "ENDPOINT_NAME        = \"ff_model_endpoint\"\n",
      "TRAINING_DS_SIZE     = \"1000\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(\"kfp version:\", kfp.__version__)\n",
    "ID='o90cp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Components variables\n",
    "BASE_IMAGE = \"python:3.7\"\n",
    "COMPONENTS_DIR = os.path.join(os.curdir, \"pipelines\", \"components\")\n",
    "INGEST_FEATURE_STORE = f\"{COMPONENTS_DIR}/ingest_feature_store_{ID}.yaml\"\n",
    "EVALUATE = f\"{COMPONENTS_DIR}/evaluate_{ID}.yaml\"\n",
    "\n",
    "# Pipeline variables\n",
    "PIPELINE_NAME = f\"fraud-finder-xgb-pipeline-{ID}\"\n",
    "PIPELINE_DIR = os.path.join(os.curdir, \"pipelines\")\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipelines\"\n",
    "PIPELINE_PACKAGE_PATH = f\"{PIPELINE_DIR}/pipeline_{ID}.json\"\n",
    "\n",
    "# Feature Store component variables\n",
    "BQ_DATASET = \"tx\"\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{ID}\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "# Dataset component variables\n",
    "DATASET_NAME = f\"fraud_finder_dataset_{ID}\"\n",
    "\n",
    "# Training component variables\n",
    "JOB_NAME = f\"fraudfinder-train-xgb-{ID}\"\n",
    "MODEL_NAME = f\"{MODEL_NAME}_xgb_pipeline_{ID}\"\n",
    "CONTAINER_URI = \"us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest\"\n",
    "MODEL_SERVING_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1:latest\"\n",
    ")\n",
    "ARGS = json.dumps([\"--bucket\", f\"gs://{BUCKET_NAME}\"])\n",
    "IMAGE_REPOSITORY = f\"fraudfinder-{ID}\"\n",
    "IMAGE_NAME = \"dask-xgb-classificator\"\n",
    "IMAGE_TAG = \"v1\"\n",
    "IMAGE_URI = f\"us-central1-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"  # TODO: get it from config\n",
    "\n",
    "# Evaluation component variables\n",
    "METRICS_URI = f\"gs://{BUCKET_NAME}/deliverables/metrics.json\"\n",
    "AVG_PR_THRESHOLD = 0.2\n",
    "AVG_PR_CONDITION = \"avg_pr_condition\"\n",
    "\n",
    "# Endpoint variables\n",
    "ENDPOINT_NAME = f\"{ENDPOINT_NAME}_xgb_pipeline_{ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --force-reinstall --no-deps kfp==1.8.22\n",
    "#!pip install --upgrade 'google-cloud-pipeline-components==2.8.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./pipeline_vertex/fs_import_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./pipeline_vertex/fs_import_component.py\n",
    "\"\"\"Lightweight component ingest features.\"\"\"\n",
    "from typing import Dict, List, NamedTuple\n",
    "\n",
    "from kfp.dsl import Metrics, Output, component\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform==1.21.0\"],\n",
    ")\n",
    "def ingest_features_gcs(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    bucket_name: str,\n",
    "    feature_store_id: str,\n",
    "    read_instances_uri: str,\n",
    ") -> NamedTuple(\"Outputs\", [(\"snapshot_uri_paths\", str),],):\n",
    "    # Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "    from datetime import datetime\n",
    "    import glob\n",
    "    import urllib\n",
    "    import json\n",
    "    #import logger TODO:\n",
    "    from typing import NamedTuple\n",
    "\n",
    "    # Feature Store\n",
    "    from google.cloud.aiplatform import Featurestore, EntityType, Feature\n",
    "\n",
    "    # Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    api_endpoint = region + \"-aiplatform.googleapis.com\"\n",
    "    bucket = urllib.parse.urlsplit(bucket_name).netloc\n",
    "    export_uri = (\n",
    "        f\"{bucket_name}/data/snapshots/{timestamp}\"  # format as new gsfuse requires\n",
    "    )\n",
    "    export_uri_path = f\"/gcs/{bucket}/data/snapshots/{timestamp}\"\n",
    "    customer_entity = \"customer\"\n",
    "    terminal_entity = \"terminal\"\n",
    "    serving_feature_ids = {customer_entity: [\"*\"], terminal_entity: [\"*\"]}\n",
    "\n",
    "    print(timestamp)\n",
    "    print(bucket)\n",
    "    print(export_uri)\n",
    "    print(export_uri_path)\n",
    "    print(customer_entity)\n",
    "    print(terminal_entity)\n",
    "    print(serving_feature_ids)\n",
    "\n",
    "    # Main -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ## Define the feature store resource path\n",
    "    feature_store_resource_path = (\n",
    "        f\"projects/{project_id}/locations/{region}/featurestores/{feature_store_id}\"\n",
    "    )\n",
    "    print(\"Feature Store: \\t\", feature_store_resource_path)\n",
    "\n",
    "    ## Run batch job request\n",
    "    # try:\n",
    "    if True:\n",
    "        ff_feature_store = Featurestore(feature_store_resource_path)\n",
    "        ff_feature_store.batch_serve_to_gcs(\n",
    "            gcs_destination_output_uri_prefix=export_uri,\n",
    "            gcs_destination_type=\"csv\",\n",
    "            serving_feature_ids=serving_feature_ids,\n",
    "            read_instances_uri=read_instances_uri,\n",
    "            pass_through_fields=[\"tx_fraud\", \"tx_amount\"],\n",
    "        )\n",
    "    # except Exception as error:\n",
    "    #     print(error)\n",
    "\n",
    "    # Store metadata\n",
    "    snapshot_pattern = f\"{export_uri_path}/*.csv\"\n",
    "    snapshot_files = glob.glob(snapshot_pattern)\n",
    "    snapshot_files_fmt = [p.replace(\"/gcs/\", \"gs://\") for p in snapshot_files]\n",
    "    snapshot_files_string = json.dumps(snapshot_files_fmt)\n",
    "\n",
    "    component_outputs = NamedTuple(\n",
    "        \"Outputs\",\n",
    "        [\n",
    "            (\"snapshot_uri_paths\", str),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(snapshot_pattern)\n",
    "    print(snapshot_files)\n",
    "    print(snapshot_files_fmt)\n",
    "    print(snapshot_files_string)\n",
    "\n",
    "    return component_outputs(snapshot_files_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline_vertex/pipeline_vertex_automl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_vertex/pipeline_vertex_automl.py\n",
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n",
    "# use this file except in compliance with the License. You may obtain a copy of\n",
    "# the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\n",
    "\"\"\"Kubeflow Covertype Pipeline.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from google_cloud_pipeline_components.v1.automl.training_job import (\n",
    "    AutoMLTabularTrainingJobRunOp,\n",
    ")\n",
    "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    ")\n",
    "\n",
    "from kfp import dsl\n",
    "from fs_import_component import ingest_features_gcs\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT = os.getenv(\"PROJECT\")\n",
    "REGION = os.getenv(\"REGION\", \"us-central1\")\n",
    "DATASET_SOURCE = os.getenv(\"DATASET_SOURCE\")\n",
    "PIPELINE_NAME = os.getenv(\"PIPELINE_NAME\", \"fraudfinder\")\n",
    "DISPLAY_NAME = os.getenv(\"MODEL_DISPLAY_NAME\", PIPELINE_NAME)\n",
    "TARGET_COLUMN = os.getenv(\"TARGET_COLUMN\", \"tx_fraud\")\n",
    "SERVING_MACHINE_TYPE = os.getenv(\"SERVING_MACHINE_TYPE\", \"n1-standard-16\")\n",
    "ID = os.getenv(\"ID\")\n",
    "\n",
    "column_specs = {\n",
    "    'tx_amount': \"numeric\",\n",
    "    'customer_id_avg_amount_14day_window': \"numeric\",\n",
    "    'customer_id_avg_amount_15min_window': \"numeric\",\n",
    "    'customer_id_avg_amount_1day_window': \"numeric\",\n",
    "    'customer_id_avg_amount_30min_window': \"numeric\",\n",
    "    'customer_id_avg_amount_60min_window': \"numeric\",\n",
    "    'customer_id_avg_amount_7day_window': \"numeric\",\n",
    "    'customer_id_nb_tx_14day_window': \"numeric\",\n",
    "    'customer_id_nb_tx_15min_window': \"numeric\",\n",
    "    'customer_id_nb_tx_1day_window': \"numeric\",\n",
    "    'customer_id_nb_tx_30min_window': \"numeric\",\n",
    "    'customer_id_nb_tx_60min_window': \"numeric\",\n",
    "    'customer_id_nb_tx_7day_window': \"numeric\",\n",
    "    'terminal_id_avg_amount_15min_window': \"numeric\",\n",
    "    'terminal_id_avg_amount_30min_window': \"numeric\",\n",
    "    'terminal_id_avg_amount_60min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_14day_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_15min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_1day_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_30min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_60min_window': \"numeric\",\n",
    "    'terminal_id_nb_tx_7day_window': \"numeric\",\n",
    "    'terminal_id_risk_14day_window': \"numeric\",\n",
    "    'terminal_id_risk_1day_window': \"numeric\",\n",
    "    'terminal_id_risk_7day_window': \"numeric\"\n",
    "}\n",
    "\n",
    "FEATURESTORE_ID=f\"fraudfinder_{ID}\"\n",
    "BUCKET_NAME = \"qwiklabs-asl-02-111b5e486eb8-fraudfinder\"\n",
    "# Feature Store component variables\n",
    "BQ_DATASET = \"tx\"\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{ID}\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "bucket_name = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"{PIPELINE_NAME}-vertex-automl-pipeline\",\n",
    "    description=f\"AutoML Vertex Pipeline for {PIPELINE_NAME}\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def create_pipeline():\n",
    "\n",
    "    #Ingest data from featurestore\n",
    "    ingest_features_op = ingest_features_gcs(\n",
    "        project_id=PROJECT,\n",
    "        region=REGION,\n",
    "        bucket_name=bucket_name,\n",
    "        feature_store_id=FEATURESTORE_ID,\n",
    "        read_instances_uri=READ_INSTANCES_URI,\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset_create_task = TabularDatasetCreateOp(\n",
    "        project=PROJECT,\n",
    "        display_name=DISPLAY_NAME,\n",
    "        gcs_source=ingest_features_op.outputs[\"snapshot_uri_paths\"],\n",
    "    ).after(ingest_features_op)\n",
    "\n",
    "    # dataset_create_task = TabularDatasetCreateOp(\n",
    "    #     display_name=DISPLAY_NAME,\n",
    "    #     bq_source=DATASET_SOURCE,\n",
    "    #     project=PROJECT,\n",
    "    # ).after(ingest_features_op)\n",
    "    \n",
    "    #2. Run the AutoML Tabular Training Job\n",
    "    #This is the core component that trains the model.\n",
    "    automl_training_task = AutoMLTabularTrainingJobRunOp(\n",
    "        project=PROJECT,\n",
    "        display_name=DISPLAY_NAME,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        dataset=dataset_create_task.outputs[\"dataset\"],\n",
    "        target_column=TARGET_COLUMN,\n",
    "        timestamp_split_column_name='timestamp',\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1,\n",
    "        # Feature list configuration\n",
    "        column_specs=column_specs,\n",
    "        # New parameters for budget and early stopping\n",
    "        budget_milli_node_hours=1000,  # 1000 milli-node hours = 1 node hour\n",
    "        disable_early_stopping=False   # Explicitly set to False to enable early stopping\n",
    "    )\n",
    "\n",
    "    endpoint_create_task = EndpointCreateOp(\n",
    "        project=PROJECT,\n",
    "        display_name=DISPLAY_NAME,\n",
    "    ).after(automl_training_task)\n",
    "\n",
    "    model_deploy_task = ModelDeployOp(  # pylint: disable=unused-variable\n",
    "        model=automl_training_task.outputs[\"model\"],\n",
    "        endpoint=endpoint_create_task.outputs[\"endpoint\"],\n",
    "        deployed_model_display_name=DISPLAY_NAME,\n",
    "        dedicated_resources_machine_type=SERVING_MACHINE_TYPE,\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the environment variables that will be passed to the pipeline compiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ROOT=gs://qwiklabs-asl-02-111b5e486eb8-kfp-artifact-store/pipeline\n",
      "env: PROJECT=qwiklabs-asl-02-111b5e486eb8\n",
      "env: REGION=us-central1\n",
      "env: DATASET_SOURCE=bq://qwiklabs-asl-02-111b5e486eb8.tx.train_table_automl_20250908\n",
      "env: ID=o90cp\n"
     ]
    }
   ],
   "source": [
    "ARTIFACT_STORE = f\"gs://{PROJECT}-kfp-artifact-store\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATASET_SOURCE = f\"bq://{PROJECT}.covertype_dataset.covertype\"\n",
    "\n",
    "DATASET_SOURCE=f\"bq://{PROJECT}.tx.train_table_automl_20250908\"\n",
    "# PIPELINE_NAME=\"ff-demo\"\n",
    "# DISPLAY_NAME=\"dd-name\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT={PROJECT}\n",
    "%env REGION={REGION}\n",
    "%env DATASET_SOURCE={DATASET_SOURCE}\n",
    "%env ID={ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that the `ARTIFACT_STORE` has been created, and let us create it if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-02-111b5e486eb8-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CLI compiler to compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the pipeline from the Python file we generated into a YAML description using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_YAML = \"fraudfinder_automl_vertex_pipeline.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade kfp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/fraudfinder/asl-ml-immersion/notebooks/fraudfinder/vertex_ai/fraudfinder_automl_vertex_pipeline.yaml\n"
     ]
    }
   ],
   "source": [
    "!kfp dsl compile --py pipeline_vertex/pipeline_vertex_automl.py --output $PIPELINE_YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can also use the Python SDK to compile the pipeline:\n",
    "\n",
    "```python\n",
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=create_pipeline, \n",
    "    package_path=PIPELINE_YAML,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the pipeline file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PIPELINE DEFINITION\n",
      "# Name: fraudfinder-vertex-automl-pipeline\n",
      "# Description: AutoML Vertex Pipeline for fraudfinder\n",
      "components:\n",
      "  comp-automl-tabular-training-job:\n",
      "    executorLabel: exec-automl-tabular-training-job\n",
      "    inputDefinitions:\n",
      "      artifacts:\n",
      "        dataset:\n",
      "          artifactType:\n"
     ]
    }
   ],
   "source": [
    "!head {PIPELINE_YAML}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/25570882233/locations/us-central1/pipelineJobs/fraudfinder-vertex-automl-pipeline-20250910160225\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/25570882233/locations/us-central1/pipelineJobs/fraudfinder-vertex-automl-pipeline-20250910160225')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/fraudfinder-vertex-automl-pipeline-20250910160225?project=25570882233\n",
      "PipelineJob projects/25570882233/locations/us-central1/pipelineJobs/fraudfinder-vertex-automl-pipeline-20250910160225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/25570882233/locations/us-central1/pipelineJobs/fraudfinder-vertex-automl-pipeline-20250910160225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/25570882233/locations/us-central1/pipelineJobs/fraudfinder-vertex-automl-pipeline-20250910160225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/25570882233/locations/us-central1/pipelineJobs/fraudfinder-vertex-automl-pipeline-20250910160225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/25570882233/locations/us-central1/pipelineJobs/fraudfinder-vertex-automl-pipeline-20250910160225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/25570882233/locations/us-central1/pipelineJobs/fraudfinder-vertex-automl-pipeline-20250910160225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"automl_fraudfinder_kfp_pipeline\",\n",
    "    template_path=PIPELINE_YAML,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

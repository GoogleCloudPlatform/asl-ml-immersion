{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Keras Functional API and Feature Engineering\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Understand the Keras Functional API for flexible model building.\n",
    "2. Implement Wide & Deep models (memorization + generalization).\n",
    "3. Use Keras preprocessing layers (`Discretization`, `HashedCrossing`, `Embedding`).\n",
    "4. Apply custom transformations with `Lambda` layers.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In the last notebook, we learned about the Keras Sequential API. The [Keras Functional API](https://www.tensorflow.org/guide/keras#functional_api) provides an alternative way of building models that is more flexible. With the Functional API, we can build models with more complex topologies, multiple input or output layers, shared layers or non-sequential data flows (e.g. residual layers).\n",
    "\n",
    "In this notebook, first, we'll use what we learned about preprocessing layers to build a Wide & Deep model, and then apply additioanl feature engineering method using Lambda layers.\n",
    "\n",
    "### Wide & Deep Models\n",
    "Recall that the idea behind Wide & Deep models is to join the two methods of learning through memorization and generalization by making a wide linear model and a deep learning model to accommodate both. You can have a look at the original research paper here: [Wide & Deep Learning for Recommender Systems](https://arxiv.org/abs/1606.07792).\n",
    "\n",
    "<img src='assets/wide_deep.png' width='80%'>\n",
    "<sup>(image: https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)</sup>\n",
    "\n",
    "The wide part of the model is associated with the memory element. In this case, we train a linear model with a wide set of crossed features and learn the correlation of this related data with the assigned label. The deep part of the model is associated with the generalization element where we use embedding vectors for features. The best embeddings are then learned through the training process. While both of these methods can work well alone, Wide & Deep models excel by combining these techniques together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Start by importing the necessary libraries for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import (\n",
    "    CategoryEncoding,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Discretization,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    HashedCrossing,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data \n",
    "\n",
    "We will use the taxifare dataset, using the CSV files that we created in the first notebook of this sequence. Those files have been saved into `../data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -l ../data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.data to read the CSV files\n",
    "\n",
    "We wrote these functions for reading data from the CSV files above in the [previous notebook](2_dataset_api.ipynb).\n",
    "\n",
    "The `tf.data` API efficiently loads and preprocesses data. \n",
    "- `parse_csv`: Parses a CSV row into features and a label. Features are returned as a tuple for Functional API compatibility with multiple inputs.\n",
    "- `create_dataset`: Builds a `tf.data.Dataset` from CSV files, including mapping `parse_csv`, repeating, shuffling (for training), and batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_csv(row):\n",
    "    ds = tf.strings.split(row, \",\")\n",
    "    # Label: fare_amount\n",
    "    label = tf.strings.to_number(ds[0])\n",
    "    # Feature: pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "    feature = tf.strings.to_number(ds[2:6])  # use some features only\n",
    "    # Passing feature in tuple so that we can handle them separately.\n",
    "    return (feature[0], feature[1], feature[2], feature[3]), label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size, mode=\"eval\"):\n",
    "    ds = tf.data.TextLineDataset(pattern)\n",
    "    ds = ds.map(parse_csv).repeat()\n",
    "    if mode == \"train\":\n",
    "        ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Wide & Deep Model with the Keras Functional API\n",
    "\n",
    "We'll construct a Wide & Deep model, which has two parts:\n",
    "1.  **Wide Path:** For memorizing specific feature interactions (often using categorical/crossed features).\n",
    "2.  **Deep Path:** For generalizing patterns (often using numerical features/embeddings through a DNN).\n",
    "The outputs are then combined for prediction. The Functional API handles this complex structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Input Layers\n",
    "\n",
    "Using the Functional API, each input feature gets its own [`keras.Input` layer](https://keras.io/api/layers/core_layers/input/). These define the entry points for data.\n",
    "We create `Input` objects for our four coordinate features, specifying their `name`, `shape` (scalar), and `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_COLS = [\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "]\n",
    "\n",
    "inputs = {\n",
    "    colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "    for colname in INPUT_COLS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing Logic\n",
    "\n",
    "Next, we define preprocessing for our features.\n",
    "1.  **Bucketization:** `Discretization` layer converts continuous latitude/longitude inputs into categorical features by assigning them to predefined buckets (`latbuckets`, `lonbuckets`).\n",
    "2.  **Feature Crossing:** `HashedCrossing` layer combines these bucketized features to create interaction features (e.g., pickup location, dropoff location, and pickup-dropoff interaction). This is useful for the wide part of the model.\n",
    "\n",
    "With the Functional API, you can define each connection independently, which allows for more intricate model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dnn_hidden_units = [32, 8]\n",
    "NBUCKETS = 16\n",
    "\n",
    "# Define Bucketization boundaries\n",
    "latbuckets = np.linspace(start=40.5, stop=41.0, num=NBUCKETS).tolist()\n",
    "lonbuckets = np.linspace(start=-74.2, stop=-73.7, num=NBUCKETS).tolist()\n",
    "\n",
    "# Bucketization with Discretization layer\n",
    "plon = Discretization(lonbuckets, name=\"plon_bkt\")(inputs[\"pickup_longitude\"])\n",
    "plat = Discretization(latbuckets, name=\"plat_bkt\")(inputs[\"pickup_latitude\"])\n",
    "dlon = Discretization(lonbuckets, name=\"dlon_bkt\")(inputs[\"dropoff_longitude\"])\n",
    "dlat = Discretization(latbuckets, name=\"dlat_bkt\")(inputs[\"dropoff_latitude\"])\n",
    "\n",
    "# Feature Cross with HashedCrossing layer\n",
    "p_fc = HashedCrossing(num_bins=(NBUCKETS + 1) ** 2, name=\"p_fc\")((plon, plat))\n",
    "d_fc = HashedCrossing(num_bins=(NBUCKETS + 1) ** 2, name=\"d_fc\")((dlon, dlat))\n",
    "pd_fc = HashedCrossing(num_bins=(NBUCKETS + 1) ** 4, name=\"pd_fc\")((p_fc, d_fc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Deep Path\n",
    "\n",
    "The deep path handles generalization:\n",
    "1.  The `pd_fc` crossed feature is embedded using an `Embedding` layer to create dense vector representations.\n",
    "2.  These embeddings are concatenated with the original numerical input coordinates.\n",
    "3.  The result is passed through a stack of `Dense` layers with ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embedding with Embedding layer\n",
    "pd_embed = Embedding(\n",
    "    input_dim=(NBUCKETS + 1) ** 4, output_dim=10, name=\"pd_embed\"\n",
    ")(pd_fc)\n",
    "\n",
    "# Concatenate and define inputs for deep network\n",
    "deep = Concatenate(name=\"deep_input\")(\n",
    "    [\n",
    "        inputs[\"pickup_longitude\"],\n",
    "        inputs[\"pickup_latitude\"],\n",
    "        inputs[\"dropoff_longitude\"],\n",
    "        inputs[\"dropoff_latitude\"],\n",
    "        Flatten(name=\"flatten_embedding\")(pd_embed),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add hidden Dense layers\n",
    "for i, num_nodes in enumerate(dnn_hidden_units, start=1):\n",
    "    deep = Dense(num_nodes, activation=\"relu\", name=f\"hidden_{i}\")(deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Wide Path\n",
    "\n",
    "The wide path handles memorization:\n",
    "1.  The crossed features (`p_fc`, `d_fc`, `pd_fc`) are one-hot encoded using the `CategoryEncoding` layer.\n",
    "2.  These one-hot encoded features are then concatenated to form the input for the wide part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Onehot Encoding with CategoryEncoding layer\n",
    "p_onehot = CategoryEncoding(num_tokens=(NBUCKETS + 1) ** 2, name=\"p_onehot\")(\n",
    "    p_fc\n",
    ")\n",
    "d_onehot = CategoryEncoding(num_tokens=(NBUCKETS + 1) ** 2, name=\"d_onehot\")(\n",
    "    d_fc\n",
    ")\n",
    "pd_onehot = CategoryEncoding(num_tokens=(NBUCKETS + 1) ** 4, name=\"pd_onehot\")(\n",
    "    pd_fc\n",
    ")\n",
    "\n",
    "# Concatenate and define inputs for wide network\n",
    "wide = Concatenate(name=\"wide_input\")([p_onehot, d_onehot, pd_onehot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Wide and Deep Paths\n",
    "\n",
    "The outputs of the `deep` and `wide` paths are concatenated. This combined tensor is then fed into a final `Dense` layer with one unit (and no activation for regression) to produce the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate wide & deep networks\n",
    "concat = Concatenate(name=\"concatenate\")([deep, wide])\n",
    "\n",
    "# Define the final output layer\n",
    "output = Dense(1, activation=None, name=\"output\")(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our custom RMSE evaluation metric and build our wide and deep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    squared_error = tf.keras.ops.square(y_pred[:, 0] - y_true)\n",
    "    return tf.keras.ops.sqrt(tf.keras.ops.mean(squared_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and Compile the Model\n",
    "\n",
    "In Functional API, `keras.Model` is used to define the model by specifying its `inputs` (our dictionary of `Input` layers) and `outputs` (the final `Dense` layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Functional API, `tf.keras.utils.plot_model` generates a diagram of the model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=False, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Wide & Deep Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up our training variables, create our datasets for training and validation, and train our model.\n",
    "\n",
    "(We refer you to the blog post [ML Design Pattern #3: Virtual Epochs](https://medium.com/google-cloud/ml-design-pattern-3-virtual-epochs-f842296de730) for further details on why we express the training in terms of `NUM_TRAIN_EXAMPLES` and `NUM_EVALS` and why, in this training code, the number of epochs is really equal to the number of evaluations we perform.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 10  # training dataset will repeat, wrap around\n",
    "NUM_EVALS = 10  # how many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 1000  # enough to get a reasonable sample\n",
    "\n",
    "trainds = create_dataset(\n",
    "    pattern=\"../data/taxi-train.csv\", batch_size=BATCH_SIZE, mode=\"train\"\n",
    ")\n",
    "\n",
    "evalds = create_dataset(\n",
    "    pattern=\"../data/taxi-valid.csv\", batch_size=BATCH_SIZE, mode=\"eval\"\n",
    ").take(NUM_EVAL_EXAMPLES // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "OUTDIR = \"./taxi_trained\"\n",
    "shutil.rmtree(path=OUTDIR, ignore_errors=True)  # start fresh each time\n",
    "\n",
    "history = model.fit(\n",
    "    x=trainds,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=NUM_EVALS,\n",
    "    validation_data=evalds,\n",
    "    callbacks=[TensorBoard(OUTDIR)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, we can examine the history to see how the RMSE changes through training on the training set and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RMSE_COLS = [\"rmse\", \"val_rmse\"]\n",
    "\n",
    "pd.DataFrame(history.history)[RMSE_COLS].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Improve Model Performance with Custom Feature Engineering\n",
    "\n",
    "Next, we'll try to improve performance by adding more feature engineering:\n",
    "1.  **Normalization:** Applied to coordinates before distance calculation and other processing.\n",
    "2.  **Euclidean Distance:** Calculated using a `Lambda` layer.\n",
    "\n",
    "For simplicity, we'll build a DNN model here, but these techniques apply to Wide & Deep models too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Feature Normalization with `Normalization` Layer\n",
    "\n",
    "The `keras.layers.Normalization` layer standardizes features by scaling them to have zero mean and unit variance.\n",
    "\n",
    "Since it requires some states (mean and variance), we'll need to either:\n",
    "1. Precompute the state values and instantiate the layer with it.\n",
    "```python\n",
    "keras.layers.Normalization(mean=..., variance=...)\n",
    "```\n",
    "2. Or, compute the values using the `adapt()` method.\n",
    "\n",
    "Here let's take a look at the latter option.\n",
    "\n",
    "We first load data to compute these statistics. Here we retrieve the latitude and longitude columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\n",
    "    \"fare_amount\",\n",
    "    \"pickup_datetime\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\",\n",
    "    \"key\",\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"../data/taxi-train.csv\", names=CSV_COLUMNS)\n",
    "lat_values = pd.concat(\n",
    "    [df[\"pickup_latitude\"], df[\"dropoff_latitude\"]], ignore_index=True\n",
    ").to_numpy()\n",
    "lon_values = pd.concat(\n",
    "    [df[\"pickup_longitude\"], df[\"dropoff_longitude\"]], ignore_index=True\n",
    ").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we instantiate `Normalization` layers (`lat_scaler`, `lon_scaler`) and use their `adapt()` method on the loaded latitude and longitude values to learn the mean and variance.\n",
    "These adapted layers can then be used in the model to apply the learned normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lat_scaler = keras.layers.Normalization(axis=None)\n",
    "lon_scaler = keras.layers.Normalization(axis=None)\n",
    "\n",
    "lat_scaler.adapt(lat_values)\n",
    "lon_scaler.adapt(lon_values)\n",
    "\n",
    "print(\"Computed statistics for latitude:\")\n",
    "print(f\"mean: {lat_scaler.mean}, variance: {lat_scaler.variance}\")\n",
    "print(f\"+++++\")\n",
    "print(\"Computed statistics for latitude:\")\n",
    "print(\"mean: {lon_scaler.mean}, variance: {lon_scaler.variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Input Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_COLS = [\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "]\n",
    "\n",
    "# input layer is all float\n",
    "inputs = {\n",
    "    colname: Input(name=colname, shape=(1,), dtype=\"float32\")\n",
    "    for colname in INPUT_COLS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Feature: Euclidean Distance with a `Lambda` Layer\n",
    "\n",
    "The `euclidean` function calculates straight-line distance. We'll use a [`keras.layers.Lambda` layer](https://keras.io/api/layers/core_layers/lambda/) later to wrap this function, allowing its direct integration into our Keras model for feature engineering. This keeps preprocessing bundled with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff * londiff + latdiff * latdiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing, Normalization, and Lambda Layer Integration\n",
    "\n",
    "Applying the feature engineering steps:\n",
    "1. **Bucket Boundaries:** Now adjusted for normalized data (range `[-5, 5]`).\n",
    "2. **Normalization:** Raw coordinates are scaled using the adapted `lon_scaler` and `lat_scaler`.\n",
    "3. **Lambda Layer:** `euclidean` function calculates distance on these *normalized* coordinates.\n",
    "4. **Discretization:** Normalized coordinates are bucketized.\n",
    "5. **Feature Crossing & Embedding:** Applied to the (now normalized and discretized) features.\n",
    "6. **Concatenate:** The `euclidean_distance` and the final `pd_embed` are combined to be fed into the DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NBUCKETS = 16\n",
    "\n",
    "latbuckets = np.linspace(start=-5, stop=5, num=NBUCKETS).tolist()\n",
    "lonbuckets = np.linspace(start=-5, stop=5, num=NBUCKETS).tolist()\n",
    "\n",
    "# Normalize longitude\n",
    "scaled_plon = lon_scaler(inputs[\"pickup_longitude\"])\n",
    "scaled_dlon = lon_scaler(inputs[\"dropoff_longitude\"])\n",
    "\n",
    "# Normalize latitude\n",
    "scaled_plat = lat_scaler(inputs[\"pickup_latitude\"])\n",
    "scaled_dlat = lat_scaler(inputs[\"dropoff_latitude\"])\n",
    "\n",
    "# Lambda layer for the custom euclidean function\n",
    "euclidean_distance = Lambda(euclidean, name=\"euclidean\")(\n",
    "    [scaled_plon, scaled_plat, scaled_dlon, scaled_dlat]\n",
    ")\n",
    "\n",
    "# Discretization\n",
    "plon = Discretization(lonbuckets, name=\"plon_bkt\")(scaled_plon)\n",
    "plat = Discretization(latbuckets, name=\"plat_bkt\")(scaled_plat)\n",
    "dlon = Discretization(lonbuckets, name=\"dlon_bkt\")(scaled_dlon)\n",
    "dlat = Discretization(latbuckets, name=\"dlat_bkt\")(scaled_dlat)\n",
    "\n",
    "\n",
    "# Feature Cross with HashedCrossing layer\n",
    "p_fc = HashedCrossing(num_bins=(NBUCKETS + 1) ** 2, name=\"p_fc\")((plon, plat))\n",
    "d_fc = HashedCrossing(num_bins=(NBUCKETS + 1) ** 2, name=\"d_fc\")((dlon, dlat))\n",
    "pd_fc = HashedCrossing(num_bins=(NBUCKETS + 1) ** 4, name=\"pd_fc\")((p_fc, d_fc))\n",
    "\n",
    "# Embedding with Embedding layer\n",
    "pd_embed = Flatten()(\n",
    "    Embedding(input_dim=(NBUCKETS + 1) ** 4, output_dim=10, name=\"pd_embed\")(\n",
    "        pd_fc\n",
    "    )\n",
    ")\n",
    "\n",
    "deep = Concatenate()([euclidean_distance, pd_embed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the DNN Layers\n",
    "\n",
    "The concatenated `euclidean_distance` and `pd_embed` tensor is passed through `Dense` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dnn_hidden_units = [32, 8]\n",
    "\n",
    "# Add hidden Dense layers\n",
    "for i, num_nodes in enumerate(dnn_hidden_units, start=1):\n",
    "    deep = Dense(num_nodes, activation=\"relu\", name=f\"hidden_{i}\")(deep)\n",
    "\n",
    "# final output is a linear activation because this is regression\n",
    "output = Dense(1, activation=\"linear\", name=\"fare\")(deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and Compile the Engineered Model\n",
    "\n",
    "Define the Keras Model with the original inputs and the final engineered output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model architecture has changed now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=False, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Engineered Model\n",
    "\n",
    "Train the new model using the same setup as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 10  # training dataset will repeat, wrap around\n",
    "NUM_EVALS = 10  # how many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainds = create_dataset(\n",
    "    pattern=\"../data/taxi-train.csv\", batch_size=BATCH_SIZE, mode=\"train\"\n",
    ")\n",
    "\n",
    "evalds = create_dataset(\n",
    "    pattern=\"../data/taxi-valid.csv\", batch_size=BATCH_SIZE, mode=\"eval\"\n",
    ").take(NUM_EVAL_EXAMPLES // BATCH_SIZE)\n",
    "\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "history = model.fit(\n",
    "    trainds,\n",
    "    validation_data=evalds,\n",
    "    epochs=NUM_EVALS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the RMSE to compare its performance against the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RMSE_COLS = [\"rmse\", \"val_rmse\"]\n",
    "\n",
    "pd.DataFrame(history.history)[RMSE_COLS].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

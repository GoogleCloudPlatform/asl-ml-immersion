{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation: Building an RNN Encoder-Decoder with Keras 3\n",
    "\n",
    "**Learning Objectives**\n",
    "1.  Learn how to build an efficient `tf.data.Dataset` pipeline for a seq2seq task.\n",
    "2.  Learn how to preprocess text using the `keras.layers.TextVectorization` layer.\n",
    "3.  Learn how to train an encoder-decoder model in Keras using the Functional API.\n",
    "4.  Learn how to create separate encoder and decoder models for inference.\n",
    "5.  Learn how to implement a translation (decoding) function from scratch.\n",
    "6.  Learn how to use the BLEU score to evaluate a translation model.\n",
    "\n",
    "\n",
    "***Note: For faster execution, please ensure you are using a GPU runtime. An NVIDIA T4 GPU is recommended for this notebook.***\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll build a Spanish-to-English translation model using a modern RNN encoder-decoder architecture with Keras 3.\n",
    "\n",
    "We will start by building an efficient and scalable input pipeline with the `tf.data.Dataset` API. A key part of our workflow will be using the `TextVectorization` layer to handle all text preprocessing—from standardization and tokenization to integer-encoding—directly within our model.\n",
    "\n",
    "Next, we will use the Keras Functional API to build and train our RNN encoder-decoder model. After training, we will create two specialized models—a dedicated encoder and a decoder—from the layers of our trained model. These specialized models are essential for performing inference, allowing us to implement a function that generates translations word by word.\n",
    "\n",
    "Finally, we'll evaluate the quality of our model's translations using the industry-standard BLEU score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import evaluate\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import GRU, Dense, Embedding, Input\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "MODEL_PATH = \"translate_models/baseline\"\n",
    "DATA_URL = (\n",
    "    \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    ")\n",
    "LOAD_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a language dataset provided by http://www.manythings.org/anki/. The dataset contains Spanish-English  translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "The dataset is a curated list of 120K translation pairs from http://tatoeba.org/, a platform for community contributed translations by native speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = keras.utils.get_file(\"spa-eng.zip\", origin=DATA_URL, extract=True)\n",
    "\n",
    "path_to_file = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"spa-eng_extracted/spa-eng/spa.txt\"\n",
    ")\n",
    "print(\"Translation data stored at:\", path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    path_to_file, sep=\"\\t\", header=None, names=[\"english\", \"spanish\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tf.data Pipeline\n",
    "To begin, we'll construct our training and evaluation datasets using the `tf.data` API, which is the standard for building efficient and scalable input pipelines in TensorFlow. This approach allows us to handle data in a memory-efficient way and seamlessly integrate it with Keras.\n",
    "\n",
    "Our process will be as follows:\n",
    "\n",
    "1. Load the data: We create a tf.data.Dataset directly from our pandas DataFrame using tf.data.Dataset.from_tensor_slices. This creates a dataset where each element is a pair of (Spanish, English) sentences.\n",
    "\n",
    "2. Split the data: We'll use the .take() and .skip() methods to create our training and validation sets. This is a clean and efficient way to split the data without having to load everything into memory at once.\n",
    "\n",
    "3. Define a standardization function: We create a custom_standardization function to preprocess our raw text. This function replicates the logic of the original preprocess_sentence function by lowercasing text, adding spaces around punctuation, and, most importantly, adding <start> and <end> tokens to each sentence. It is built using tf.strings operations, which allows it to be embedded directly into our TextVectorization layer and run efficiently on the GPU/TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "TEST_PROP = 0.2\n",
    "NUM_EXAMPLES = 30000\n",
    "\n",
    "# Create a single dataset from your pandas DataFrame\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data[\"spanish\"][:NUM_EXAMPLES], data[\"english\"][:NUM_EXAMPLES])\n",
    ")\n",
    "\n",
    "# Create the training and validation splits using take() and skip()\n",
    "TRAIN_SIZE = int(NUM_EXAMPLES * (1 - TEST_PROP))\n",
    "train_raw = full_dataset.take(TRAIN_SIZE)\n",
    "val_raw = full_dataset.skip(TRAIN_SIZE)\n",
    "\n",
    "# Define the custom standardization function for TextVectorization\n",
    "\n",
    "\n",
    "@keras.utils.register_keras_serializable()\n",
    "def custom_standardization(input_string):\n",
    "    # Lowercase and strip leading/trailing whitespace\n",
    "    s = tf.strings.lower(input_string)\n",
    "    s = tf.strings.strip(s)\n",
    "\n",
    "    # Add spaces around punctuation\n",
    "    s = tf.strings.regex_replace(s, r\"([?.!,¿])\", r\" \\1 \")\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    s = tf.strings.regex_replace(s, r'[\" \"]+', \" \")\n",
    "\n",
    "    # Filter out unwanted characters, replacing them with a space\n",
    "    s = tf.strings.regex_replace(s, r\"[^a-zA-Z?.!,¿]+\", \" \")\n",
    "\n",
    "    # Strip again to remove any leading/trailing spaces created by cleaning\n",
    "    s = tf.strings.strip(s)\n",
    "\n",
    "    # Add the <start> and <end> tokens\n",
    "    s = tf.strings.join([\"<start>\", s, \"<end>\"], separator=\" \")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Adapt the TextVectorization Layers\n",
    "With our raw text datasets ready, we need to convert the sentences from strings into integer sequences that our model can understand. In Keras 3, the modern and efficient way to do this is with the `keras.layers.TextVectorization` layer. This layer handles tokenization, vocabulary creation, and integer-encoding directly within our model's graph.\n",
    "\n",
    "We will create two separate TextVectorization layers: one for the source language (Spanish) and one for the target language (English). We pass our custom_standardization function to the standardize argument to ensure the text is preprocessed according to our specific rules (including adding `<start>` and `<end>` tokens) before tokenization.\n",
    "\n",
    "The most crucial step is calling the `.adapt()` method. The `adapt` method reads through the text from our training dataset and builds a vocabulary of all the unique words. It's during this step that the layer learns the mapping from each word to a unique integer index. We do this for both the source and target vectorization layers on their respective text data.\n",
    "\n",
    "***NOTE: This cell takes 10-15 to run while it's adapting the pre-processing layer to our dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and adapt the TextVectorization layers\n",
    "source_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "target_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "# Adapt the layers to the training data\n",
    "source_text = train_raw.map(lambda x, y: x)\n",
    "target_text = train_raw.map(lambda x, y: y)\n",
    "\n",
    "source_vectorization.adapt(source_text)\n",
    "target_vectorization.adapt(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Final Datasets\n",
    "Now that our TextVectorization layers have learned the vocabularies, we can build our final input pipelines.\n",
    "\n",
    "First, we store the vocabulary sizes for later use in our model's Embedding layers. Then, we define two helper functions:\n",
    "\n",
    "1. `vectorize_text`: This function takes the raw text sentences and applies the appropriate TextVectorization layer to convert them into integer sequences.\n",
    "\n",
    "2. `create_dataset`: This function prepares the integerized sequences for our encoder-decoder model. The decoder needs two versions of the target sequence during training: one for input (to predict the next word) and one for output (to compare against the prediction for calculating loss). This function creates:\n",
    "   - target_in: The target sequence with the last token removed.\n",
    "   - target_out: The target sequence with the first (<start>) token removed.  \n",
    "\n",
    "Finally, we chain together several `tf.data` methods to construct our final train_dataset and eval_dataset. We `.shuffle()` the training data, `.batch()` both datasets, apply our preprocessing functions using `.map()`, and call `.prefetch()` for better performance. We also inspect the shape of the first batch to confirm our pipeline is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set vocabulary sizes\n",
    "INPUT_VOCAB_SIZE = source_vectorization.vocabulary_size()\n",
    "TARGET_VOCAB_SIZE = target_vectorization.vocabulary_size()\n",
    "\n",
    "\n",
    "def vectorize_text(source, target):\n",
    "    source = source_vectorization(source)\n",
    "    target = target_vectorization(target)\n",
    "    return source, target\n",
    "\n",
    "\n",
    "def create_dataset(source, target):\n",
    "    target_in = target[:, :-1]\n",
    "    target_out = target[:, 1:]\n",
    "    return (source, target_in), target_out\n",
    "\n",
    "\n",
    "# Create the final training and validation datasets\n",
    "train_dataset = (\n",
    "    train_raw.shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(vectorize_text)\n",
    "    .map(create_dataset)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "eval_dataset = (\n",
    "    val_raw.batch(BATCH_SIZE)\n",
    "    .map(vectorize_text)\n",
    "    .map(create_dataset)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Set max lengths for the Embedding layers\n",
    "# We do this by inspecting the element_spec of the dataset\n",
    "for (source, target_in), target_out in train_dataset.take(1):\n",
    "    max_length_inp = source.shape[1]\n",
    "    max_length_targ = target_in.shape[1]\n",
    "    print(\"Source shape:\", source.shape)\n",
    "    print(\"Target in shape:\", target_in.shape)\n",
    "    print(\"Target out shape:\", target_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview a Batch of Data\n",
    "To verify that our data pipeline is working correctly, let's take one batch from our train_dataset and inspect the first example. We'll convert the integer tensors back to text to see what the model will receive during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabularies from the vectorization layers\n",
    "source_vocab = source_vectorization.get_vocabulary()\n",
    "source_index_lookup = {i: word for i, word in enumerate(source_vocab)}\n",
    "\n",
    "target_vocab = target_vectorization.get_vocabulary()\n",
    "target_index_lookup = {i: word for i, word in enumerate(target_vocab)}\n",
    "\n",
    "# Helper function to convert a tensor of token IDs back to a string\n",
    "\n",
    "\n",
    "def to_text(tensor, lookup_dict):\n",
    "    # Join the words, filtering out the padding token (ID 0)\n",
    "    return \" \".join([lookup_dict[i] for i in tensor.numpy() if i != 0])\n",
    "\n",
    "\n",
    "# Take one batch from the training dataset and inspect the first example\n",
    "for (source_batch, target_in_batch), target_out_batch in train_dataset.take(1):\n",
    "    # Get the first example from the batch\n",
    "    source_example = source_batch[0]\n",
    "    target_in_example = target_in_batch[0]\n",
    "    target_out_example = target_out_batch[0]\n",
    "\n",
    "    print(\"--- Example from a Training Batch ---\")\n",
    "    print(\"\\nSource (Spanish):\")\n",
    "    print(f\"  Tensor: {source_example.numpy()}\")\n",
    "    print(f\"  Text:   {to_text(source_example, source_index_lookup)}\")\n",
    "\n",
    "    print(\"\\nTarget Input (English - for Decoder):\")\n",
    "    print(f\"  Tensor: {target_in_example.numpy()}\")\n",
    "    print(f\"  Text:   {to_text(target_in_example, target_index_lookup)}\")\n",
    "\n",
    "    print(\"\\nTarget Output (English - for Loss Calculation):\")\n",
    "    print(f\"  Tensor: {target_out_example.numpy()}\")\n",
    "    print(f\"  Text:   {to_text(target_out_example, target_index_lookup)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_integerize(path, num_examples=None):\n",
    "\n",
    "    targ_lang, inp_lang = load_and_preprocess(path, num_examples)\n",
    "\n",
    "    # TODO 1b\n",
    "    input_tensor, inp_lang_tokenizer = # TODO\n",
    "    target_tensor, targ_lang_tokenizer = # TODO\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training the RNN encoder-decoder model\n",
    "\n",
    "We use an encoder-decoder architecture, however we embed our words into a latent space prior to feeding them into the RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_UNITS = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Encoder\n",
    "\n",
    "**Exercise: Build the Encoder Model**  \n",
    "Build the encoder component for a standard encoder-decoder translator. The encoder's job is to process an entire input sentence and compress its meaning into a single fixed-size \"thought vector\" (or \"context vector\"), which will eventually be passed to the decoder.\n",
    "\n",
    "Use the Keras Functional API to construct the encoder, which gives us a clear way to define the flow of data. Our encoder will have two main layers:\n",
    "\n",
    "1. **Embedding Layer**: This layer takes the integer-encoded vocabulary and learns a dense vector representation (an embedding) for each word. These embeddings can capture semantic relationships between words.\n",
    "\n",
    "2. **GRU (Gated Recurrent Unit) Layer**: This is a type of Recurrent Neural Network (RNN) that processes the sequence of word embeddings one by one. We configure it with return_state=True to get the final hidden state of the GRU after it has processed the entire input sentence. This final hidden state is the \"thought vector\" that encapsulates the meaning of the input sentence and will be passed to the decoder. Add the argument `reset_after=False` to make this layer GPU compatible. See this [documentation](https://keras.io/api/layers/recurrent_layers/gru/) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), name=\"encoder_input\")\n",
    "\n",
    "# TODO - Define the Embedding Layer\n",
    "encoder_inputs_embedded = Embedding()(encoder_inputs)\n",
    "\n",
    "encoder_rnn = GRU()  # TODO - Define the GRU Layer\n",
    "\n",
    "encoder_outputs, encoder_state = encoder_rnn(encoder_inputs_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Decoder\n",
    "**Exercise: Define the Decoder**  \n",
    "Build the decoder so that it takes the \"thought vector\" from the encoder and generates the translated sentence word by word.\n",
    "\n",
    "Its architecture mirrors the encoder, containing Embedding and GRU layers. The crucial difference is that we initialize the decoder's GRU layer with the final hidden state of the encoder `(initial_state=encoder_state)`. This is how the decoder receives the context from the source sentence, which it then uses to generate the correct translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,), name=\"decoder_input\")\n",
    "\n",
    "# TODO - Define the Embedding Layer\n",
    "decoder_inputs_embedded = Embedding()(decoder_inputs)\n",
    "\n",
    "decoder_rnn = GRU()  # TODO - Define the Decoder Layer\n",
    "\n",
    "decoder_outputs, decoder_state = decoder_rnn(\n",
    "    # TODO - Connect the encoder and decoder using the \"initial_state\" parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the encoder-decoder architecture is a softmax `Dense` layer that will create the next word probability vector or next word `predictions` from the `decoder_output`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(TARGET_VOCAB_SIZE, activation=\"softmax\", name=\"dense\")\n",
    "\n",
    "predictions = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Compile the Model\n",
    "\n",
    "**Exercise: Complete the compilation code for the model**\n",
    "Complete the compile code for the trainable model, instantiate a Model object and specify the encoder and decoder inputs, and the final predictions as the output.\n",
    "\n",
    "Compile the model, configuring it for training. Use the `adam` optimizer and the `sparse_categorical_crossentropy` loss function. This loss is ideal for our task because our targets are integers (the word indices) and the model's output is a probability distribution over the vocabulary.\n",
    "\n",
    "Finally, call `.summary()` to print a useful overview of the model's architecture, including the layers, output shapes, and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=predictions)\n",
    "\n",
    "# TODO- Fill in the parameters\n",
    "model.compile()\n",
    "# TODO - Print the model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model!\n",
    "\n",
    "***NOTE: Update the number of `EPOCHS` to 10/15 to get a decent translation performance. However, this will increase the training time.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, validation_data=eval_dataset, epochs=EPOCHS, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Translation (Inference) Function\n",
    "\n",
    "Now that our model is trained, we need a way to use it for translation. We can't simply call `model.predict()` on a Spanish sentence because the model expects both a source (Spanish) and a target (English) sentence as input. During training, we used the ground-truth target sentence in a technique called \"teacher forcing.\" For inference, we don't have the target sentence—that's what we need to generate!\n",
    "\n",
    "The solution is to generate the translation word by word in a loop. The process works like this:\n",
    "\n",
    "1.  Take the input sentence (e.g., \"No estamos comiendo.\") and pass it through the encoder to get its final hidden state (the \"thought vector\").\n",
    "2.  Start the decoder with the `<start>` token.\n",
    "3.  Feed the `<start>` token and the encoder's state into the decoder to predict the first word of the translation.\n",
    "4.  Take the predicted word, feed it back into the decoder as input for the next step, and use the new decoder state.\n",
    "5.  Repeat this process until the decoder predicts the `<end>` token, signaling that the translation is complete.\n",
    "\n",
    "**Exercise**  \n",
    "To implement this, create two new, specialized models using the layers from the model we just trained:\n",
    "\n",
    "*   An **`encoder_model`** that takes a raw string sentence, vectorizes it, and returns the encoder's final hidden state.\n",
    "*   A **`decoder_model`** that takes the current predicted sequence and a hidden state, and returns the prediction for the next word, along with the updated hidden state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Inference Encoder Model ---\n",
    "# This model will convert raw Spanish sentences into the encoder's final state.\n",
    "\n",
    "# Input layer for raw strings\n",
    "encoder_string_input = keras.Input(\n",
    "    shape=(1,), dtype=\"string\", name=\"encoder_string_input\"\n",
    ")\n",
    "\n",
    "# Vectorize the strings using the adapted layer\n",
    "# TODO\n",
    "\n",
    "# Reuse the trained Embedding layer\n",
    "# TODO\n",
    "\n",
    "# Reuse the trained GRU layer\n",
    "# TODO\n",
    "\n",
    "# Create the final encoder model for inference\n",
    "encoder_model = Model(\n",
    "    inputs=encoder_string_input,\n",
    "    outputs=encoder_state_output,\n",
    "    name=\"inference_encoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Create Inference Decoder Model ---\n",
    "# This model's structure is similar to the original, as it works with token IDs.\n",
    "\n",
    "# Input layers for the decoder\n",
    "decoder_input = keras.Input(shape=(None,), name=\"decoder_input\")\n",
    "decoder_state_input = keras.Input(\n",
    "    shape=(HIDDEN_UNITS,), name=\"decoder_state_input\"\n",
    ")\n",
    "\n",
    "# Reuse trained layers from the main model\n",
    "# TODO\n",
    "\n",
    "# Build the decoder graph\n",
    "# TODO\n",
    "\n",
    "# Create the final decoder model for inference\n",
    "decoder_model = Model(\n",
    "    inputs=[decoder_input, decoder_state_input],\n",
    "    outputs=[decoder_predictions, decoder_state_output],\n",
    "    name=\"inference_decoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the Translation\n",
    "\n",
    "With our specialized encoder and decoder models ready, we can now define the main `decode_sequences` function that ties everything together to perform the translation.\n",
    "\n",
    "This function implements the step-by-step decoding process:\n",
    "\n",
    "1.  **Encode the input**: The raw input sentences are passed to the `encoder_model` to get the initial \"thought vector\" or hidden state.\n",
    "\n",
    "2.  **Initialize the sequence**: A target sequence is created, containing only the `<start>` token for each sentence in the batch.\n",
    "\n",
    "3.  **Iteratively decode**: In a loop, the `decoder_model` is called with the current target sequence and the hidden state to predict the next token. For simplicity, we use `argmax` to select the most probable token as our prediction.\n",
    "\n",
    "4.  **Append and update**: The predicted token is appended to our result, and the process is repeated with the new token and the updated hidden state from the decoder.\n",
    "\n",
    "5.  **Stop condition**: The loop continues until the special `<end>` token is generated or the maximum sequence length is reached.\n",
    "\n",
    "Finally, we'll test our function with a few example sentences to see the translation results in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Define the Translation Function (decode_sequences) ---\n",
    "\n",
    "# Get the vocabulary and create the reverse mapping\n",
    "targ_vocab = target_vectorization.get_vocabulary()\n",
    "targ_index_lookup = dict(zip(range(len(targ_vocab)), targ_vocab))\n",
    "start_token_id = target_vectorization.get_vocabulary().index(\"<start>\")\n",
    "end_token_id = target_vectorization.get_vocabulary().index(\"<end>\")\n",
    "\n",
    "\n",
    "def decode_sequences(input_sentences, max_decode_length=50):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        input_sentences: A tensor of raw strings in the source language.\n",
    "    Returns:\n",
    "        A list of translated sentences.\n",
    "    \"\"\"\n",
    "    batch_size = input_sentences.shape[0]\n",
    "\n",
    "    # Encode the input strings to get the initial state.\n",
    "    states_value = encoder_model(input_sentences)\n",
    "\n",
    "    # Initialize the target sequence with the <start> token ID.\n",
    "    target_seq = keras.ops.full((batch_size, 1), start_token_id, dtype=\"int32\")\n",
    "\n",
    "    decoded_sentences = [\"\" for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(max_decode_length):\n",
    "        output_tokens, decoder_state = decoder_model([target_seq, states_value])\n",
    "\n",
    "        # Sample a token (we use argmax for simplicity)\n",
    "        sampled_token_index = keras.ops.argmax(output_tokens[:, -1, :], axis=-1)\n",
    "\n",
    "        # Update the target sequence for the next iteration\n",
    "        target_seq = keras.ops.expand_dims(sampled_token_index, axis=-1)\n",
    "\n",
    "        # Update the decoder state\n",
    "        states_value = decoder_state\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            token = targ_index_lookup[sampled_token_index[j].numpy()]\n",
    "            if token == \"<end>\":\n",
    "                continue  # Don't add the end token to the output\n",
    "            decoded_sentences[j] += token + \" \"\n",
    "\n",
    "    return [s.strip() for s in decoded_sentences]\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "]\n",
    "\n",
    "machine_translations = decode_sequences(keras.ops.convert_to_tensor(sentences))\n",
    "print(machine_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(encoder_input, decoder_input):\n",
    "    \n",
    "    # shift ahead by 1\n",
    "    target = tf.roll(decoder_input, -1, 1)\n",
    "\n",
    "    # replace last column with 0s\n",
    "    zeros = tf.zeros([target.shape[0], 1], dtype=tf.int32)\n",
    "    target = tf.concat((target[:, :-1], zeros), axis=-1)\n",
    "\n",
    "    dataset = # TODO\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the actual train and eval dataset using the function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    create_dataset(input_tensor_train, target_tensor_train)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .repeat()\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataset = create_dataset(input_tensor_val, target_tensor_val).batch(\n",
    "    BATCH_SIZE, drop_remainder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "    \"El invierno se acerca.\",\n",
    "    \"Tom no comio nada.\",\n",
    "    \"Su pierna mala le impidió ganar la carrera.\",\n",
    "    \"Su respuesta es erronea.\",\n",
    "    \"¿Qué tal si damos un paseo después del almuerzo?\",\n",
    "]\n",
    "\n",
    "reference_translations = [\n",
    "    \"We're not eating.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Tom ate nothing.\",\n",
    "    \"His bad leg prevented him from winning the race.\",\n",
    "    \"Your answer is wrong.\",\n",
    "    \"How about going for a walk after lunch?\",\n",
    "]\n",
    "\n",
    "machine_translations = decode_sequences(keras.ops.convert_to_tensor(sentences))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(\"-\")\n",
    "    print(\"INPUT:\")\n",
    "    print(sentences[i])\n",
    "    print(\"REFERENCE TRANSLATION:\")\n",
    "    print(reference_translations[i])\n",
    "    print(\"MACHINE TRANSLATION:\")\n",
    "    print(machine_translations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric (BLEU)\n",
    "\n",
    "Unlike say, image classification, there is no one right answer for a machine translation. However our current loss metric, cross entropy, only gives credit when the machine translation matches the exact same word in the same order as the reference translation. \n",
    "\n",
    "Many attempts have been made to develop a better metric for natural language evaluation. The most popular currently is Bilingual Evaluation Understudy (BLEU).\n",
    "\n",
    "- It is quick and inexpensive to calculate.\n",
    "- It allows flexibility for the ordering of words and phrases.\n",
    "- It is easy to understand.\n",
    "- It is language independent.\n",
    "- It correlates highly with human evaluation.\n",
    "- It has been widely adopted.\n",
    "\n",
    "The score is from 0 to 1, where 1 is an exact match.\n",
    "\n",
    "It works by counting matching n-grams between the machine and reference texts, regardless of order. BLUE-4 counts matching n grams from 1-4 (1-gram, 2-gram, 3-gram and 4-gram). It is common to report both BLUE-1 and BLUE-4\n",
    "\n",
    "It still is imperfect, since it gives no credit to synonyms and so human evaluation is still best when feasible. However BLEU is commonly considered the best among bad options for an automated metric.\n",
    "\n",
    "The Hugging Face evaluate framework has an implementation that we will use.\n",
    "\n",
    "We can't run calculate BLEU during training, because at that time the correct decoder input is used. Instead we'll calculate it now.\n",
    "\n",
    "For more info: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(sentence):\n",
    "    filtered = list(filter(lambda x: x != \"\" and x != \"<end>\", sentence))\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now average the `bleu_1` and `bleu_4` scores for all the sentence pairs in the eval set. The next cell takes around 1 minute (8 minutes for full dataset eval) to run, the bulk of which is decoding the sentences in the validation set. Please wait until completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EVALUATE = 1000\n",
    "\n",
    "source_sentences = []\n",
    "reference = []  # This will be a list of lists for the BLEU score calculation\n",
    "\n",
    "# Iterate over the raw validation dataset to get the source and reference texts.\n",
    "# We use .numpy().decode() to convert the EagerTensors from the dataset into strings.\n",
    "for spa, eng in tqdm(val_raw.take(NUM_EVALUATE), total=NUM_EVALUATE):\n",
    "    source_sentences.append(spa.numpy().decode(\"utf-8\"))\n",
    "    reference.append([eng.numpy().decode(\"utf-8\")])\n",
    "\n",
    "# Generate all machine translations in a single batch for better performance.\n",
    "candidate = decode_sequences(tf.constant(source_sentences))\n",
    "\n",
    "# Print a few examples to see the results\n",
    "for i in range(5):\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Source:    {source_sentences[i]}\")\n",
    "    print(f\"Reference: {reference[i][0]}\")\n",
    "    print(f\"Machine:   {candidate[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_1 = bleu.compute(predictions=candidate, references=reference, max_order=1)\n",
    "bleu_4 = bleu.compute(predictions=candidate, references=reference, max_order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_1[\"bleu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_4[\"bleu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

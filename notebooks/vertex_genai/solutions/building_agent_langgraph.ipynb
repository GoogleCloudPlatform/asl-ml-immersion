{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78db9fda-db2e-4a97-90bc-b7509d077892",
   "metadata": {},
   "source": [
    "# Building Agents with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e773923-1ae0-4bd3-9ce1-d32d982cabf4",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Understand Graph State: Learn how to define and manage the state within a LangGraph application.\n",
    "2. Build Simple Graphs: Discover how to construct and run basic cyclical and acyclical graphs.\n",
    "3. Implement Tool Use: Explore how to integrate tools (function calling) within your graphs for more complex tasks.\n",
    "4. Incorporate Human-in-the-Loop: Learn how to add human review and intervention points within your graph execution.\n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, you'll dive into LangGraph, a library designed to help you build robust, stateful, and agentic applications with Large Language Models (LLMs). LangGraph extends the LangChain ecosystem, allowing you to create cyclical graphs, which are essential for many agent-like behaviors where you might need to loop, re-plan, or conditionally execute steps.\n",
    "\n",
    "Unlike traditional LangChain chains, which are typically directed acyclic graphs (DAGs), LangGraph enables you to define more complex flows that can revisit nodes and manage state explicitly across these steps. This is crucial for building sophisticated applications like AI agents that can plan, execute actions, observe results, and adapt their behavior.\n",
    "\n",
    "This notebook will guide you through the fundamental concepts of LangGraph, from understanding its core component—the graph state—to building simple graphs, integrating tools for enhanced capabilities, and incorporating human oversight into your LLM applications.\n",
    "\n",
    "For more information, check out the official [LangGraph documentation](https://langchain-ai.github.io/langgraph/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769268f9-b3fe-4516-9aa8-83f3d166950b",
   "metadata": {},
   "source": [
    "## The Simplest Graph\n",
    "\n",
    "Let's build a simple graph with 3 nodes and one conditional edge. \n",
    "\n",
    "![Screenshot 2024-08-20 at 3.11.22 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dba5f465f6e9a2482ad935_simple-graph1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf8d87-3433-4477-a7ff-b87a3613bd72",
   "metadata": {},
   "source": [
    "### State\n",
    "\n",
    "The [State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state) is a critical concept in LangGraph. It represents the single, unified object that is passed around between all the nodes (steps) in your graph. Think of it as the memory or working context of your application. As your graph executes, each node can read from and write to this state, allowing information to be accumulated, modified, and used to make decisions throughout the workflow.\n",
    "\n",
    "\n",
    "State enables:\n",
    "* Persistence of Information: It allows data generated in one step (e.g., an LLM response, a tool output) to be available to subsequent steps.\n",
    "* Shared Memory: All nodes in the graph operate on the same state object (though they might only care about specific parts of it). This ensures consistency.\n",
    "* Decision Making: The current state can be used by conditional edges to route the flow of execution within the graph. For example, an agent might decide which tool to use next based on the current state of the problem.\n",
    "* Cyclical Flows: In graphs that loop or revisit steps (a core strength of LangGraph), the state ensures that each iteration builds upon the last, rather than starting from scratch.\n",
    "\n",
    "\n",
    "### Defining the State Schema\n",
    "The structure of your State, its \"schema,\" defines what kind of information your graph will track. This schema is then used as the input type for all nodes and the output type for most nodes within your graph. LangGraph is flexible in how you define this schema. For this lab, we will use Pydantic's BaseModel to define our graph's state, which provides a good balance of structure, validation, and ease of use. LangGraph State also supports use of TypedDict objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8afdb1-c671-4bd0-bcd4-c6c9d3fea1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class State(BaseModel):\n",
    "    graph_state: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20919de-3bef-471b-aede-53e0f193f8da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nodes\n",
    "\n",
    "In LangGraph, [Nodes](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes) are the fundamental units of computation. They represent the individual steps, actions, or processing logic within your graph. Each node is responsible for performing a specific task, such as calling an LLM, executing a tool, processing data, or even waiting for human input.\n",
    "\n",
    "##### Nodes are Python Functions\n",
    "One of the elegant aspects of LangGraph is its simplicity in defining nodes: nodes are just Python functions. This design choice offers several advantages:\n",
    "\n",
    "Every function that you designate as a node in your LangGraph graph receives the current State object as its first positional argument. This State object is an instance of the schema you defined earlier (e.g., your Pydantic BaseModel). In our simple graph, the nodes will update the State's `graph_state` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1748d9-edf6-420f-9fab-f3d383bbc6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def node_1(state: State):\n",
    "    print(\"---Node 1---\")\n",
    "    return {\"graph_state\": state.graph_state + \" I am\"}\n",
    "\n",
    "\n",
    "def node_2(state: State):\n",
    "    print(\"---Node 2---\")\n",
    "    return {\"graph_state\": state.graph_state + \" happy!\"}\n",
    "\n",
    "\n",
    "def node_3(state: State):\n",
    "    print(\"---Node 3---\")\n",
    "    return {\"graph_state\": state.graph_state + \" sad!\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12407a8a-9b7f-40c0-95f2-442b14a744c5",
   "metadata": {},
   "source": [
    "With our node functions defined, the next step is to construct the actual graph. This is done using the StateGraph class.\n",
    "\n",
    "When we instantiate StateGraph, we pass our defined State schema (e.g., our Pydantic BaseModel or TypedDict) to its constructor. This tells the graph what the structure of its internal state will look like, ensuring that all nodes and edges operate on a consistent data model.\n",
    "\n",
    "Once the graph object is created (here, builder), we then register our Python functions as nodes within this graph using the add_node() method. Each call to add_node() takes two main arguments:\n",
    "\n",
    "1. A unique string name for the node (e.g., \"node_1\"). This name is used to refer to the node when defining edges and execution flow.\n",
    "2. The Python function itself that implements the node's logic (e.g., node_1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e633fbc-ecbf-4cb2-8c37-7362d5ad57a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", node_2)\n",
    "builder.add_node(\"node_3\", node_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf276b-d836-46ed-b7b6-05a0a215d6d6",
   "metadata": {},
   "source": [
    "### Edges\n",
    "\n",
    "Now that we have nodes (our functions that do the work) and a defined state (the data that flows through), we need to connect them. Edges in LangGraph define the pathways between nodes, dictating the sequence of operations and the overall logic of your application. They determine which node executes after another.\n",
    "\n",
    "LangGraph provides a few ways to define these connections, primarily through normal edges for fixed transitions and conditional edges for dynamic routing.\n",
    "\n",
    "#### The START and END Points\n",
    "Before diving into specific edge types, it's important to know two special keywords:\n",
    "\n",
    "* START: This is a special identifier used to define the entry point of your graph. You'll typically add an edge from START to your initial processing node.\n",
    "* END: This is another special identifier. When the graph transitions to END, it signifies that a particular path of execution has completed. A graph can have multiple paths leading to END.\n",
    "\n",
    "\n",
    "#### Normal Edges\n",
    "A [Normal Edge](https://langchain-ai.github.io/langgraph/concepts/low_level/#edges) creates a straightforward, unconditional link from one node to another. If node A has a normal edge to node B, then every time node A finishes executing, node B will be the next one to run.\n",
    "\n",
    "You create a normal edge using the `builder.add_edge(source_node_name: str, destination_node_name: str)` method.\n",
    "\n",
    "For instance, after a specific processing step, you might always want to save the results, or if a branch leads to a final action, you'd connect that action's node to END.\n",
    "\n",
    "#### Conditional Edges: Dynamic Routing Based on State\n",
    "[Conditional Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#conditional-edges) allow you to route the execution flow based on the current State of your graph. Instead of a fixed next step, a function (the \"condition\") is evaluated, and its output determines which node(s) to visit next.\n",
    "\n",
    "Conditional edges are implemented as follows:\n",
    "\n",
    "* A Condition Function: You write a Python function that accepts the current State object as its argument.\n",
    "* Decision Logic: Inside this function, you implement logic that inspects the state and decides where the graph should go next.\n",
    "* Return Value: The function typically returns a string (or a list of strings) which is the name of the next node(s) to execute.\n",
    "\n",
    "You add conditional edges using the `builder.add_conditional_edges(source_node_name: str, condition_function: Callable, path_map: Optional[dict] = None)` method.\n",
    "\n",
    "* `source_node_name`: The node from which the conditional branching originates.\n",
    "* `condition_function`: Your Python function that takes the state and returns the name of the next node.\n",
    "* `path_map` (Optional): If your condition function returns a value that isn't directly a node name (e.g., it returns 'category_a' or 'category_b'), you can provide a dictionary mapping these return values to actual node names (e.g., {\"category_a\": \"node_for_a\", \"category_b\": \"node_for_b\"}). However, as you'll see in the example below, if your condition function directly returns the string name of the next node, you don't need a complex path_map.\n",
    "\n",
    "In the following code, we'll define a `decide_mood` function that looks at the state (though in this simple example, it will just make a random choice for illustrative purposes) and returns the name of the next node to visit. This function will be used to conditionally route from node_1 to either node_2 or node_3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336cd71-bd50-4872-8e35-c6ec0c06157f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def decide_mood(state) -> Literal[\"node_2\", \"node_3\"]:\n",
    "\n",
    "    # Often, we will use state to decide on the next node to visit\n",
    "    user_input = state.graph_state\n",
    "\n",
    "    # Here, let's just do a 50 / 50 split between nodes 2, 3\n",
    "    if random.random() < 0.5:\n",
    "\n",
    "        # 50% of the time, we return Node 2\n",
    "        return \"node_2\"\n",
    "\n",
    "    # 50% of the time, we return Node 3\n",
    "    return \"node_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b0e90-b7a8-4440-8cf8-702ca3e3a2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logic\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_conditional_edges(\"node_1\", decide_mood)\n",
    "builder.add_edge(\"node_2\", END)\n",
    "builder.add_edge(\"node_3\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbaef77-def4-46a4-8864-47651c882c9c",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "Before we can run our graph, we need to call the `.compile()` method on our StateGraph builder instance. This step finalizes the graph structure, verifies its integrity, and returns an executable LangChain Runnable.\n",
    "\n",
    "We can also visualize our graph! Note that conditional edges are displayed with dotted lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294d3cf-ce9f-4360-a2c6-37fb5b9f2bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a65210-2356-45a1-8a9a-70228cc1f246",
   "metadata": {},
   "source": [
    "### Graph Invocation\n",
    "\n",
    "Once we've defined our State, added our Nodes (the functions that do the work), connected them with Edges (defining the flow), and compiled it, our graph blueprint is complete. The next step is to run it.\n",
    "\n",
    "The compiled LangGraph graph adheres to the [LangChain Runnable](https://python.langchain.com/docs/concepts/runnables/) protocol. This is a standard interface used across the LangChain ecosystem, providing a consistent way to interact with different components. If you're familiar with LangChain, methods like invoke(), stream(), batch(), and their asynchronous counterparts (ainvoke(), etc.) will feel natural. This standardization makes it easier to integrate LangGraph into larger LangChain applications.\n",
    "\n",
    "##### Executing the Graph\n",
    "For a single, synchronous execution of the graph, we can use the `invoke()` method. The `invoke()` method takes a single argument: a dictionary that defines the **initial state** of the graph. The keys in this dictionary must correspond to the fields you defined in your State schema. For instance, if your State object has a field named graph_state, your input to invoke would look like {\"graph_state\": \"initial value for graph_state\"}.\n",
    "\n",
    "##### Execution Process\n",
    "\n",
    "* Initialization: When `invoke()` is called, the provided input dictionary populates the graph's initial state.\n",
    "* Starting Point: Execution begins at the special START node you defined when adding edges.\n",
    "* Node Execution & State Updates: The graph then transitions to the first actual node (e.g., node_1 in our example). This node function executes, receiving the current state. It performs its logic and returns a dictionary of updates. These updates are then applied to the graph's state (by default, overriding the values of the returned keys).\n",
    "* Edge Traversal: After a node completes, the graph consults its edges to determine the next step:\n",
    "* * If there's a normal edge, it proceeds to the specified next node.\n",
    "* * If there's a conditional edge (like the one from node_1 using our decide_mood function), the condition function is called with the current state. The return value of this function dictates which node to visit next (e.g., node_2 or node_3 based on our 50/50 logic).\n",
    "* Continuation: This process of node execution, state update, and edge traversal continues. The state evolves with each step, carrying information and context through the graph.\n",
    "* Termination: The execution continues until a path leads to the special END node. Once END is reached, that particular execution path of the graph concludes.\n",
    "\n",
    "The `invoke()` method returns the **final state** of the graph after it has reached an END node. This final state will contain all the accumulated and modified data from the sequence of executed nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571984d-aaec-45cd-bc61-6408b2939020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph.invoke({\"graph_state\": \"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079aa58d-27d5-4443-88b2-f0ac2be9c133",
   "metadata": {},
   "source": [
    "## Simple Agent with Function Calling tool\n",
    "\n",
    "In many real-world applications, an LLM needs to:\n",
    "* Access up-to-date information (e.g., search the web, query a database).\n",
    "* Perform precise calculations (like the mathematical operations we'll explore).\n",
    "* Interact with other APIs or external systems.\n",
    "\n",
    "Function Calling (or Tool Use) allows an LLM to indicate when it needs to invoke such external capabilities. Within a LangGraph structure, we can:\n",
    "* Have a node (often an LLM) that decides if and which tool to call based on the current state and task.\n",
    "* Execute that tool (which could be another node or an external Python function).\n",
    "* Feed the tool's output back into the graph's state, allowing subsequent nodes (perhaps the LLM again) to process these results and continue the task.\n",
    "\n",
    "This section will demonstrate how to integrate tool-calling functionality, transforming our graph into a simple agent capable of deciding when to use a tool and incorporating its results into its workflow. This mirrors how, in more basic function calling setups, a model might choose to call a tool and then process a ToolMessage containing the outcome.\n",
    " \n",
    "![Screenshot 2024-08-21 at 12.45.43 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0b4a2c1e5e02f3e78b_agent2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ec193-ee71-43b3-bd2a-19b0e8f828ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI, VertexAI, VertexAIEmbeddings\n",
    "\n",
    "llm = ChatVertexAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71432edb-2332-48bf-a29a-aa52a0bbde16",
   "metadata": {},
   "source": [
    "Define the functions/tools as Python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be573c-d487-4637-ab71-7a7342dfbdb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1835f3-5ca7-4439-a773-330b8b261b18",
   "metadata": {},
   "source": [
    "#### Defining the assistant node\n",
    "\n",
    "Here is what we need:\n",
    "* State with MessagesState: We use MessagesState to keep track of our conversation. It holds a list of messages (human, AI, tool) under the messages key and new messages are appended, maintaining history. Our assistant node, as defined, will leverage this.\n",
    "\n",
    "* Assistant Node: This node houses our LLM (llm_with_tools), which is equipped to call functions. It takes the current MessagesState, includes the system prompt, and generates a response. This response might be a direct answer or a request to use a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cda783-8353-4dd8-af4a-0c71efa6b799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba1d2c-940a-4328-aa62-28a862519bd3",
   "metadata": {},
   "source": [
    "#### Agentic Loop for Tool Use\n",
    "To make tools functional, we'll create a graph with a loop:\n",
    "\n",
    "1. The assistant runs.\n",
    "2. A conditional edge then checks the assistant's output.\n",
    "* If it's a tool call, flow goes to a Tool Executor node (which we'll define to run the actual tool function).\n",
    "* If it's a direct answer, flow goes to END.\n",
    "3. After the Tool Executor runs and adds the tool's result (as a ToolMessage) to the state, the graph routes back to the assistant.\n",
    "\n",
    "This loop (assistant -> conditional check -> Tool Executor -> assistant) allows the agent to call a tool, see its output, and then decide on the next action—either calling another tool or providing a final response. The process stops when the assistant generates a response that isn't a tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c4640-b031-4620-aab0-96fbd4c23b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f68be-b4d9-4b6b-9219-91edcc9cd863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "react_graph = builder.compile()\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac1b4a-2dc8-46f5-b44b-66e51e399a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3.\")}\n",
    "\n",
    "# thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "for event in react_graph.stream(initial_input, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc5aca-1f72-44b1-9582-62ca7a016ea9",
   "metadata": {},
   "source": [
    "## Breakpoint and Human-in-the-loop\n",
    "\n",
    "While automated agents are powerful, sometimes we need points for human oversight or intervention. LangGraph facilitates this through breakpoints and checkpointing.\n",
    "\n",
    "By setting breakpoints, we can pause the graph's execution just before certain nodes run. This pause allows a human to review the current state, potentially modify it, or validate the agent's next intended action before allowing it to proceed.\n",
    "\n",
    "To enable this, we use:\n",
    "\n",
    "* Checkpointers (like MemorySaver): These save the graph's state at various points, including when it's interrupted. This ensures that we can resume execution later.\n",
    "* The interrupt_before argument during graph compilation. This tells LangGraph which node(s) should trigger an interruption before they execute.\n",
    "\n",
    "The following code will demonstrate how to compile our graph with a checkpointer and set an interruption point, allowing us to inspect the flow and simulate a human-in-the-loop scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ece1e-ae92-42d9-89f0-4c4bbaabe255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "react_graph = builder.compile(checkpointer=memory, interrupt_before=[\"tools\"])\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "for event in react_graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2f296-663c-4cfd-8ff9-1e518a779ffb",
   "metadata": {},
   "source": [
    "In the preceding step, we compiled our graph using MemorySaver for checkpointing and specified `interrupt_before=[\"tools\"]`. This tells the graph to pause execution just before the \"tools\" node (our tool executor) would normally run. We also initiated a stream using thread_id: \"1\". During that stream, the graph processed the input and, upon reaching the \"tools\" node, would have paused, saving its current state associated with thread_id: \"1\" into our memory checkpointer.\n",
    "\n",
    "Now, we'll demonstrate a more explicit human-in-the-loop scenario. We will:\n",
    "\n",
    "* Start a new, independent execution stream with a fresh thread_id: \"2\". This allows us to manage multiple graph executions and their states separately.\n",
    "* Let this new stream run until it hits the same `interrupt_before=[\"tools\"]` breakpoint.\n",
    "* At this pause, we will then simulate a human making a decision by prompting for input.\n",
    "* Based on this input, we will either resume the graph's execution for thread_id: \"2\" from where it left off (if approved) or halt further processing for this thread.\n",
    "\n",
    "This illustrates how you can intervene in an agent's workflow, inspect its intentions, and guide its actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49b364-ffc6-4574-be37-4e3f50645670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in react_graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Get user feedback\n",
    "user_approval = input(\"Do you want to call the tool? (yes/no): \")\n",
    "\n",
    "# Check approval\n",
    "if user_approval.lower() == \"yes\":\n",
    "\n",
    "    # If approved, continue the graph execution\n",
    "    for event in react_graph.stream(None, thread, stream_mode=\"values\"):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "else:\n",
    "    print(\"Operation cancelled by user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d6db2-b234-4237-bd8f-1a5424df13ab",
   "metadata": {},
   "source": [
    "---\n",
    "## Build a complex Essays writing agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcbd59-279b-4f07-9ac3-80e5a200f62c",
   "metadata": {},
   "source": [
    "This section demonstrates how to build a LangGraph-powered AI agent to generate, revise, and critique essays using Gemini. The LangGraph code was adapted from the awesome DeepLearning.AI course on AI Agents in LangGraph.\n",
    "\n",
    "By defining a structured state flow with nodes such as \"Planner,\" \"Research Plan,\" \"Generate,\" \"Reflect,\" and \"Research Critique,\" the system iteratively creates an essay on a given topic, incorporates feedback, and provides research-backed insights.\n",
    "\n",
    "The agent will perform the following steps:\n",
    "1.  **Plan:** Create an initial outline for the essay based on the given topic.\n",
    "3.  **Generate:** Write a draft of the essay using the plan and researched content.\n",
    "4.  **Reflect:** Critique the generated draft, identifying areas for improvement.\n",
    "6.  **Loop:** The generation, reflection, and research critique steps can loop a specified number of times to iteratively improve the essay.\n",
    "\n",
    "The workflow enables automated essay generation with revision controls, making it ideal for structured writing tasks or educational use cases. Additionally, the notebook uses external search tools to gather and integrate real-time information into the essay content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5a730-cf29-432e-91fb-d7ee70dbed1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d9e57-feb1-4601-8f11-98563c96942b",
   "metadata": {},
   "source": [
    "### Configure Google Gen AI SDK with Google Search\n",
    "In this agentic flow, in addition to LangChain, let's also use Gen AI SDK so that we can easily leverage the power of Google Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e596fb-d8af-4660-b3a9-04697476f404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Configure the client\n",
    "client = genai.Client(vertexai=True, location=\"us-central1\")\n",
    "\n",
    "# Define the grounding tool\n",
    "grounding_tool = types.Tool(google_search=types.GoogleSearch())\n",
    "\n",
    "# Configure generation settings\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[grounding_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce64436-e887-4d9b-bc2d-29dbf244e52b",
   "metadata": {},
   "source": [
    "### Initialize agent memory, agent state, and schema for search queries\n",
    "\n",
    "Before defining the agent's nodes and graph structure, we need to set up the foundational components:\n",
    "* **Agent State (`AgentState`):** This pydantic `BaseModel` will hold all the information that flows through our graph. It includes the task, essay plan, draft, critique, revision number, and maximum allowed revisions. Each node in the graph will read from and write to this state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad89dc4e-27d3-4d14-a631-6ac14e68bec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AgentState(BaseModel):\n",
    "    task: str\n",
    "    plan: Optional[str] = \"\"\n",
    "    draft: Optional[str] = \"\"\n",
    "    critique: Optional[str] = \"\"\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01ed5a-cd6e-494a-aeab-49ba26ce389a",
   "metadata": {},
   "source": [
    "### Define prompt templates for each stage\n",
    "\n",
    "Clear and effective prompts are essential for guiding the LLM at each stage of the essay writing process. We define specific prompt templates for:\n",
    "* **PLAN_PROMPT:** Instructs the LLM to act as an expert writer and create a high-level outline for the essay.\n",
    "* **WRITER_PROMPT:** Instructs the LLM to act as an essay assistant, write a 3-page essay based on the plan and research, and revise based on critique. It also specifies Markdown formatting.\n",
    "* **REFLECTION_PROMPT:** Tells the LLM to act as a teacher grading the essay, providing critique and detailed recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b236d0-46d6-4b3b-b6c3-a5a10abbead7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay.\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any\n",
    "relevant notes or instructions for the sections.\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 3-pages essays.\n",
    "First, gather information using the search tool to write a great essay.\n",
    "Then, generate the best essay possible for the user's request.\n",
    "If the user provides critique , respond with a revised version of your previous attempts.\n",
    "Use Markdown formatting to specify a title and section headers for each paragraph.\n",
    "\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission.\n",
    "Generate critique and recommendations for the user's submission.\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\n",
    "Also, provide suggetion about additional information the writer should gather.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fb67c-afc3-4afb-8259-bbc9d7462d1a",
   "metadata": {},
   "source": [
    "### Define node functions for each stage\n",
    "\n",
    "Each node in our LangGraph represents a specific step in the essay writing workflow. These Python functions take the current `AgentState` as input and return a dictionary with the updated state values for their respective operations:\n",
    "\n",
    "* `plan_node`: Invokes the LLM with the `PLAN_PROMPT` to generate an essay outline.\n",
    "* `generation_node`: Invokes the LLM with the `WRITER_PROMPT`, which calls the google search tool to gather information, to generate an essay draft. It also increments the revision number.\n",
    "* `reflection_node`: Uses the LLM with the `REFLECTION_PROMPT` to critique the current draft.\n",
    "* `should_continue`: This is a conditional edge function. It checks if the current `revision_number` has exceeded `max_revisions`. If so, it returns `END` to terminate the graph; otherwise, it returns \"reflect\" to continue the revision cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a37154-1a4f-433e-a2dc-56d0a146d428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content=state.task),\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"plan\": response.content}\n",
    "\n",
    "\n",
    "# Generates a draft based on the content and plan\n",
    "def generation_node(state: AgentState):\n",
    "    # Configure generation settings\n",
    "    config = types.GenerateContentConfig(\n",
    "        tools=[grounding_tool], system_instruction=WRITER_PROMPT\n",
    "    )\n",
    "    contents = f\"Task: {state.task}, critique: {state.critique}\"\n",
    "\n",
    "    # Make the request\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=contents,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Display the search queries\n",
    "    if response.candidates[0].grounding_metadata is not None:\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"**Generator is gathering information with these search queries...:**\"\n",
    "            )\n",
    "        )\n",
    "        display(\n",
    "            HTML(\n",
    "                response.candidates[\n",
    "                    0\n",
    "                ].grounding_metadata.search_entry_point.rendered_content\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"draft\": response.text,\n",
    "        \"revision_number\": state.revision_number + 1,\n",
    "    }\n",
    "\n",
    "\n",
    "# Provides feedback or critique on the draft\n",
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state.draft),\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"critique\": response.content}\n",
    "\n",
    "\n",
    "# Determines whether the critique and research cycle should\n",
    "# continue based on the number of revisions\n",
    "def should_continue(state):\n",
    "    if state.revision_number >= state.max_revisions:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78534ac3-e0bc-4450-b3be-7c96165861ad",
   "metadata": {},
   "source": [
    "### Define and compile the graph\n",
    "\n",
    "With the state, prompts, and node functions defined, we can now construct the essay writing agent's graph:\n",
    "1.  **Initialize `StateGraph`:** We create an instance of `StateGraph` with our `AgentState`.\n",
    "2.  **Add Nodes:** Each function defined in the previous step (`plan_node`, `generation_node`, etc.) is added as a node to the graph, with a unique string identifier.\n",
    "3.  **Set Entry Point:** `builder.set_entry_point(\"planner\")` designates the `planner` node as the starting point of the graph.\n",
    "4.  **Add Edges:**\n",
    "    * Sequential Edges: We define the primary flow: `planner` -> `generate`. And the revision loop: `reflect` -> `generate`.\n",
    "    * Conditional Edges: `builder.add_conditional_edges` is used after the `generate` node. It calls the `should_continue` function. Based on its return value, the graph either transitions to the `reflect` node (to continue revising) or to `END` (to finish).\n",
    "5.  **Initialize Memory:** A `MemorySaver` is initialized to enable checkpointing, which saves the state of the graph.\n",
    "6.  **Compile Graph:** `builder.compile(checkpointer=memory)` finalizes the graph structure and incorporates the memory saver, making the graph ready for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba9da6-20d9-45d3-8bf1-1331c67b4e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the state graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes for each step in the workflow\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "\n",
    "# Set the entry point of the workflow\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "# Add conditional edges for task continuation or end\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", should_continue, {END: END, \"reflect\": \"reflect\"}\n",
    ")\n",
    "\n",
    "# Define task sequence edges\n",
    "builder.add_edge(\"planner\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "\n",
    "# Initialize agent memory\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with memory state management\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c66c38-f4f4-42fe-b48d-f20c29bb158e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a275491-136b-4bb9-a1af-d7f941d1f88f",
   "metadata": {},
   "source": [
    "### Run the agent - write on!\n",
    "\n",
    "Now it's time to run our essay writing agent. We'll provide an initial essay topic and configuration, then stream the agent's execution to observe each step and its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e58ee3-9299-407c-941e-14f77b0b3ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the topic of the essay\n",
    "ESSAY_TOPIC = (\n",
    "    \"What were the impacts of Hurricane Helene and Hurricane Milton in 2024?\"\n",
    ")\n",
    "\n",
    "# Define a thread configuration with a unique thread ID\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Stream through the graph execution with an initial task and state\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"task\": ESSAY_TOPIC,  # Initial task\n",
    "        \"max_revisions\": 3,  # Maximum number of revisions allowed\n",
    "        \"revision_number\": 0,  # Current revision number\n",
    "    },\n",
    "    thread,\n",
    "):\n",
    "    step = next(iter(s))\n",
    "    display(Markdown(f\"# {step}\"))\n",
    "    for key, content in s[step].items():\n",
    "        if key == \"revision_number\":\n",
    "            display(Markdown(f\"**Revision Number**: {content}\"))\n",
    "        elif isinstance(content, list):\n",
    "            for c in content:\n",
    "                display(Markdown(c))\n",
    "        else:\n",
    "            display(Markdown(content))\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93624d4-bda2-40b3-8943-03f792bc64e0",
   "metadata": {},
   "source": [
    "### Dive Deeper into LangGraph\n",
    "Ready to explore further? The official LangGraph documentation is an excellent resource:\n",
    "\n",
    "* [Core Concepts](https://langchain-ai.github.io/langgraph/): For a refresher on the foundational elements like State, Nodes, and Edges.\n",
    "* [Tutorials](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/): See LangGraph in action with a variety of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e69a4-4224-4003-b11b-3f4876860a25",
   "metadata": {
    "tags": []
   },
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7150b9-ab4d-44b1-9f76-c29bcdeebfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

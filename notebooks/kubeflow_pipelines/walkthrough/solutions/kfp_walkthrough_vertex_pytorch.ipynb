{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using custom containers with Vertex AI Training\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Learn how to create a train and a validation split with BigQuery\n",
    "1. Learn how to wrap a machine learning model into a Docker container and train in on Vertex AI\n",
    "1. Learn how to use the hyperparameter tuning engine on Vertex AI to find the best hyperparameters\n",
    "1. Learn how to deploy a trained machine learning model on Vertex AI as a REST API and query it\n",
    "\n",
    "In this lab, you develop, package as a docker image, and run on **Vertex AI Training** a training application that trains a multi-class classification model that predicts the type of forest cover from cartographic data. The [dataset](../../../datasets/covertype/README.md) used in the lab is based on **Covertype Data Set** from UCI Machine Learning Repository.\n",
    "\n",
    "The training code uses `scikit-learn` for data pre-processing and pytorch for modeling. The code has been instrumented using the `hypertune` package so it can be used with **Vertex AI** hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install\n",
    "```\n",
    "pip install --upgrade torch==1.11.0 torchtext==0.12.0 --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "pip install --upgrade torch-model-archiver\n",
    "pip install --upgrade google-cloud-aiplatform[prediction]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext.vocab import vocab\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set location paths, connections strings, and other environment settings. Make sure to update   `REGION`, and `ARTIFACT_STORE`  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for Vertex AI Training and Prediction\n",
    "- `ARTIFACT_STORE` - A GCS bucket in the created in the same region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\"\n",
    "\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "JOB_DIR_ROOT = f\"{ARTIFACT_STORE}/jobs\"\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JOB_DIR_ROOT\"] = JOB_DIR_ROOT\n",
    "os.environ[\"TRAINING_FILE_PATH\"] = TRAINING_FILE_PATH\n",
    "os.environ[\"VALIDATION_FILE_PATH\"] = VALIDATION_FILE_PATH\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the `ARTIFACT_STORE` bucket if it's not there. Note that this bucket should be created in the region specified in the variable `REGION` (if you have already a bucket with this name in a different region than `REGION`, you may want to change the `ARTIFACT_STORE` name so that you can recreate a bucket in `REGION` with the command in the cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=covertype_dataset\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://asl-public/data/covertype/dataset.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Covertype dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM `covertype_dataset.covertype`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation splits\n",
    "\n",
    "Use BigQuery to sample training and validation splits and save them to GCS storage\n",
    "### Create a training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (1, 2, 3, 4)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.training \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.validation \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (8)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.validation \\\n",
    "$VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a training application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "NUMERICAL_FEATURES = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "LABEL_COLUMN = \"Cover_Type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformerBase:\n",
    "    def fit(self, series: pd.Series):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def transform(self, feature: list) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit_transform(self, series: pd.Series) -> torch.Tensor:\n",
    "        self.fit(series)\n",
    "        return self.transform(series)\n",
    "    \n",
    "    def seriarize_constants(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "class OneHotEncoder(FeatureTransformerBase):\n",
    "    def fit(self, series):\n",
    "        categories = series.unique().tolist()\n",
    "        dictionary = vocab(OrderedDict([(cat, 1) for cat in categories]))\n",
    "        self.dictionary = dictionary\n",
    "        return self\n",
    "\n",
    "    def transform(self, feature, dictionary=None):\n",
    "        if dictionary:\n",
    "            assert type(dictionary) == list\n",
    "            self.dictionary = vocab(OrderedDict([(cat, 1) for cat in dictionary]))\n",
    "\n",
    "        indices = self.dictionary.lookup_indices(feature)\n",
    "        indices = torch.tensor(indices, dtype=torch.long)\n",
    "        one_hot = F.one_hot(indices, num_classes=len(self.dictionary.get_itos()))\n",
    "        return torch.tensor(one_hot).float()\n",
    "\n",
    "    def seriarize_constants(self):\n",
    "        return {\"dictionary\": self.dictionary.get_itos()}\n",
    "\n",
    "        \n",
    "class StandardScaler(FeatureTransformerBase):\n",
    "    def fit(self, series):\n",
    "        self.mean = np.float64(series.mean())\n",
    "        self.std = np.float64(series.std())\n",
    "        return self\n",
    "\n",
    "    def transform(self, feature, mean=None, std=None):\n",
    "        if mean:\n",
    "            self.mean = np.float64(mean)\n",
    "        if std:\n",
    "            self.std = np.float64(std)\n",
    "            \n",
    "        standardized = (feature - self.mean) / self.std\n",
    "        return torch.tensor(standardized)[:, None].float()\n",
    "    \n",
    "    def seriarize_constants(self):\n",
    "        return {\"mean\": self.mean, \"std\": self.std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, transformers):\n",
    "    transformed_features = [transformers[c].transform(df[c].to_list()) for c in df.columns if c != LABEL_COLUMN]\n",
    "    features = torch.cat(transformed_features, 1)\n",
    "    \n",
    "    label = df[LABEL_COLUMN].to_list()\n",
    "    label = torch.LongTensor(label)\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproc Categorical columns\n",
    "transformers = {c_feature: OneHotEncoder().fit(df_train[c_feature]) for c_feature in CATEGORICAL_FEATURES}\n",
    "transformers.update({n_feature: StandardScaler().fit(df_train[n_feature]) for n_feature in NUMERICAL_FEATURES})\n",
    "\n",
    "training_transformed = preprocess(df_train, transformers)\n",
    "validation_transformed = preprocess(df_validation, transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export preprocessing setting file for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export json for preprocessing\n",
    "preprocesinng_json = {c: transformers[c].seriarize_constants() for c in df_train.columns if c != LABEL_COLUMN}\n",
    "\n",
    "with open('preprocessing.json', 'w') as f:\n",
    "    json.dump(preprocesinng_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesinng_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Model and training/validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "# Model\n",
    "class NeuralNetworkModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(54, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear_relu_stack(x)\n",
    "        return y\n",
    "\n",
    "model = NeuralNetworkModel().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            validation_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    validation_loss /= num_batches\n",
    "    correct /= size\n",
    "    return correct, validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = TensorDataset(*training_transformed)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "validation_dataset = TensorDataset(*validation_transformed)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    accuracy, validation_loss = validation(validation_dataloader, model, loss_fn, device)\n",
    "    print(f\"Validation Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {validation_loss:>8f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "### Prepare the hyperparameter tuning application.\n",
    "Since the training run on this dataset is computationally expensive you can benefit from running a distributed hyperparameter tuning job on Vertex AI Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = \"training_app\"\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the tuning script. \n",
    "\n",
    "Notice the use of the `hypertune` package to report the `accuracy` optimization metric to Vertex AI hyperparameter tuning service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext.vocab import vocab\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "CATEGORICAL_FEATURES = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "NUMERICAL_FEATURES = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "LABEL_COLUMN = \"Cover_Type\"\n",
    "\n",
    "\n",
    "class FeatureTransformerBase:\n",
    "    def fit(self, series: pd.Series):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def transform(self, feature: list) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit_transform(self, series: pd.Series) -> torch.Tensor:\n",
    "        self.fit(series)\n",
    "        return self.transform(series)\n",
    "    \n",
    "    def seriarize_constants(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "class OneHotEncoder(FeatureTransformerBase):\n",
    "    def fit(self, series):\n",
    "        categories = series.unique().tolist()\n",
    "        dictionary = vocab(OrderedDict([(cat, 1) for cat in categories]))\n",
    "        self.dictionary = dictionary\n",
    "        return self\n",
    "\n",
    "    def transform(self, feature, dictionary=None):\n",
    "        if dictionary:\n",
    "            assert type(dictionary) == list\n",
    "            self.dictionary = vocab(OrderedDict([(cat, 1) for cat in dictionary]))\n",
    "\n",
    "        indices = self.dictionary.lookup_indices(feature)\n",
    "        indices = torch.tensor(indices, dtype=torch.long)\n",
    "        one_hot = F.one_hot(indices, num_classes=len(self.dictionary.get_itos()))\n",
    "        return torch.tensor(one_hot).float()\n",
    "\n",
    "    def seriarize_constants(self):\n",
    "        return {\"dictionary\": self.dictionary.get_itos()}\n",
    "        \n",
    "        \n",
    "class StandardScaler(FeatureTransformerBase):\n",
    "    def fit(self, series):\n",
    "        self.mean = np.float64(series.mean())\n",
    "        self.std = np.float64(series.std())\n",
    "        return self\n",
    "\n",
    "    def transform(self, feature, mean=None, std=None):\n",
    "        if mean:\n",
    "            self.mean = np.float64(mean)\n",
    "        if std:\n",
    "            self.std = np.float64(std)\n",
    "            \n",
    "        standardized = (feature - self.mean) / self.std\n",
    "        return torch.tensor(standardized)[:, None].float()\n",
    "    \n",
    "    def seriarize_constants(self):\n",
    "        return {\"mean\": self.mean, \"std\": self.std}\n",
    "\n",
    "\n",
    "def preprocess(df, transformers):\n",
    "    transformed_features = [transformers[c].transform(df[c].to_list()) for c in df.columns if c != LABEL_COLUMN]\n",
    "    features = torch.cat(transformed_features, 1)\n",
    "    \n",
    "    label = df[LABEL_COLUMN].to_list()\n",
    "    label = torch.LongTensor(label)\n",
    "    return features, label\n",
    "\n",
    "    \n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, batch_size, max_iter, hptune):\n",
    "    \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    \n",
    "    # Preproc Categorical columns\n",
    "    transformers = {c_feature: OneHotEncoder().fit(df_train[c_feature]) for c_feature in CATEGORICAL_FEATURES}\n",
    "    transformers.update({n_feature: StandardScaler().fit(df_train[n_feature]) for n_feature in NUMERICAL_FEATURES})\n",
    "\n",
    "    training_transformed = preprocess(df_train, transformers)\n",
    "    validation_transformed = preprocess(df_validation, transformers)\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    train_dataset = TensorDataset(*training_transformed)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    validation_dataset = TensorDataset(*validation_transformed)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Set device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Model\n",
    "    class NeuralNetworkModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(54, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 7)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            y = self.linear_relu_stack(x)\n",
    "            return y\n",
    "\n",
    "    model = NeuralNetworkModel().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    def train(dataloader, model, loss_fn, optimizer, device):\n",
    "        model.train()\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def validation(dataloader, model, loss_fn, device):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        validation_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(X)\n",
    "                validation_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        validation_loss /= num_batches\n",
    "        correct /= size\n",
    "        return correct, validation_loss\n",
    "\n",
    "    epochs = max_iter\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "\n",
    "    if hptune:\n",
    "        accuracy, _ = validation(validation_dataloader, model, loss_fn, device)\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    if not hptune:\n",
    "        # Save the model\n",
    "        model_filename = \"model.pt\"\n",
    "        model_scripted = torch.jit.script(model)\n",
    "        model_scripted.save(model_filename)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "\n",
    "        # export json for preprocessing\n",
    "        preprocesinng_json = {c: transformers[c].seriarize_constants() for c in df_train.columns if c != LABEL_COLUMN}\n",
    "        preproc_json_filename = 'preprocessing.json'\n",
    "        with open(preproc_json_filename, 'w') as f:\n",
    "            json.dump(preprocesinng_json, f)\n",
    "        gcs_preprocessing_json_path = \"{}/{}\".format(job_dir, preproc_json_filename)\n",
    "\n",
    "        # send files to GCS\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        subprocess.check_call(['gsutil', 'cp', preproc_json_filename, gcs_preprocessing_json_path], stderr=sys.stdout)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "Notice that we are installing specific versions of `scikit-learn` and `pandas` in the training image. This is done to make sure that the training runtime in the training container is aligned with the serving runtime in the serving container. \n",
    "\n",
    "Make sure to update the URI for the base image so that it points to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11:latest\n",
    "RUN pip install -U fire cloudml-hypertune pandas==0.25.3\n",
    "RUN pip install -U torchtext==0.12.0\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the docker image. \n",
    "\n",
    "You use **Cloud Build** to build the image and push it your project's **Container Registry**. As you use the remote cloud service to build the image, you don't need a local installation of Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"trainer_image\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit an Vertex AI hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the training code uses `SGDClassifier`. The training application has been designed to accept two hyperparameters that control `SGDClassifier`:\n",
    "- Max iterations\n",
    "- Alpha\n",
    "\n",
    "The file below configures Vertex AI hypertuning to run up to 5 trials in parallel and to choose from two discrete values of `max_iter` and the linear range between `1.0e-4` and `1.0e-1` for `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"forestcover_tuning_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "CONFIG_YAML=config.yaml\n",
    "\n",
    "cat <<EOF > $CONFIG_YAML\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: accuracy\n",
    "    goal: MAXIMIZE\n",
    "  parameters:\n",
    "  - parameterId: max_iter\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 10\n",
    "      - 20\n",
    "  - parameterId: batch_size\n",
    "    integerValueSpec:\n",
    "      minValue: 16\n",
    "      maxValue: 128\n",
    "    scaleType: SCALE_TYPE_UNSPECIFIED\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  workerPoolSpecs:  \n",
    "  - machineSpec:\n",
    "      machineType: $MACHINE_TYPE\n",
    "    replicaCount: $REPLICA_COUNT\n",
    "    containerSpec:\n",
    "      imageUri: $IMAGE_URI\n",
    "      args:\n",
    "      - --job_dir=$JOB_DIR\n",
    "      - --training_dataset_path=$TRAINING_FILE_PATH\n",
    "      - --validation_dataset_path=$VALIDATION_FILE_PATH\n",
    "      - --hptune\n",
    "EOF\n",
    "\n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=$CONFIG_YAML \\\n",
    "    --max-trial-count=5 \\\n",
    "    --parallel-trial-count=5\n",
    "\n",
    "echo \"JOB_NAME: $JOB_NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the Vertex AI Training dashboard and view the progression of the HP tuning job under \"Hyperparameter Tuning Jobs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job completes you can review the results using GCP Console or programmatically using the following functions (note that this code supposes that the metrics that the hyperparameter tuning engine optimizes is maximized): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials(job_name):\n",
    "    jobs = aiplatform.HyperparameterTuningJob.list()\n",
    "    match = [job for job in jobs if job.display_name == JOB_NAME]\n",
    "    tuning_job = match[0] if match else None\n",
    "    return tuning_job.trials if tuning_job else None\n",
    "\n",
    "\n",
    "def get_best_trial(trials):\n",
    "    metrics = [trial.final_measurement.metrics[0].value for trial in trials]\n",
    "    best_trial = trials[metrics.index(max(metrics))]\n",
    "    return best_trial\n",
    "\n",
    "\n",
    "def retrieve_best_trial_from_job_name(jobname):\n",
    "    trials = get_trials(jobname)\n",
    "    best_trial = get_best_trial(trials)\n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to wait for the hyperparameter job to complete before being able to retrieve the best job by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = retrieve_best_trial_from_job_name(JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model with the best hyperparameters\n",
    "\n",
    "You can now retrain the model using the best hyperparameters and using combined training and validation splits as a training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best_trial.parameters[0].value)\n",
    "max_iter = int(best_trial.parameters[1].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"JOB_VERTEX_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "\n",
    "WORKER_POOL_SPEC = f\"\"\"\\\n",
    "machine-type={MACHINE_TYPE},\\\n",
    "replica-count={REPLICA_COUNT},\\\n",
    "container-image-uri={IMAGE_URI}\\\n",
    "\"\"\"\n",
    "\n",
    "ARGS = f\"\"\"\\\n",
    "--job_dir={JOB_DIR},\\\n",
    "--training_dataset_path={TRAINING_FILE_PATH},\\\n",
    "--validation_dataset_path={VALIDATION_FILE_PATH},\\\n",
    "--batch_size={batch_size},\\\n",
    "--max_iter={max_iter},\\\n",
    "--nohptune\\\n",
    "\"\"\"\n",
    "\n",
    "!gcloud ai custom-jobs create \\\n",
    "  --region={REGION} \\\n",
    "  --display-name={JOB_NAME} \\\n",
    "  --worker-pool-spec={WORKER_POOL_SPEC} \\\n",
    "  --args={ARGS}\n",
    "\n",
    "\n",
    "print(\"The model will be exported at:\", JOB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the training output\n",
    "\n",
    "The training script saved the trained model as the 'model.pkl' in the `JOB_DIR` folder on GCS.\n",
    "\n",
    "**Note:** We need to wait for job triggered by the cell above to complete before running the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to Vertex AI Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_APP_FOLDER = \"predict_app\"\n",
    "os.makedirs(PREDICT_APP_FOLDER, exist_ok=True)\n",
    "os.environ[\"PREDICT_APP_FOLDER\"] = PREDICT_APP_FOLDER\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Torchserve handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PREDICT_APP_FOLDER}/handler.py\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchtext.vocab import vocab\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "CATEGORICAL_FEATURES = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "NUMERICAL_FEATURES = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "    \n",
    "class ModelHandler(BaseHandler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._context = None\n",
    "        self.initialized = False\n",
    "        self.explain = False\n",
    "        self.target = 0\n",
    "\n",
    "\n",
    "    def _one_hot_transformer(self, feature, dictionary):\n",
    "        dictionary = vocab(OrderedDict([(cat, 1) for cat in dictionary]))\n",
    "        \n",
    "        indices = dictionary.lookup_indices(feature)\n",
    "        indices = torch.tensor(indices, dtype=torch.long)\n",
    "        one_hot = F.one_hot(indices, num_classes=len(dictionary.get_itos()))\n",
    "        return torch.tensor(one_hot).float()\n",
    "\n",
    "    def _standard_scale_transformer(self, feature, mean, std):\n",
    "        standardized = (feature - np.float64(mean)) / np.float64(std)\n",
    "        return torch.tensor(standardized)[:, None].float()    \n",
    "\n",
    "\n",
    "    def initialize(self, context):\n",
    "        self._context = context\n",
    "        self.initialized = True\n",
    "        \n",
    "        #  load the model\n",
    "        self.manifest = context.manifest\n",
    "\n",
    "        properties = context.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest['model']['serializedFile']\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.pth file\")\n",
    "\n",
    "        self.model = torch.jit.load(model_pt_path)\n",
    "        \n",
    "        # Read preprocesinng setting\n",
    "        preprocessing_json_path = os.path.join(model_dir, \"preprocessing.json\")\n",
    "        with open(preprocessing_json_path) as f:\n",
    "            self.preprocessing_json = json.load(f)\n",
    "        \n",
    "        self.initialized = True\n",
    "\n",
    "\n",
    "    def preprocess(self, _data):\n",
    "        data = _data[0]\n",
    "        \n",
    "        preprocessed_data = [self._one_hot_transformer(data[c_feature], self.preprocessing_json[c_feature][\"dictionary\"]) for c_feature in CATEGORICAL_FEATURES]\n",
    "        preprocessed_data.extend([self._standard_scale_transformer(data[n_feature], self.preprocessing_json[n_feature][\"mean\"], self.preprocessing_json[n_feature][\"std\"]) for n_feature in NUMERICAL_FEATURES])\n",
    "        preprocessed_data = torch.cat(preprocessed_data, 1)\n",
    "        \n",
    "        return preprocessed_data\n",
    "\n",
    "    def inference(self, model_input):\n",
    "        model_output = self.model.forward(model_input)\n",
    "        return model_output\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        postprocess_output = torch.argmax(inference_output, dim=1)        \n",
    "        return [postprocess_output.cpu().numpy().tolist()]\n",
    "\n",
    "    def handle(self, data, context):\n",
    "        model_input = self.preprocess(data)\n",
    "        model_output = self.inference(model_input)\n",
    "        output = self.postprocess(model_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MAR File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp $JOB_DIR/model.pt ./$PREDICT_APP_FOLDER\n",
    "!gsutil cp $JOB_DIR/preprocessing.json ./$PREDICT_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver -f \\\n",
    "  --model-name model \\\n",
    "  --version 1.0 \\\n",
    "  --serialized-file ./$PREDICT_APP_FOLDER/model.pt \\\n",
    "  --handler ./$PREDICT_APP_FOLDER/handler.py \\\n",
    "  --extra-files ./$PREDICT_APP_FOLDER/preprocessing.json \\\n",
    "  --export-path=./$PREDICT_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_route = \"/ping\"\n",
    "predict_route = \"/predictions/model\"\n",
    "serving_container_ports = [8080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_container_image_uri = \"us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-11:latest\"\n",
    "\n",
    "local_model = LocalModel(\n",
    "    serving_container_image_uri=serving_container_image_uri,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./instances.json\n",
    "{\n",
    "    \"instances\": [{\"Elevation\":[2841.0], \n",
    "              \"Aspect\": [45.0],\n",
    "              \"Slope\":[0.0],\n",
    "              \"Horizontal_Distance_To_Hydrology\": [644.0], \n",
    "              \"Vertical_Distance_To_Hydrology\": [282.0],\n",
    "              \"Horizontal_Distance_To_Roadways\": [1376.0], \n",
    "              \"Hillshade_9am\": [218.0], \n",
    "              \"Hillshade_Noon\": [237.0], \n",
    "              \"Hillshade_3pm\": [156.0], \n",
    "              \"Horizontal_Distance_To_Fire_Points\": [1003.0],\n",
    "              \"Wilderness_Area\": [\"Commanche\"], \n",
    "              \"Soil_Type\": [\"C4758\"]}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with local_model.deploy_to_local_endpoint(artifact_uri=PREDICT_APP_FOLDER,) as local_endpoint:\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request_file=\"./instances.json\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model to Vertex AI Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "DEPLOY_MODEL_GCS_URI = f\"{ARTIFACT_STORE}/pytorch-model-deploy/{TIMESTAMP}\"\n",
    "\n",
    "!gsutil cp -r $PREDICT_APP_FOLDER $DEPLOY_MODEL_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -al $DEPLOY_MODEL_GCS_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_display_name = f\"pytorch-model-{TIMESTAMP}\"\n",
    "model_description = \"PyTorch based forest cover classifier with the pre-built PyTorch image\"\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=serving_container_image_uri,\n",
    "    artifact_uri=DEPLOY_MODEL_GCS_URI,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the uploaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_type = \"n1-standard-4\"\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve predictions\n",
    "#### Prepare the input file with JSON formated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [{\n",
    "    \"Elevation\":[2841.0], \n",
    "    \"Aspect\": [45.0],\n",
    "    \"Slope\":[0.0],\n",
    "    \"Horizontal_Distance_To_Hydrology\": [644.0], \n",
    "    \"Vertical_Distance_To_Hydrology\": [282.0],\n",
    "    \"Horizontal_Distance_To_Roadways\": [1376.0], \n",
    "    \"Hillshade_9am\": [218.0], \n",
    "    \"Hillshade_Noon\": [237.0], \n",
    "    \"Hillshade_3pm\": [156.0], \n",
    "    \"Horizontal_Distance_To_Fire_Points\": [1003.0],\n",
    "    \"Wilderness_Area\": [\"Commanche\"], \n",
    "    \"Soil_Type\": [\"C4758\"]\n",
    "}]\n",
    "\n",
    "endpoint.predict(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa28be6-9dc8-406d-aa45-babbd2df37a3",
   "metadata": {},
   "source": [
    "# Function Calling with Gemini \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Learn about function calling and relevant use cases\n",
    "1. Learn how to implement function calling with Gemini Pro\n",
    "1. Learn patterns for handling function calls in a chat session\n",
    "1. Learn how function calling can be used in different situations and use cases \n",
    "\n",
    "Function calling allows developers to define custom functions and provide these functions to Gemini. While processing a query, Gemini can choose to delegate certain data processing tasks to these functions. Gemini does not call these functions, rather it provides structured data output that includes the name of a selected function and the arguments the function should be called with. You can use this output to perform tasks like invoking external APIs, performing mathematical computations, extracting structured data, and more. You can then provide the function response back to the model, allowing it to complete its answer to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15fc4e-2284-4c1d-bbc1-e7c855e9dd3b",
   "metadata": {},
   "source": [
    "<img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/multimodal/images/function-calling.png\" alt=\"Function Calling\" class=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64da616a-fd18-401e-9616-a98d56639965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.43.0\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "print(aiplatform.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d9e8ee-e3f0-4430-898b-43f3db4c23d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 16:48:38.627281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from vertexai.generative_models import (\n",
    "    ChatSession,\n",
    "    Content,\n",
    "    FunctionDeclaration,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39feb35-b724-4ec1-baea-2ade785c1758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484081e-ac89-48cf-b092-290297943f57",
   "metadata": {},
   "source": [
    "## Chat Session with Function Calling\n",
    "First, lets think about how function calling can be implemented within a chat session. Essentially, when the model returns a function call, instead of returning to the user, we need to invoke a Python function that executes the specified function with the provided arguments, then feeds the result back into the model. This may happen multiple times (e.g. model returns function call -> function call response fed back into model -> model returns another function call -> ... ). \n",
    "\n",
    "Our goal is to create a simple class for example chat sessions, that implements a reasoning loop in its `send_message` method. The class should be instantiated with:\n",
    "1) `model`: An instance of `GenerativeModel`\n",
    "2) `tool_handler_fn`. A Python callable that accepts the function call name (str) and the function call arguments (dict) when invoked. This function should implement the logic of the function call itself and return the result.\n",
    "\n",
    "For example, if we had a tool with function calls for reading from and writing to a database, then we may have a `tool_handler_fn` that looks like:\n",
    "\n",
    "```python\n",
    "def tool_handler_fn(fn_name, fn_args):\n",
    "    \"\"\"This assumes function call read_row has parameter row_id, and function call write_row has parameter row\"\"\"\n",
    "    if fn_name == \"read_row\":\n",
    "        result = db.read_row(fn_args[\"row_id\"])\n",
    "    elif fn_name == \"write_row\":\n",
    "        result = db.write_row(fn_args[\"row\"])\n",
    "    return result \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e76206-c1bf-4351-a2e8-71c9481a9cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChatAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: GenerativeModel,\n",
    "        tool_handler_fn: Callable[[str, dict], Any],\n",
    "        max_iterative_calls: int = 5,\n",
    "    ):\n",
    "        self.tool_handler_fn = tool_handler_fn\n",
    "        self.chat_session = model.start_chat()\n",
    "        self.max_iterative_calls = 5\n",
    "\n",
    "    def send_message(self, message: str) -> GenerationResponse:\n",
    "        response = self.chat_session.send_message(message)\n",
    "\n",
    "        # This is None if a function call was not triggered\n",
    "        fn_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "        num_calls = 0\n",
    "        # Reasoning loop. If fn_call is None then we never enter this\n",
    "        # and simply return the response\n",
    "        while fn_call:\n",
    "            if num_calls > self.max_iterative_calls:\n",
    "                break\n",
    "\n",
    "            # Handle the function call\n",
    "            fn_call_response = self.tool_handler_fn(\n",
    "                fn_call.name, dict(fn_call.args)\n",
    "            )\n",
    "            num_calls += 1\n",
    "\n",
    "            # Send the function call result back to the model\n",
    "            response = self.chat_session.send_message(\n",
    "                Part.from_function_response(\n",
    "                    name=fn_call.name,\n",
    "                    response={\n",
    "                        \"content\": fn_call_response,\n",
    "                    },\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # If the response is another function call then we want to\n",
    "            # stay in the reasoning loop and keep calling functions.\n",
    "            fn_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a82803-7dd7-4d77-b048-b30905252c4c",
   "metadata": {},
   "source": [
    "## Simple API Example\n",
    "Now that we have a way to use function calling in a chat session, let's implement some common use cases. Imagine you want to call an API to get the current weather for a specific location, when a user asks for it. This requires a mechanism to identify that the current weather is being asked for, and also to extract the location required in the API request. \n",
    "\n",
    "With function calling, this is fairly straightforward. Simply define a function declaration with the intent and required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78242dc8-1910-4b76-ae0b-1de2a22b32de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_weather_func = FunctionDeclaration(\n",
    "    name=\"current_weather\",\n",
    "    description=\"Get the current weather at a specified location\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Location\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Simulate a function that calls a weather API\n",
    "\n",
    "\n",
    "def current_weather(location: str) -> dict:\n",
    "    print(\"Executing current_weather function...\")\n",
    "    api_response = {\n",
    "        \"location\": \"New York City\",\n",
    "        \"temperature\": \"55 degrees (F)\",\n",
    "        \"wind\": \"8 mph\",\n",
    "        \"wind_direction\": \"West\",\n",
    "        \"skies\": \"clear/sunny\",\n",
    "        \"chance_of_rain\": \"0%\",\n",
    "    }\n",
    "    return api_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76fcb03-c63a-4834-b6bc-52c06ec5774c",
   "metadata": {},
   "source": [
    "Instantiate a `Tool` with the single function declaration, and then write a tool handler function to invoke when the model returns a function call. Then instantiate the model with the `Tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399f9659-3217-4f7b-8ef8-ab7b47db2f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tools can wrap around one or multiple functions\n",
    "weather_tool = Tool(\n",
    "    function_declarations=[current_weather_func],\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiate model with weather tool\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.0-pro-001\",\n",
    "    tools=[weather_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749dca7-1d9f-4e13-9ada-5577a1558a0f",
   "metadata": {},
   "source": [
    "Send a chat through the model without using `ChatAgent` to see what the response of a function call looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265d56ff-d6b2-403b-8b74-2c3fd56dfbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidates {\n",
       "  content {\n",
       "    role: \"model\"\n",
       "    parts {\n",
       "      function_call {\n",
       "        name: \"current_weather\"\n",
       "        args {\n",
       "          fields {\n",
       "            key: \"location\"\n",
       "            value {\n",
       "              string_value: \"New York City\"\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  finish_reason: STOP\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_HATE_SPEECH\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_HARASSMENT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "}\n",
       "usage_metadata {\n",
       "  prompt_token_count: 24\n",
       "  candidates_token_count: 7\n",
       "  total_token_count: 31\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = model.start_chat()\n",
    "response = chat.send_message(\"What is the weather like in New York City?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a5a9f-159c-4c76-b783-5bdf84007a4a",
   "metadata": {},
   "source": [
    "Notice how instead of returning a text response, Gemini returned the function name to call and arguments to call it with. Now implement a function that we can instantiate `ChatAgent` with, that we will pass the function name and arguments to any time Gemini returns a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb8cbef4-6f11-43b3-8a0c-44251f6e627e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing current_weather function...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "candidates {\n",
       "  content {\n",
       "    role: \"model\"\n",
       "    parts {\n",
       "      text: \"The current weather in New York City is clear and sunny with a temperature of 55 degrees Fahrenheit. There is an 0% chance of rain. The wind is blowing at 8 mph from the west.\"\n",
       "    }\n",
       "  }\n",
       "  finish_reason: STOP\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_HATE_SPEECH\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_HARASSMENT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "}\n",
       "usage_metadata {\n",
       "  prompt_token_count: 65\n",
       "  candidates_token_count: 43\n",
       "  total_token_count: 108\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weather_tool_handler_fn(fn_name: str, fn_args: dict) -> dict:\n",
    "    if fn_name == \"current_weather\":\n",
    "        return current_weather(fn_args[\"location\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown function call: {fn_name}\")\n",
    "\n",
    "\n",
    "chat = ChatAgent(model=model, tool_handler_fn=weather_tool_handler_fn)\n",
    "response = chat.send_message(\"What is the weather like in New York City?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35158569-c82b-4b40-acbf-2d24e42c22e3",
   "metadata": {},
   "source": [
    "If we take a look at the chat history, we can see that a function call was returned, our handler function was invoked, which then invoked the Python function simulating an API. The response from that was then sent back into the model and incorporated in its response about the weather!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4260763e-a73b-4f76-ac3e-4e3cf7aede1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[role: \"user\"\n",
       " parts {\n",
       "   text: \"What is the weather like in New York City?\"\n",
       " },\n",
       " role: \"model\"\n",
       " parts {\n",
       "   function_call {\n",
       "     name: \"current_weather\"\n",
       "     args {\n",
       "       fields {\n",
       "         key: \"location\"\n",
       "         value {\n",
       "           string_value: \"New York City\"\n",
       "         }\n",
       "       }\n",
       "     }\n",
       "   }\n",
       " },\n",
       " role: \"user\"\n",
       " parts {\n",
       "   function_response {\n",
       "     name: \"current_weather\"\n",
       "     response {\n",
       "       fields {\n",
       "         key: \"content\"\n",
       "         value {\n",
       "           struct_value {\n",
       "             fields {\n",
       "               key: \"chance_of_rain\"\n",
       "               value {\n",
       "                 string_value: \"0%\"\n",
       "               }\n",
       "             }\n",
       "             fields {\n",
       "               key: \"location\"\n",
       "               value {\n",
       "                 string_value: \"New York City\"\n",
       "               }\n",
       "             }\n",
       "             fields {\n",
       "               key: \"skies\"\n",
       "               value {\n",
       "                 string_value: \"clear/sunny\"\n",
       "               }\n",
       "             }\n",
       "             fields {\n",
       "               key: \"temperature\"\n",
       "               value {\n",
       "                 string_value: \"55 degrees (F)\"\n",
       "               }\n",
       "             }\n",
       "             fields {\n",
       "               key: \"wind\"\n",
       "               value {\n",
       "                 string_value: \"8 mph\"\n",
       "               }\n",
       "             }\n",
       "             fields {\n",
       "               key: \"wind_direction\"\n",
       "               value {\n",
       "                 string_value: \"West\"\n",
       "               }\n",
       "             }\n",
       "           }\n",
       "         }\n",
       "       }\n",
       "     }\n",
       "   }\n",
       " },\n",
       " role: \"model\"\n",
       " parts {\n",
       "   text: \"The current weather in New York City is clear and sunny with a temperature of 55 degrees Fahrenheit. There is an 0% chance of rain. The wind is blowing at 8 mph from the west.\"\n",
       " }]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.chat_session.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17572e-18b3-4be9-a2a8-455751271690",
   "metadata": {},
   "source": [
    "## Function calling to perform mathematical operations\n",
    "Function calling can also help in an area that LLMs have long struggled - mathematics. Language models build up deep and insightful representations of natural language, but often lack the ability to (correctly and consistently) perform mathematical operations. We can provide a degree of consistency and accuracy by creating a tool that identifies when a mathematical operation is needed, and calls a function to actually perform that operation. \n",
    "\n",
    "Create function declarations for simple mathematical operations (addition, subtraction, multiplication, division)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd781f3b-3751-4684-9908-79ec3bc0395d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"first_number\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"First number\",\n",
    "        },\n",
    "        \"second_number\": {\"type\": \"number\", \"description\": \"Second number\"},\n",
    "    },\n",
    "    \"required\": [\"first_number\", \"second_number\"],\n",
    "}\n",
    "\n",
    "add_two_numbers_func = FunctionDeclaration(\n",
    "    name=\"add_two_numbers\",\n",
    "    description=\"Add two numbers together\",\n",
    "    parameters=parameters,\n",
    ")\n",
    "\n",
    "\n",
    "subtract_two_numbers_func = FunctionDeclaration(\n",
    "    name=\"subtract_two_numbers\",\n",
    "    description=\"Subtract second_number from first_number\",\n",
    "    parameters=parameters,\n",
    ")\n",
    "\n",
    "\n",
    "multiply_two_numbers_func = FunctionDeclaration(\n",
    "    name=\"multiply_two_numbers\",\n",
    "    description=\"Multiply two numbers together\",\n",
    "    parameters=parameters,\n",
    ")\n",
    "\n",
    "\n",
    "divide_two_numbers_func = FunctionDeclaration(\n",
    "    name=\"divide_two_numbers\",\n",
    "    description=\"Divide first_number by second_number\",\n",
    "    parameters=parameters,\n",
    ")\n",
    "\n",
    "math_tool = Tool(\n",
    "    function_declarations=[\n",
    "        add_two_numbers_func,\n",
    "        subtract_two_numbers_func,\n",
    "        multiply_two_numbers_func,\n",
    "        divide_two_numbers_func,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca48def-6b12-414d-9ee6-ad2432cd7b0e",
   "metadata": {},
   "source": [
    "Instead of simulating the response from functions, lets actually write the Python functions that we will call with arguments provided when Gemini responds with a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b86df3a6-b282-46d1-a706-e70b60a0ea7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define functions for each function declaration used in the math tool\n",
    "add_two_numbers = lambda a, b: a + b\n",
    "subtract_two_numbers = lambda a, b: a - b\n",
    "multiply_two_numbers = lambda a, b: a * b\n",
    "divide_two_numbers = lambda a, b: a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7548d-aa0e-4713-a3de-683156fa4aa1",
   "metadata": {},
   "source": [
    "Now define a function to handle all of the function calls for this tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48c94689-212c-403f-bcaa-27bb72e67440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_math_fn_call(fn_name: str, fn_args: dict) -> Union[int, float]:\n",
    "    \"\"\"Handles math tool function calls.\"\"\"\n",
    "\n",
    "    print(f\"Function calling: {fn_name} with args: {fn_args}\")\n",
    "    a = fn_args[\"first_number\"]\n",
    "    b = fn_args[\"second_number\"]\n",
    "\n",
    "    if fn_name == \"add_two_numbers\":\n",
    "        result = add_two_numbers(a, b)\n",
    "\n",
    "    elif fn_name == \"subtract_two_numbers\":\n",
    "        result = subtract_two_numbers(a, b)\n",
    "\n",
    "    elif fn_name == \"multiply_two_numbers\":\n",
    "        result = multiply_two_numbers(a, b)\n",
    "\n",
    "    elif fn_name == \"divide_two_numbers\":\n",
    "        result = divide_two_numbers(a, b)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown function call: {fn_name}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53a1dc-5297-45d2-98f1-d9edf83bdf58",
   "metadata": {
    "tags": []
   },
   "source": [
    "Instantiate a model, chat agent, and test out some queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de912d28-2a6b-422c-bed0-842661fe737d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\n",
    "    \"gemini-1.0-pro-001\",\n",
    "    tools=[math_tool],\n",
    "    generation_config=GenerationConfig(temperature=0.0),\n",
    ")\n",
    "\n",
    "chat = ChatAgent(model=model, tool_handler_fn=handle_math_fn_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b83ea2e-8a44-489a-bfb0-6853050952d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calling: add_two_numbers with args: {'second_number': 1.0, 'first_number': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The answer is 2.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\"What is one plus one?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b84ad122-21e8-4517-b187-333ff4419266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calling: multiply_two_numbers with args: {'second_number': 5.0, 'first_number': 5.0}\n",
      "Function calling: add_two_numbers with args: {'second_number': 1.0, 'first_number': 4.0}\n",
      "Function calling: divide_two_numbers with args: {'first_number': 25.0, 'second_number': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The answer is 5.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\"Thanks! What is (5 * 5) / (4 + 1)?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86bcaf-6793-48b2-8190-2cb21e407085",
   "metadata": {},
   "source": [
    "Notice how Gemini called more than one function, sequentially and logically, in order to answer the question. Very cool! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4549b-38e9-45ac-bb38-4dadad2fd5d9",
   "metadata": {},
   "source": [
    "## Natural Language to SQL with Database Execution \n",
    "Function calling can be helpful with systems that require SQL generation and execution, and using the response to answer a query. Start by creating a dataset in BigQuery and cloning some public tables into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4a98056-225e-4f8a-959f-110be2d71d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'kylesteckler-sandbox:iowa_liquor_sales' successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "!bq mk --location=\"US\" iowa_liquor_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce29c24-6f68-4f7f-b758-1e4dfe1303f8",
   "metadata": {},
   "source": [
    "Create the table by querying the public data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86d3ef85-9b49-4c0c-b6b8-db762536b43b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d617f0a9c0e4a2681623d8288df2c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE iowa_liquor_sales.sales AS \n",
    "SELECT * FROM `bigquery-public-data.iowa_liquor_sales.sales`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916dcf7-ba93-4eeb-a15e-47ab8a0675e0",
   "metadata": {},
   "source": [
    "Update the schema of your table to include column definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e71c54f-0b5e-4a18-b94a-4a57b0ad0ded",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'kylesteckler-sandbox:iowa_liquor_sales.sales' successfully updated.\n"
     ]
    }
   ],
   "source": [
    "SCHEMA_FILE = \"liquor_sales_schema.json\"\n",
    "!bq show --schema --format=prettyjson bigquery-public-data:iowa_liquor_sales.sales > {SCHEMA_FILE}\n",
    "!bq update {PROJECT}:iowa_liquor_sales.sales {SCHEMA_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78089e35-7e94-4f13-92d7-5a3ba748e983",
   "metadata": {},
   "source": [
    "Create function declarations and instantiate a new `Tool`. The function declarations should be:\n",
    "1) Listing available tables `list_available_tables`\n",
    "2) Retrieving information and schema about a specific table `get_table_info`\n",
    "3) Retrieves information from BigQuery to answer a users question `sql_query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7a7307-ac28-4140-84f4-457fd2763ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since we only have one table we will just hardcode the response from this function if triggered\n",
    "list_available_tables_func = FunctionDeclaration(\n",
    "    name=\"list_available_tables\",\n",
    "    description=\"Get and list all available BigQuery tables with fully qualified IDs.\",\n",
    "    parameters={\"type\": \"object\", \"properties\": {}},\n",
    ")\n",
    "\n",
    "get_table_info_func = FunctionDeclaration(\n",
    "    name=\"get_table_info\",\n",
    "    description=\"Get information about a BigQuery table and it's schema so you can better answer user questions.\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"table_id\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Fully qualified ID of BigQuery table\",\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "sql_query_func = FunctionDeclaration(\n",
    "    name=\"sql_query\",\n",
    "    description=\"Get information from data in BigQuery using SQL queries\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": f\"SQL query on a single line (no \\\\n characters) that will help give answers to users questions when run on BigQuery. Only query tables in project: {PROJECT}\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"query\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "query_tool = Tool(\n",
    "    function_declarations=[\n",
    "        list_available_tables_func,\n",
    "        get_table_info_func,\n",
    "        sql_query_func,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a4a422-a4ba-4d74-8e70-fd093516bddf",
   "metadata": {},
   "source": [
    "Now we need to create Python functions that will be executed when the model returns any of these function calls. The functions should be implement as such:\n",
    "* `list_available_tables` should accept no parameters and simply return the name of the BigQuery table created above: `f\"{PROJECT}/iowa_liquor_sales.sales\"`\n",
    "* `get_table_info` should accept a table_id parameter and use the BigQuery client library to retrieve table information and schema \n",
    "* `sql_query` should accept a query_sting parameter and use the BigQuery client library to execute a sql query and return the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "640741b4-c326-44ff-9c3b-c2483cacf540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_available_tables():\n",
    "    return [f\"{PROJECT}.iowa_liquor_sales.sales\"]\n",
    "\n",
    "\n",
    "def get_table_info(table_id: str) -> dict:\n",
    "    \"\"\"Returns dict from BigQuery API with table information\"\"\"\n",
    "    bq_client = bigquery.Client()\n",
    "    return bq_client.get_table(table_id).to_api_repr()\n",
    "\n",
    "\n",
    "def sql_query(query_str: str):\n",
    "    bq_client = bigquery.Client()\n",
    "    try:\n",
    "        # clean up query string a bit\n",
    "        query_str = (\n",
    "            query_str.replace(\"\\\\n\", \"\").replace(\"\\n\", \"\").replace(\"\\\\\", \"\")\n",
    "        )\n",
    "        # print(query_str)\n",
    "        query_job = bq_client.query(query_str)\n",
    "        result = query_job.result()\n",
    "        result = str([dict(x) for x in result])\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error from BigQuery Query API: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b2c1f3-2434-44ec-9935-e8c947c7d58b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create a Python function to handle function calls returned by the model and invoke the needed logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b4321e6-7880-4ebb-852c-884d10e308e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_query_fn_call(fn_name: str, fn_args: dict):\n",
    "    \"\"\"Handles query tool function calls.\"\"\"\n",
    "\n",
    "    print(f\"Function calling: {fn_name} with args: {str(fn_args)}\\n\")\n",
    "    if fn_name == \"list_available_tables\":\n",
    "        result = list_available_tables()\n",
    "\n",
    "    elif fn_name == \"get_table_info\":\n",
    "        result = get_table_info(fn_args[\"table_id\"])\n",
    "\n",
    "    elif fn_name == \"sql_query\":\n",
    "        result = sql_query(fn_args[\"query\"])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown function call: {fn_name}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f56cc2-7f04-4344-9e56-1873d9f940dc",
   "metadata": {},
   "source": [
    "Instantiate a model, chat agent, and test out some queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e6b127d-a44d-4005-a89f-e2b5df974813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\n",
    "    \"gemini-1.0-pro-001\",\n",
    "    tools=[query_tool],\n",
    "    generation_config=GenerationConfig(temperature=0.0),\n",
    ")\n",
    "chat = ChatAgent(model=model, tool_handler_fn=handle_query_fn_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b495a24-9dd6-4ca8-b0c8-c6b1127761b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calling: list_available_tables with args: {}\n",
      "\n",
      "Function calling: get_table_info with args: {'table_id': 'kylesteckler-sandbox.iowa_liquor_sales.sales'}\n",
      "\n",
      "Function calling: sql_query with args: {'query': 'SELECT store_name, SUM(bottles_sold) AS total_bottles_sold FROM `kylesteckler-sandbox.iowa_liquor_sales.sales` GROUP BY store_name ORDER BY total_bottles_sold DESC LIMIT 1'}\n",
      "\n",
      "The store with the highest total number of bottles sold is HY-VEE #3 / BDI / DES MOINES, with 7,971,741 bottles sold.\n"
     ]
    }
   ],
   "source": [
    "# Insert an initialization prompt before the first chat to help guide model behavior and output style/format\n",
    "\n",
    "init_prompt = \"\"\"\n",
    "    Please give a concise and easy to understand answer to any questions. \n",
    "    Only use information that you learn by querying the BigQuery table. \n",
    "    Do not make up information. Be sure to look at which tables are available \n",
    "    and get the info of any relevant tables before trying to write a query. \n",
    "    \n",
    "    Question:\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Which store has sold the most bottles of all time?\"\n",
    "response = chat.send_message(init_prompt + prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "768dd419-6879-461f-bc98-61f576b48c58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calling: sql_query with args: {'query': 'SELECT item_description, SUM(bottles_sold) AS total_bottles_sold FROM `kylesteckler-sandbox.iowa_liquor_sales.sales` GROUP BY item_description ORDER BY total_bottles_sold DESC LIMIT 1'}\n",
      "\n",
      "The most popular bottle of all time is FIREBALL CINNAMON WHISKEY, with 17,082,679 bottles sold.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    \"Interesting! What is the most popular bottle of all time?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67d546ab-1a5b-4a48-b534-eb120f068049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calling: sql_query with args: {'query': \"SELECT item_description, SUM(bottles_sold) AS total_bottles_sold FROM `kylesteckler-sandbox.iowa_liquor_sales.sales` WHERE county = 'POLK' GROUP BY item_description ORDER BY total_bottles_sold DESC LIMIT 5\"}\n",
      "\n",
      "The five most popular bottles in Polk County are:\n",
      "\n",
      "1. FIREBALL CINNAMON WHISKEY (4,840,632 bottles sold)\n",
      "2. TITOS HANDMADE VODKA (2,412,290 bottles sold)\n",
      "3. BLACK VELVET (2,190,747 bottles sold)\n",
      "4. HAWKEYE VODKA (1,866,134 bottles sold)\n",
      "5. FIREBALL CINNAMON (1,029,414 bottles sold)\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    \"What are the five most popular bottles in polk county?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45a1c500-9a9b-46e4-8e56-dbd431593d51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calling: sql_query with args: {'query': 'SELECT vendor_name, SUM(sale_dollars) AS total_revenue FROM `kylesteckler-sandbox.iowa_liquor_sales.sales` GROUP BY vendor_name ORDER BY total_revenue DESC LIMIT 5'}\n",
      "\n",
      "The top 5 vendors with the highest total revenue are:\n",
      "\n",
      "1. DIAGEO AMERICAS ($896,548,524.26)\n",
      "2. SAZERAC COMPANY INC ($345,882,242.31)\n",
      "3. JIM BEAM BRANDS ($321,401,583.08)\n",
      "4. PERNOD RICARD USA ($199,918,196.21)\n",
      "5. HEAVEN HILL BRANDS ($184,274,291.31)\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    \"What vendors have made the most revenue selling liquor?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32be2bc-4cee-43f6-8323-41e1a5e41668",
   "metadata": {},
   "source": [
    "Feel free to execute the generated SQL to verify/validate the responses! An easy way to do this is in a code cell with `%%bigquery` at the top. For example:\n",
    "\n",
    "```\n",
    "%%bigquery\n",
    "SELECT ... \n",
    "FROM ... \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e8f47-54f8-460b-8fd3-3ddff28f2f84",
   "metadata": {},
   "source": [
    "## Function calling for entity extraction \n",
    "In the previous examples we used entity extraction to pass parameters along to another function, API, or client library. However, you might want to only perform the entity extraction step, and stop there without actually invoking anything else. You can think of this functionality as a convenient way to transform unstructured text data into structured fields.\n",
    "\n",
    "For example, we can easily build a log extractor that transforms raw logs into structured data with details about error messages.\n",
    "\n",
    "Start by specifying the function declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d5e8e13-3295-48b8-aa9e-c42a5ffcd01e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract_log_data_func = FunctionDeclaration(\n",
    "    name=\"extract_log_data\",\n",
    "    description=\"Extracts specific details from errors in log data\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"errors\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"Errors\",\n",
    "                \"items\": {\n",
    "                    \"description\": \"Details of the error\",\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"error_message\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Full error message\",\n",
    "                        },\n",
    "                        \"error_code\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Error code\",\n",
    "                        },\n",
    "                        \"error_type\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Error type\",\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "error_extraction_tool = Tool(\n",
    "    function_declarations=[extract_log_data_func],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb296121-24ba-4cb0-a98d-3d5e808c35f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\n",
    "    \"gemini-1.0-pro-001\",\n",
    "    tools=[error_extraction_tool],\n",
    "    generation_config=GenerationConfig(temperature=0.0),\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "[15:43:28] ERROR: Could not process image upload: Unsupported file format. (Error Code: 308)\n",
    "[15:44:10] INFO: Search index updated successfully. \n",
    "[15:45:02] ERROR: Service dependency unavailable (payment gateway). Retrying... (Error Code: 5522) \n",
    "[15:45:33] ERROR: Application crashed due to out-of-memory exception. (Error Code: 9001) \n",
    "\"\"\"\n",
    "response = model.generate_content(prompt)\n",
    "function_call = response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "035deb00-eb2c-4045-bf30-444da13c4e15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error_type': 'ERROR', 'error_message': 'Could not process image upload: Unsupported file format.', 'error_code': '308'}\n",
      "{'error_type': 'ERROR', 'error_message': 'Service dependency unavailable (payment gateway). Retrying...', 'error_code': '5522'}\n",
      "{'error_message': 'Application crashed due to out-of-memory exception.', 'error_type': 'ERROR', 'error_code': '9001'}\n"
     ]
    }
   ],
   "source": [
    "for err in dict(function_call.args).get(\"errors\"):\n",
    "    print(dict(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588e750-d149-4760-88a4-01c96b6614a6",
   "metadata": {},
   "source": [
    "Function calling is an incredibly versatile tool! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ddcf33-329d-49e1-895f-f28526806ab2",
   "metadata": {},
   "source": [
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450aa40-5443-457f-99e6-cd1f71b34d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m122"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

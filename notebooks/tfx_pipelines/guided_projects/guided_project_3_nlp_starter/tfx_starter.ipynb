{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification TFX Pipeline Starter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** In this notebook, we show you how to put a text classification model implemented in `model.py`, `preprocessing.py`, and `config.py` into an interactive TFX pipeline. Using these files and the code snippets in this notebook, you'll configure a TFX pipeline generated by the `tfx template` tool as in the previous guided project so that the text classification can be run on a `CAIP Pipelines` Kubeflow cluster. The dataset itself consists of article titles along with their source, and the goal is to predict the source from the title. (This dataset can be re-generated by running either the `keras_for_text_classification.ipynb` notebook or the `reusable_embeddings.ipynb` notebook, which contain different models to solve this problem.) The solution we propose here is fairly simple and you can build on it by inspecting these notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import absl\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform as tft\n",
    "import tfx\n",
    "from tensorflow_metadata.proto.v0 import (\n",
    "    anomalies_pb2,\n",
    "    schema_pb2,\n",
    "    statistics_pb2,\n",
    ")\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tfx.components import (\n",
    "    CsvExampleGen,\n",
    "    Evaluator,\n",
    "    ExampleValidator,\n",
    "    InfraValidator,\n",
    "    Pusher,\n",
    "    ResolverNode,\n",
    "    SchemaGen,\n",
    "    StatisticsGen,\n",
    "    Trainer,\n",
    "    Transform,\n",
    "    Tuner,\n",
    ")\n",
    "from tfx.components.common_nodes.importer_node import ImporterNode\n",
    "from tfx.components.trainer import executor as trainer_executor\n",
    "from tfx.dsl.components.base import executor_spec\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import (\n",
    "    InteractiveContext,\n",
    ")\n",
    "from tfx.proto import (\n",
    "    evaluator_pb2,\n",
    "    example_gen_pb2,\n",
    "    infra_validator_pb2,\n",
    "    pusher_pb2,\n",
    "    trainer_pb2,\n",
    ")\n",
    "from tfx.proto.evaluator_pb2 import SingleSlicingSpec\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import (\n",
    "    HyperParameters,\n",
    "    InfraBlessing,\n",
    "    Model,\n",
    "    ModelBlessing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: this lab was developed and tested with the following TF ecosystem package versions:\n",
    "\n",
    "`Tensorflow Version: 2.3.1`  \n",
    "`TFX Version: 0.25.0`  \n",
    "`TFDV Version: 0.25.0`  \n",
    "`TFMA Version: 0.25.0`\n",
    "\n",
    "If you encounter errors with the above imports (e.g. TFX component not found), check your package versions in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "print(\"TFX Version:\", tfx.__version__)\n",
    "print(\"TFDV Version:\", tfdv.__version__)\n",
    "print(\"TFMA Version:\", tfma.VERSION_STRING)\n",
    "\n",
    "absl.logging.set_verbosity(absl.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the versions above do not match, update your packages in the current Jupyter kernel below. The default `%pip` package installation location is not on your system installation PATH; use the command below to append the local installation path to pick up the latest package versions. Note that you may also need to restart your notebook kernel to pick up the specified package versions and re-run the imports cell above before proceeding with the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + \"/home/jupyter/.local/bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure lab settings\n",
    "\n",
    "Set constants, location paths and other environment settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_STORE = os.path.join(os.sep, \"home\", \"jupyter\", \"artifact-store\")\n",
    "SERVING_MODEL_DIR = os.path.join(os.sep, \"home\", \"jupyter\", \"serving_model\")\n",
    "DATA_ROOT = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "!mkdir -p $DATA_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/titles_sample.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING = {\"github\": 0, \"nytimes\": 1, \"techcrunch\": 2}\n",
    "data[\"source\"] = data[\"source\"].apply(lambda label: LABEL_MAPPING[label])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f\"{DATA_ROOT}/dataset.csv\", index=None)\n",
    "!head $DATA_ROOT/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Context\n",
    "\n",
    "TFX Interactive Context allows you to create and run TFX Components in an interactive mode. It is designed to support experimentation and development in a Jupyter Notebook environment. It is an experimental feature and major changes to interface and functionality are expected. When creating the interactive context you can specifiy the following parameters:\n",
    "- `pipeline_name` - Optional name of the pipeline for ML Metadata tracking purposes. If not specified, a name will be generated for you.\n",
    "- `pipeline_root` - Optional path to the root of the pipeline's outputs. If not specified, an ephemeral temporary directory will be created and used.\n",
    "- `metadata_connection_config` - Optional `metadata_store_pb2.ConnectionConfig` instance used to configure connection to a ML Metadata connection. If not specified, an ephemeral SQLite MLMD connection contained in the pipeline_root directory with file name \"metadata.sqlite\" will be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"tfx-title-classifier\"\n",
    "PIPELINE_ROOT = os.path.join(\n",
    "    ARTIFACT_STORE, PIPELINE_NAME, time.strftime(\"%Y%m%d_%H%M%S\")\n",
    ")\n",
    "os.makedirs(PIPELINE_ROOT, exist_ok=True)\n",
    "\n",
    "context = InteractiveContext(\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    metadata_connection_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting data using ExampleGen\n",
    "\n",
    "In any ML development process the first step  is to ingest the training and test datasets. The `ExampleGen` component ingests data into a TFX pipeline. It consumes external files/services to generate a set file files in the `TFRecord` format,  which will be used by other TFX components. It can also shuffle the data and split into an arbitrary number of partitions.\n",
    "\n",
    "<img src=../../images/ExampleGen.png width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run CsvExampleGen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(\n",
    "        splits=[\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=4),\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=1),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "example_gen = tfx.components.CsvExampleGen(\n",
    "    input_base=DATA_ROOT, output_config=output_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the ingested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_uri = example_gen.outputs[\"examples\"].get()[0].uri\n",
    "\n",
    "tfrecord_filenames = [\n",
    "    os.path.join(examples_uri, \"train\", name)\n",
    "    for name in os.listdir(os.path.join(examples_uri, \"train\"))\n",
    "]\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "for tfrecord in dataset.take(2):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(tfrecord.numpy())\n",
    "\n",
    "    for name, feature in example.features.feature.items():\n",
    "        if feature.HasField(\"bytes_list\"):\n",
    "            value = feature.bytes_list.value\n",
    "        if feature.HasField(\"float_list\"):\n",
    "            value = feature.float_list.value\n",
    "        if feature.HasField(\"int64_list\"):\n",
    "            value = feature.int64_list.value\n",
    "        print(f\"{name}: {value}\")\n",
    "    print(\"******\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating statistics using StatisticsGen\n",
    "\n",
    "The `StatisticsGen`  component generates data statistics that can be used by other TFX components. StatisticsGen uses [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started). `StatisticsGen` generate statistics for each split in the `ExampleGen` component's output. In our case there two splits: `train` and `eval`.\n",
    "\n",
    "<img src=../../images/StatisticsGen.png width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and  run the `StatisticsGen` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_gen = tfx.components.StatisticsGen(\n",
    "    examples=example_gen.outputs[\"examples\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize statistics\n",
    "\n",
    "The generated statistics can be visualized using the `tfdv.visualize_statistics()` function from the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library or using a utility method of the `InteractiveContext` object. In fact, most of the artifacts generated by the TFX components can be visualized using `InteractiveContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(statistics_gen.outputs[\"statistics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infering data schema using SchemaGen\n",
    "\n",
    "Some TFX components use a description input data called a schema. The schema is an instance of `schema.proto`. It can specify data types for feature values, whether a feature has to be present in all examples, allowed value ranges, and other properties. `SchemaGen` automatically generates the schema by inferring types, categories, and ranges from data statistics. The auto-generated schema is best-effort and only tries to infer basic properties of the data. It is expected that developers review and modify it as needed. `SchemaGen` uses [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started).\n",
    "\n",
    "The `SchemaGen` component generates the schema using the statistics for the `train` split. The statistics for other splits are ignored.\n",
    "\n",
    "<img src=../../images/SchemaGen.png width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `SchemaGen` components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs[\"statistics\"], infer_feature_shape=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the inferred schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(schema_gen.outputs[\"schema\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the auto-generated schema\n",
    "\n",
    "In most cases the auto-generated schemas must be fine-tuned manually using insights from data exploration and/or domain knowledge about the data. For example, you know that in the `covertype` dataset there are seven types of forest cover (coded using 1-7 range) and that the value of the `Slope` feature should be in the 0-90 range. You can manually add these constraints to the auto-generated schema by setting the feature domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the auto-generated schema proto file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_proto_path = \"{}/{}\".format(\n",
    "    schema_gen.outputs[\"schema\"].get()[0].uri, \"schema.pbtxt\"\n",
    ")\n",
    "schema = tfdv.load_schema_text(schema_proto_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the schema\n",
    "\n",
    "You can use the protocol buffer APIs to modify the schema using `tfdv.set_somain`.\n",
    "Review the [TFDV library API documentation](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/set_domain) on setting a feature's domain. You can use the protocol buffer APIs to modify the schema. Review the [Tensorflow Metadata proto definition](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto#L405) for configuration options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the updated schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dir = os.path.join(ARTIFACT_STORE, \"schema\")\n",
    "tf.io.gfile.makedirs(schema_dir)\n",
    "schema_file = os.path.join(schema_dir, \"schema.pbtxt\")\n",
    "\n",
    "tfdv.write_schema_text(schema, schema_file)\n",
    "\n",
    "!cat {schema_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the updated schema using ImporterNode\n",
    "\n",
    "The `ImporterNode` component allows you to import an external artifact, including the schema file, so it can be used by other TFX components in your workflow. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `ImporterNode` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_importer = ImporterNode(\n",
    "    instance_name=\"Schema_Importer\",\n",
    "    source_uri=schema_dir,\n",
    "    artifact_type=tfx.types.standard_artifacts.Schema,\n",
    "    reimport=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(schema_importer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the imported schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(schema_importer.outputs[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating data with ExampleValidator\n",
    "\n",
    "The `ExampleValidator` component identifies anomalies in data.  It identifies anomalies by comparing data statistics computed by the `StatisticsGen` component against a schema generated by `SchemaGen` or imported by `ImporterNode`.\n",
    "\n",
    "`ExampleValidator` can detect different classes of anomalies. For example it can:\n",
    "\n",
    "- perform validity checks by comparing data statistics against a schema \n",
    "- detect training-serving skew by comparing training and serving data.\n",
    "- detect data drift by looking at a series of data.\n",
    "\n",
    "\n",
    "The `ExampleValidator` component validates the data in the `eval` split only. Other splits are ignored. \n",
    "\n",
    "<img src=../../images/ExampleValidator.png width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `ExampleValidator` component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_validator = ExampleValidator(\n",
    "    instance_name=\"Data_Validation\",\n",
    "    statistics=statistics_gen.outputs[\"statistics\"],\n",
    "    schema=schema_importer.outputs[\"result\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the output of `ExampleValidator`\n",
    "\n",
    "The output artifact of the ExampleValidator is the `anomalies.pbtxt` file describing an anomalies_pb2.Anomalies protobuf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = example_validator.outputs[\"anomalies\"].get()[0].uri\n",
    "train_anomalies_filename = os.path.join(train_uri, \"train/anomalies.pbtxt\")\n",
    "!cat $train_anomalies_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize validation results\n",
    "\n",
    "The file `anomalies.pbtxt` can be visualized using `context.show`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(example_validator.outputs[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case no anomalies were detected in the `eval` split.\n",
    "\n",
    "For a detailed deep dive into data validation and schema generation refer to the `lab-31-tfdv-structured-data` lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data with Transform\n",
    "\n",
    "The `Transform` component performs data transformation and feature engineering. The `Transform` component consumes `tf.Examples` emitted from the `ExampleGen` component and emits the transformed feature data and the `SavedModel` graph that was used to process the data. The emitted `SavedModel`  can then be used by serving components to make sure that the same data pre-processing logic is applied at training and serving.\n",
    "\n",
    "The `Transform` component requires more code than many other components because of the arbitrary complexity of the feature engineering that you may need for the data and/or model that you're working with. It requires code files to be available which define the processing needed.\n",
    "\n",
    "<img src=../../images/Transform.png width=\"400\">\n",
    "\n",
    "### Define the pre-processing module\n",
    "\n",
    "To configure `Transform`, you need to encapsulate your pre-processing code in the Python `preprocessing_fn` function and save it to a  python module that is then provided to the Transform component as an input. This module will be loaded by transform and the `preprocessing_fn` function will be called when the `Transform` component runs.\n",
    "\n",
    "In most cases, your implementation of the `preprocessing_fn` makes extensive use of [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/tft) for performing feature engineering on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.py\n",
    "FEATURE_KEY = 'title'\n",
    "LABEL_KEY = 'source'\n",
    "N_CLASSES = 3\n",
    "HUB_URL = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "HUB_DIM = 50\n",
    "N_NEURONS = 16\n",
    "TRAIN_BATCH_SIZE = 5\n",
    "EVAL_BATCH_SIZE = 5\n",
    "MODEL_NAME = 'tfx_title_classifier'\n",
    "\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from config import (\n",
    "    LABEL_KEY,\n",
    "    N_CLASSES,\n",
    "    FEATURE_KEY,\n",
    "    transformed_name\n",
    ")\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "    default_value = '' if x.dtype == tf.string else 0\n",
    "    return tf.squeeze(\n",
    "        tf.sparse.to_dense(\n",
    "            tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "            default_value),\n",
    "        axis=1)\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    features = _fill_in_missing(inputs[FEATURE_KEY])\n",
    "    labels =  _fill_in_missing(inputs[LABEL_KEY])\n",
    "    return {\n",
    "        transformed_name(FEATURE_KEY): features,\n",
    "        transformed_name(LABEL_KEY): labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_MODULE = \"preprocessing.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `Transform` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transform(\n",
    "    examples=example_gen.outputs[\"examples\"],\n",
    "    schema=schema_importer.outputs[\"result\"],\n",
    "    module_file=TRANSFORM_MODULE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the `Transform` component's outputs\n",
    "\n",
    "The Transform component has 2 outputs:\n",
    "\n",
    "- `transform_graph` - contains the graph that can perform the preprocessing operations (this graph will be included in the serving and evaluation models).\n",
    "- `transformed_examples` - contains the preprocessed training and evaluation data.\n",
    "\n",
    "Take a peek at the `transform_graph` artifact: it points to a directory containing 3 subdirectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(transform.outputs[\"transform_graph\"].get()[0].uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `transform.examples` artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(transform.outputs[\"transformed_examples\"].get()[0].uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_uri = transform.outputs[\"transformed_examples\"].get()[0].uri\n",
    "\n",
    "tfrecord_filenames = [\n",
    "    os.path.join(transform_uri, \"train\", name)\n",
    "    for name in os.listdir(os.path.join(transform_uri, \"train\"))\n",
    "]\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "for tfrecord in dataset.take(4):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(tfrecord.numpy())\n",
    "    for name, feature in example.features.feature.items():\n",
    "        if feature.HasField(\"bytes_list\"):\n",
    "            value = feature.bytes_list.value\n",
    "        if feature.HasField(\"float_list\"):\n",
    "            value = feature.float_list.value\n",
    "        if feature.HasField(\"int64_list\"):\n",
    "            value = feature.int64_list.value\n",
    "        print(f\"{name}: {value}\")\n",
    "    print(\"******\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your TensorFlow model with the `Trainer` component\n",
    "\n",
    "The `Trainer` component trains a model using TensorFlow.\n",
    "\n",
    "`Trainer` takes:\n",
    "\n",
    "- tf.Examples used for training and eval.\n",
    "- A user provided module file that defines the trainer logic.\n",
    "- A data schema created by `SchemaGen` or imported by `ImporterNode`.\n",
    "- A proto definition of train args and eval args.\n",
    "- An optional transform graph produced by upstream Transform component.\n",
    "- An optional base models used for scenarios such as warmstarting training.\n",
    "\n",
    "<img src=../../images/Trainer.png width=\"400\">\n",
    "\n",
    "\n",
    "### Define the trainer module\n",
    "\n",
    "To configure `Trainer`, you need to encapsulate your training code in a Python module that is then provided to the `Trainer` as an input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow_hub import KerasLayer\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tfx_bsl.tfxio import dataset_options\n",
    "\n",
    "\n",
    "from config import (\n",
    "    HUB_URL,\n",
    "    HUB_DIM,\n",
    "    N_NEURONS,\n",
    "    N_CLASSES,\n",
    "    LABEL_KEY,\n",
    "    TRAIN_BATCH_SIZE,\n",
    "    EVAL_BATCH_SIZE,\n",
    "    MODEL_NAME,\n",
    "    transformed_name\n",
    ")\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    \n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        return model(transformed_features)\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern, data_accessor, tf_transform_output, batch_size=200):\n",
    "    return data_accessor.tf_dataset_factory(\n",
    "        file_pattern,\n",
    "        dataset_options.TensorFlowDatasetOptions(\n",
    "            batch_size=batch_size,\n",
    "            label_key=transformed_name(LABEL_KEY)),\n",
    "        tf_transform_output.transformed_metadata.schema\n",
    "    )\n",
    "\n",
    "\n",
    "def _load_hub_module_layer():\n",
    "    hub_module = KerasLayer(\n",
    "        HUB_URL, output_shape=[HUB_DIM], \n",
    "        input_shape=[], dtype=tf.string, trainable=True)\n",
    "    return hub_module\n",
    "    \n",
    "\n",
    "def _build_keras_model():\n",
    "    hub_module = _load_hub_module_layer()\n",
    "    model = Sequential([\n",
    "        hub_module,\n",
    "        Dense(N_NEURONS, activation='relu'),\n",
    "        Dense(N_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def run_fn(fn_args):\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,\n",
    "                            tf_transform_output, TRAIN_BATCH_SIZE)\n",
    "    \n",
    "    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,\n",
    "                           tf_transform_output, EVAL_BATCH_SIZE)\n",
    "\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    with mirrored_strategy.scope():\n",
    "        model = _build_keras_model()\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=fn_args.model_run_dir, update_freq='batch')\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=[tensorboard_callback])\n",
    "\n",
    "    signatures = {\n",
    "        'serving_default':\n",
    "            _get_serve_tf_examples_fn(model,\n",
    "                                   tf_transform_output).get_concrete_function(\n",
    "                                        tf.TensorSpec(\n",
    "                                            shape=[None],\n",
    "                                            dtype=tf.string,\n",
    "                                            name='examples')),\n",
    "    }\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_MODULE_FILE = \"model.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run the Trainer component\n",
    "\n",
    "As of the 0.25.0 release of TFX, the `Trainer` component only supports passing a single field - `num_steps` - through the `train_args` and `eval_args` arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    custom_executor_spec=executor_spec.ExecutorClassSpec(\n",
    "        trainer_executor.GenericExecutor\n",
    "    ),\n",
    "    module_file=TRAINER_MODULE_FILE,\n",
    "    transformed_examples=transform.outputs.transformed_examples,\n",
    "    schema=schema_importer.outputs.result,\n",
    "    transform_graph=transform.outputs.transform_graph,\n",
    "    train_args=trainer_pb2.TrainArgs(splits=[\"train\"], num_steps=20),\n",
    "    eval_args=trainer_pb2.EvalArgs(splits=[\"eval\"], num_steps=5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing training runs with TensorBoard\n",
    "\n",
    "In this step you will analyze the training run with [TensorBoard.dev](https://blog.tensorflow.org/2019/12/introducing-tensorboarddev-new-way-to.html). `TensorBoard.dev` is a managed service that enables you to easily host, track and share your ML experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the location of TensorBoard logs\n",
    "\n",
    "Each model run's train and eval metric logs are written to the `model_run` directory by the Tensorboard callback defined in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = trainer.outputs[\"model_run\"].get()[0].uri\n",
    "print(logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the logs and start TensorBoard.dev\n",
    "\n",
    "1. Open a new JupyterLab terminal window\n",
    "\n",
    "2. From the terminal window, execute the following command\n",
    "```\n",
    "tensorboard dev upload --logdir [YOUR_LOGDIR]\n",
    "```\n",
    "\n",
    "Where [YOUR_LOGDIR] is an URI retrieved by the previous cell.\n",
    "\n",
    "You will be asked to authorize `TensorBoard.dev` using your Google account. If you don't have a Google account or you don't want to authorize `TensorBoard.dev` you can skip this exercise.\n",
    "\n",
    "After the authorization process completes, follow the link provided to view your experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating trained models with Evaluator\n",
    "The `Evaluator` component analyzes model performance using the [TensorFlow Model Analysis library](https://www.tensorflow.org/tfx/model_analysis/get_started). It runs inference requests on particular subsets of the test dataset, based on which slices are defined by the developer. Knowing which slices should be analyzed requires domain knowledge of what is important in this particular use case or domain. \n",
    "\n",
    "The `Evaluator` can also optionally validate a newly trained model against a previous model. In this lab, you only train one model, so the Evaluator automatically will label the model as \"blessed\".\n",
    "\n",
    "\n",
    "<img src=../../images/Evaluator.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the Evaluator component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `ResolverNode` to pick the previous model to compare against.  The model resolver is only required if performing model validation in addition to evaluation. In this case we validate against the latest blessed model. If no model has been blessed before (as in this case) the evaluator will make our candidate the first blessed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resolver = ResolverNode(\n",
    "    instance_name=\"latest_blessed_model_resolver\",\n",
    "    resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
    "    model=Channel(type=Model),\n",
    "    model_blessing=Channel(type=ModelBlessing),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(model_resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure evaluation metrics and slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_threshold = tfma.MetricThreshold(\n",
    "    value_threshold=tfma.GenericValueThreshold(\n",
    "        lower_bound={\"value\": 0.30}, upper_bound={\"value\": 0.99}\n",
    "    )\n",
    ")\n",
    "\n",
    "metrics_specs = tfma.MetricsSpec(\n",
    "    metrics=[\n",
    "        tfma.MetricConfig(\n",
    "            class_name=\"SparseCategoricalAccuracy\", threshold=accuracy_threshold\n",
    "        ),\n",
    "        tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_config = tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key=\"source\")],\n",
    "    metrics_specs=[metrics_specs],\n",
    ")\n",
    "eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analyzer = Evaluator(\n",
    "    examples=example_gen.outputs.examples,\n",
    "    model=trainer.outputs.model,\n",
    "    baseline_model=model_resolver.outputs.model,\n",
    "    eval_config=eval_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(model_analyzer, enable_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the model performance validation status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_blessing_uri = model_analyzer.outputs.blessing.get()[0].uri\n",
    "!ls -l {model_blessing_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying models with Pusher\n",
    "\n",
    "The `Pusher` component checks whether a model has been \"blessed\", and if so, deploys it by pushing the model to a well known file destination.\n",
    "\n",
    "<img src=../../images/Pusher.png width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `Pusher` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.outputs[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher = Pusher(\n",
    "    model=trainer.outputs[\"model\"],\n",
    "    model_blessing=model_analyzer.outputs[\"blessing\"],\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory=SERVING_MODEL_DIR\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the output of `Pusher`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `PATH` to include a directory containing `saved_model_cli.\n",
    "PATH = get_ipython().run_line_magic(\"env\", \"PATH\")\n",
    "%env PATH=/opt/conda/envs/tfx/bin:{PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_pushed_model = os.path.join(\n",
    "    SERVING_MODEL_DIR, max(os.listdir(SERVING_MODEL_DIR))\n",
    ")\n",
    "!saved_model_cli show --dir {latest_pushed_model} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "bf92b9a1-f786-4a8e-b89c-97918c6df64a",
   "metadata": {
    "tags": []
   },
   "source": [
    "import logging\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam import Create, FlatMap, Map, ParDo, Filter, Flatten, Partition, MapTuple, FlatMapTuple\n",
    "from apache_beam import Keys, Values\n",
    "from apache_beam.transforms.util import WithKeys\n",
    "\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.gemini_inference import GeminiModelHandler, generate_from_string\n",
    "from collections.abc import Callable\n",
    "from collections.abc import Iterable\n",
    "from collections.abc import Sequence\n",
    "from typing import Any\n",
    "from typing import Optional\n",
    "\n",
    "from google import genai\n",
    "from google.genai import errors\n",
    "\n",
    "from apache_beam.ml.inference import utils\n",
    "from apache_beam.ml.inference.base import PredictionResult\n",
    "from apache_beam.ml.inference.base import RemoteModelHandler\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import DebugOptions\n",
    "from apache_beam.runners import DataflowRunner"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e429a6b5-f804-4207-97ac-ddad7d8ca541",
   "metadata": {
    "tags": []
   },
   "source": [
    "PROJECT_ID = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "%env GOOGLE_CLOUD_PROJECT={PROJECT_ID}\n",
    "BUCKET_NAME=f'dataflow_demo_{PROJECT_ID}'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1cc6de5a-dd01-452d-8619-aa61a31f572c",
   "metadata": {},
   "source": [
    "### Creating a GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f5ac666-9f6a-480e-aa62-33feb641a9b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "!gsutil mb -l {REGION} gs://{BUCKET_NAME}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a4b5d865-ba9b-4910-b0e6-cb425e4764d3",
   "metadata": {},
   "source": [
    "#### Creating BigQuery Table \n",
    "As an alternative option you can create table "
   ]
  },
  {
   "cell_type": "code",
   "id": "7fe55f2f-6028-49b3-9a0e-fb269b79412e",
   "metadata": {},
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE `genai.genai_pipeline`\n",
    "(\n",
    "example STRING,\n",
    "inference STRING\n",
    ")\n",
    "OPTIONS(\n",
    "    description=\"Table to store results\"\n",
    ");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20ca85b0-f523-42ea-b8f9-c07daa66f1e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "import json\n",
    "\n",
    "import logging\n",
    "\n",
    "#Custom utility method to specifiy gemini client invocation\n",
    "def response_to_text(\n",
    "    model_name: str, batch: Sequence[str], \n",
    "    model: genai.Client, inference_args: dict[str, Any]):\n",
    "    response_object = model.models.generate_content(\n",
    "            model=model_name, contents=batch, **inference_args\n",
    "        )\n",
    "    try:\n",
    "        if hasattr(response_object, 'text') and response_object.text:\n",
    "            return [response_object.text]\n",
    "        else:\n",
    "            [\"\"]\n",
    "            \n",
    "    except ValueError:\n",
    "        # TODO: Imprement proper error handling\n",
    "        return [\"ERROR\"]\n",
    "\n",
    "#Implementation of the ModelHandler interface for Google Gemini\n",
    "model_handler = GeminiModelHandler(\n",
    "        model_name=MODEL_NAME,\n",
    "        request_fn=response_to_text,\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "\n",
    "#Output BigQuery table\n",
    "output_table = f'{PROJECT_ID}.genai.genai_pipeline'\n",
    "\n",
    "#Output BigQuery table schema\n",
    "output_table_schema = 'input_text:STRING, output_text:STRING'\n",
    "\n",
    "#Input elements to process\n",
    "elements = [\n",
    "    \"What is the capital of Ireland?\", \n",
    "    \"What is the capital of France?\",\n",
    "]\n",
    "\n",
    "#Custom utility method to prepare records to sore to BigQuery\n",
    "class PrepareRecords(beam.DoFn):\n",
    "    \"\"\"\n",
    "    Extract the relevant data from the PredictionResult object.\n",
    "    \"\"\"\n",
    "    def process(self, element: PredictionResult) -> Iterable[dict[str, str]]:\n",
    "        yield {'input_text': element.example, 'output_text': element.inference}\n",
    "\n",
    "# The pipeline:\n",
    "lines = (p  | \"Create elements\" >> Create(elements)\n",
    "            | \"RunInference\" >> RunInference(model_handler)\n",
    "            | \"Prepare Record\" >> beam.ParDo(PrepareRecords())\n",
    "            | \"Write BigQuery\" >> beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "                table=output_table,\n",
    "                schema=output_table_schema,\n",
    "                method=beam.io.gcp.bigquery.WriteToBigQuery.Method.STREAMING_INSERTS,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
    "        )\n",
    "\n",
    "# Uncomment if you want to use Interactive Runner:\n",
    "# ib.show(lines)\n",
    "\n",
    "#Uncomment if you want to use Dataflow Runner:\n",
    "options = pipeline_options.PipelineOptions(\n",
    "    flags={},\n",
    "    project=PROJECT_ID,\n",
    "    region='us-central1',\n",
    "    staging_location=f'gs://{BUCKET_NAME}/staging',\n",
    "    temp_location=f'gs://{BUCKET_NAME}/temp',\n",
    "    #sdk_container_image=f'gcr.io/{PROJECT_ID}/cc_gpu:latest',\n",
    "    machine_type='n1-standard-4',\n",
    "    disk_size_gb=50)\n",
    "\n",
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)\n",
    "pipeline_result.wait_until_finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "280a8c2d-768e-4b5a-804b-13ce17470842",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4723fc9e-b806-44ae-8d88-8aba6e0ce226",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.69.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.69.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

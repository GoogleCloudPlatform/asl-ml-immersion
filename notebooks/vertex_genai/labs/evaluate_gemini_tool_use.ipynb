{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKiAD3qCCtOu"
   },
   "source": [
    " # Evaluate Generative Model Tool Use | Gen AI Evaluation SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmNXwtYGCtOv"
   },
   "source": [
    "## Overview\n",
    "\n",
    "* Define an API function and a Tool for Gemini model, and evaluate the Gemini model tool use quality with *Vertex AI Python SDK for Gen AI Evaluation Service*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58zvbIBKCtOv"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4FOrb_sCtOx"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2LhRUspCtOx"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = PROJECT_ID[0]  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EvqYQT2CtOx"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHDPrnbQlTFX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Main\n",
    "from vertexai.evaluation import EvalTask\n",
    "from vertexai.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEv1ZxXPlTFX"
   },
   "source": [
    "### Library settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilX3gNbllTFX"
   },
   "outputs": [],
   "source": [
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4nrCUUnlTFX"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HohglC48lTFX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_uuid(length: int = 8) -> str:\n",
    "    \"\"\"Generate a uuid of a specified length (default=8).\"\"\"\n",
    "    return \"\".join(\n",
    "        random.choices(string.ascii_lowercase + string.digits, k=length)\n",
    "    )\n",
    "\n",
    "\n",
    "def display_eval_report(eval_result, metrics=None):\n",
    "    \"\"\"Display the evaluation results.\"\"\"\n",
    "\n",
    "    title, summary_metrics, report_df = eval_result\n",
    "    metrics_df = pd.DataFrame.from_dict(summary_metrics, orient=\"index\").T\n",
    "    if metrics:\n",
    "        metrics_df = metrics_df.filter(\n",
    "            [\n",
    "                metric\n",
    "                for metric in metrics_df.columns\n",
    "                if any(selected_metric in metric for selected_metric in metrics)\n",
    "            ]\n",
    "        )\n",
    "        report_df = report_df.filter(\n",
    "            [\n",
    "                metric\n",
    "                for metric in report_df.columns\n",
    "                if any(selected_metric in metric for selected_metric in metrics)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Display the title with Markdown for emphasis\n",
    "    display(Markdown(f\"## {title}\"))\n",
    "\n",
    "    # Display the metrics DataFrame\n",
    "    display(Markdown(\"### Summary Metrics\"))\n",
    "    display(metrics_df)\n",
    "\n",
    "    # Display the detailed report DataFrame\n",
    "    display(Markdown(\"### Report Metrics\"))\n",
    "    display(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPBjzk96w9gV"
   },
   "source": [
    "## Evaluate Tool use and Function Calling quality for Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY5hG6VMnS_s"
   },
   "source": [
    "#### Tool evaluation metrics\n",
    "\n",
    "* `tool_call_valid`\n",
    "* `tool_name_match`\n",
    "* `tool_parameter_key_match`\n",
    "* `tool_parameter_kv_match`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcZ2UikInZe0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tool_metrics = [\n",
    "    \"tool_call_valid\",\n",
    "    \"tool_name_match\",\n",
    "    \"tool_parameter_key_match\",\n",
    "    \"tool_parameter_kv_match\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJgpakfIw9gc"
   },
   "source": [
    "### 1. Evaluate a Bring-Your-Own-Prediction dataset\n",
    "\n",
    "Generative model's tool use quality can be evaluated if the eval dataset contains saved model tool call responses, and expected references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Euv-SQw9gc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = [\n",
    "    '{\"content\": \"\", \"tool_calls\": [{\"name\": \"book_tickets\", \"arguments\": {\"movie\": \"Mission Impossible Dead Reckoning Part 1\", \"theater\": \"Regal Edwards 14\", \"location\": \"Mountain View CA\", \"showtime\": \"7:30\", \"date\": \"2024-03-30\", \"num_tix\": \"2\"}}]}',\n",
    "    '{\"content\": \"\", \"tool_calls\": [{\"name\": \"book_tickets\", \"arguments\": {\"movie\": \"Mission Impossible Dead Reckoning Part 1\", \"theater\": \"Regal Edwards 14\", \"location\": \"Mountain View CA\", \"showtime\": \"7:30\", \"date\": \"2024-03-30\", \"num_tix\": \"2\"}}]}',\n",
    "    '{\"content\": \"\", \"tool_calls\": [{\"name\": \"book_tickets\", \"arguments\": {\"movie\": \"Mission Impossible Dead Reckoning Part 1\", \"theater\": \"Regal Edwards 14\"}}]}',\n",
    "    '{\"content\": \"\", \"tool_calls\": [{\"name\": \"book_tickets\", \"arguments\": {\"movie\": \"Mission Impossible Dead Reckoning Part 1\", \"theater\": \"Cinemark\", \"location\": \"Mountain View CA\", \"showtime\": \"5:30\", \"date\": \"2024-03-30\", \"num_tix\": \"2\"}}]}',\n",
    "]\n",
    "\n",
    "reference = [\n",
    "    '{\"content\": \"\", \"tool_calls\": [{\"name\": \"book_tickets\", \"arguments\": {\"movie\": \"Mission Impossible Dead Reckoning Part 1\", \"theater\": \"Regal Edwards 14\", \"location\": \"Mountain View CA\", \"showtime\": \"7:30\", \"date\": \"2024-03-30\", \"num_tix\": \"2\"}}]}',\n",
    "    '{\"content\": \"\", \"tool_calls\": [{\"name\": \"book_tickets\", \"arguments\": {\"movie\": \"Godzilla\", \"theater\": \"Regal Edwards 14\", \"location\": \"Mountain View CA\", \"showtime\": \"9:30\", \"date\": \"2024-03-30\", \"num_tix\": \"2\"}}]}',\n",
    "    '{\"content\": \"\", \"tool_calls\": [{\"name\": \"book_tickets\", \"arguments\": {\"movie\": \"Mission Impossible Dead Reckoning Part 1\", \"theater\": \"Regal Edwards 14\", \"location\": \"Mountain View CA\", \"showtime\": \"7:30\", \"date\": \"2024-03-30\", \"num_tix\": \"2\"}}]}',\n",
    "    '{\"content\": \"\", \"tool_calls\": [{\"name\": \"book_tickets\", \"arguments\": {\"movie\": \"Mission Impossible Dead Reckoning Part 1\", \"theater\": \"Regal Edwards 14\", \"location\": \"Mountain View CA\", \"showtime\": \"7:30\", \"date\": \"2024-03-30\", \"num_tix\": \"2\"}}]}',\n",
    "]\n",
    "\n",
    "eval_dataset = pd.DataFrame(\n",
    "    {\n",
    "        \"response\": response,\n",
    "        \"reference\": reference,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyUj7kOnnNtH"
   },
   "source": [
    "#### Define EvalTask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Define the EvalTask for this job. The EvalTask needs the eval_dataset, tool_metrics and experiment name. Use this [documentation for reference.](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.preview.evaluation.EvalTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYl6A1zInObM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"eval-saved-llm-tool-use\"  # @param {type:\"string\"}\n",
    "\n",
    "tool_use_eval_task = None  # TODO: Define EvalTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8U4KVzAhsP2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_id = generate_uuid()\n",
    "\n",
    "experiment_run_name = f\"eval-{run_id}\"\n",
    "\n",
    "eval_result = tool_use_eval_task.evaluate(\n",
    "    experiment_run_name=experiment_run_name\n",
    ")\n",
    "display_eval_report(\n",
    "    (\n",
    "        \"Tool Use Quality Evaluation Metrics\",\n",
    "        eval_result.summary_metrics,\n",
    "        eval_result.metrics_table,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WXnsT2qo-OD"
   },
   "outputs": [],
   "source": [
    "tool_use_eval_task.display_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxow7GrIRgxo"
   },
   "source": [
    "## 2. Tool Use and Function Calling with Gemini\n",
    "\n",
    "[Function Calling Documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ew82QX0QWxYa",
    "tags": []
   },
   "source": [
    "### Define a function and tool\n",
    "\n",
    "**Exercise:** Define a Function call for booking movie tickets. The parameters are \"movie\", \"location\", \"showtime\", \"date\" and \"num_tix\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiuafKieQgjF"
   },
   "outputs": [],
   "source": [
    "from vertexai.generative_models import FunctionDeclaration, Tool\n",
    "\n",
    "book_tickets_func = (\n",
    "    FunctionDeclaration()\n",
    ")  # TODO- Define the FunctionDeclaration for booking movie tickets\n",
    "\n",
    "\n",
    "book_tickets_tool = Tool(\n",
    "    function_declarations=[book_tickets_func],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQgreOFGXJNF"
   },
   "source": [
    "### Generate a function call\n",
    "\n",
    "Prompt the Gemini model and include the tool that you defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNevmS2ngB0Z"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"I'd like to book 2 tickets for the movie \"Mission Impossible Dead Reckoning Part 1\"\n",
    "at the Regal Edwards 14 theater in Mountain View, CA. The showtime is 7:30 PM on March 30th, 2024.\n",
    "\"\"\"\n",
    "\n",
    "gemini_model = GenerativeModel(\"gemini-2.0-pro\")\n",
    "\n",
    "gemini_response = gemini_model.generate_content(\n",
    "    prompt,\n",
    "    tools=[book_tickets_tool],\n",
    ")\n",
    "\n",
    "gemini_response.candidates[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOAbbxFFXkdn"
   },
   "source": [
    "###  Unpack the Gemini response into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x84XHpuWXmu6"
   },
   "outputs": [],
   "source": [
    "def unpack_response(response):\n",
    "    output = {}\n",
    "    function_call = {}\n",
    "    for key, value in response.candidates[0].content.parts[0].to_dict().items():\n",
    "        function_call[key] = value\n",
    "    output[\"content\"] = \"\"\n",
    "    output[\"tool_calls\"] = [function_call[\"function_call\"]]\n",
    "    output[\"tool_calls\"][0][\"arguments\"] = output[\"tool_calls\"][0].pop(\"args\")\n",
    "    return json.dumps(output)\n",
    "\n",
    "\n",
    "response = unpack_response(gemini_response)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsLT0B75aJYz"
   },
   "source": [
    "### Evaluate the Gemini's Function Call Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgzK90mybqbK"
   },
   "outputs": [],
   "source": [
    "reference_str = json.dumps(\n",
    "    {\n",
    "        \"content\": \"\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"book_tickets\",\n",
    "                \"arguments\": {\n",
    "                    \"movie\": \"Mission Impossible Dead Reckoning Part 1\",\n",
    "                    \"theater\": \"Regal Edwards 14\",\n",
    "                    \"location\": \"Mountain View CA\",\n",
    "                    \"showtime\": \"7:30\",\n",
    "                    \"date\": \"2024-03-30\",\n",
    "                    \"num_tix\": \"2\",\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "eval_dataset = pd.DataFrame(\n",
    "    {\"response\": [response], \"reference\": [reference_str]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttRycKaCg6GG"
   },
   "outputs": [],
   "source": [
    "# Expected Tool Call Response\n",
    "json.loads(eval_dataset.reference[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5SmLtFAdVCh"
   },
   "outputs": [],
   "source": [
    "# Actual Gemini Tool Call Response\n",
    "json.loads(eval_dataset.response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Similar to above define the EvalTask for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUqjfQH5aJE2"
   },
   "outputs": [],
   "source": [
    "experiment_name = \"eval-gemini-model-function-call\"  # @param {type:\"string\"}\n",
    "\n",
    "gemini_functiona_call_eval_task = EvalTask()  # TODO - Define the EvalTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3MfX9QCZxHW"
   },
   "outputs": [],
   "source": [
    "run_id = generate_uuid()\n",
    "\n",
    "eval_result = gemini_functiona_call_eval_task.evaluate(\n",
    "    experiment_run_name=f\"eval-{run_id}\"\n",
    ")\n",
    "\n",
    "display_eval_report(\n",
    "    (\n",
    "        \"Gemini Tool Use Quality Evaluation Metrics\",\n",
    "        eval_result.summary_metrics,\n",
    "        eval_result.metrics_table,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "evaluate_gemini_tool_use.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

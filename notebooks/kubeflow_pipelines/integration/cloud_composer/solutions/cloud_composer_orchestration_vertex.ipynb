{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Using Cloud Composer to orchestrate Kubeflow pipeline on Vertex AI"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to create a custom DAG for Cloud Composer to trigger and check status of Kubeflow pipeline on Vertex AI\n",
    "1. Learn how to write a Cloud Build config file to build and push all the artifacts for a KFP\n",
    "1. Learn how to setup a Cloud Build GitHub trigger a new run of the Kubeflow PIpeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading an Airflow DAG to Cloud Composer Storage via Jupyter (Illustrative)\n",
    "This notebook demonstrates the Python code for an Airflow DAG and shows how you *would* programmatically interact with Google Cloud Storage to upload files.\n",
    "**Important Notes:**\n",
    "1.  **DAG Execution:** Airflow DAGs are typically uploaded directly to your Cloud Composer environment's GCS DAGs folder. Airflow workers then discover and parse these files. You do *not* run the DAG code directly from this notebook to execute the Airflow workflow.\n",
    "2.  **Authentication:** To run the GCS upload code, ensure your Jupyter environment (e.g., Colab, a local Jupyter server with `gcloud` authenticated, or a Vertex AI Workbench instance) has the necessary Google Cloud credentials and permissions to write to your Composer DAGs bucket.\n",
    "3.  **Cloud Composer DAGs Folder:** The target GCS path for DAGs in Cloud Composer is usually `gs://YOUR_COMPOSER_BUCKET/dags/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\"\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"ARTIFACT_STORE\"] = ARTIFACT_STORE\n",
    "VERTEX_AI_PIPELINE_YAML = \"gs://your-bucket-name/path/to/covertype_kfp_pipeline.yaml\" # TODO: Update path to your compiled KFP YAML\n",
    "GCS_SOURCE_DATASET_PATH = \"data/covertype/dataset.csv\"\n",
    "BIGQUERY_DATASET_ID\n",
    "TABLE_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAG Code\n",
    "Below is the Airflow DAG (`demo_vertex_ai_pipeline_integration.py`) that you intend to upload to your Cloud Composer environment."
   ]
  },
  {
   "metadata": {
    "tags": []
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "airflow_dag_code = f\"\"\"\n",
    "import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.google.cloud.operators.vertex_ai.pipeline_job import (\n",
    "    DeletePipelineJobOperator,\n",
    "    GetPipelineJobOperator,\n",
    "    RunPipelineJobOperator,\n",
    ")\n",
    "from airflow.providers.google.cloud.transfers.bigquery_to_gcs import (\n",
    "    BigQueryToGCSOperator,\n",
    ")\n",
    "from airflow.providers.google.cloud.transfers.gcs_to_bigquery import (\n",
    "    GCSToBigqueryOperator,\n",
    ")\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "# Replace with your actual project and region\n",
    "PROJECT_ID = \"{PROJECT_ID}\"  # Update with your Project ID\n",
    "REGION = \"{REGION}\"\n",
    "\n",
    "# Path to a compiled kubeflow pipeline yaml\n",
    "VERTEX_AI_PIPELINE_YAML = \"{VERTEX_AI_PIPELINE_YAML}\" # Update path to your compiled KFP YAML\n",
    "\n",
    "GCS_SOURCE_DATASET_PATH = \"{GCS_SOURCE_DATASET_PATH}\"\n",
    "GCS_BUCKET_NAME = \"asl-public\" # This is a public bucket, if you use your own data, use your own bucket name\n",
    "\n",
    "GCS_TRAIN_DATASET_PATH = \"gs://your-bucket-name/data/train_export.csv\" # <<< IMPORTANT: Update path for exported training data\n",
    "\n",
    "# Put your BigQuery dataset id here:\n",
    "BIGQUERY_DATASET_ID = \"airflow_demo_dataset\"\n",
    "TABLE_ID = \"covertype\"\n",
    "\n",
    "BIGQUERY_TABLE_SCHEMA = (\n",
    "    [\n",
    "        {\"name\": \"Elevation\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Aspect\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Slope\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\n",
    "            \"name\": \"Horizontal_Distance_To_Hydrology\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Vertical_Distance_To_Hydrology\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Horizontal_Distance_To_Roadways\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\"name\": \"Hillshade_9am\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Hillshade_Noon\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Hillshade_3pm\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\n",
    "            \"name\": \"Horizontal_Distance_To_Fire_Points\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\",\n",
    "        },\n",
    "        {\"name\": \"Wilderness_Area\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Soil_Type\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"Cover_Type\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"demo_vertex_ai_pipeline_integration\",\n",
    "    start_date=datetime.datetime(2025, 1, 1),\n",
    "    schedule=None,\n",
    "    catchup=False,\n",
    "    tags=[\"vertex_ai\", \"pipeline\", \"ml\"],\n",
    "    params={\n",
    "        \"gcs_train_dataset_path\": GCS_TRAIN_DATASET_PATH,\n",
    "    },\n",
    ") as dag:\n",
    "\n",
    "    # Loading dataset from GCS to BigQuery (Emulating basic ETL process)\n",
    "    load_gcs_to_bigquery = GCSToBigqueryOperator(\n",
    "        task_id=\"load_csv_to_bigquery\",\n",
    "        bucket=GCS_BUCKET_NAME,\n",
    "        source_objects=[GCS_SOURCE_DATASET_PATH],\n",
    "        destination_project_dataset_table=\"{BIGQUERY_DATASET_ID}.{TABLE_ID}\",\n",
    "        # Optional: Define schema, remove if auto-detect works for you\n",
    "        schema_fields=BIGQUERY_TABLE_SCHEMA,\n",
    "        # Or \"NEWLINE_DELIMITED_JSON\", \"PARQUET\", \"AVRO\", etc.\n",
    "        source_format=\"CSV\",\n",
    "        # Creates the table if it doesn't exist\n",
    "        create_disposition=\"CREATE_IF_NEEDED\",\n",
    "        # Overwrites the table if it exists. Use \"WRITE_APPEND\" to append.\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        skip_leading_rows=1,  # For CSVs with a header row\n",
    "        field_delimiter=\",\",  # For CSVs\n",
    "    )\n",
    "\n",
    "    # exporting dataset from BigQuery to GCS\n",
    "    bigquery_to_gcs = BigQueryToGCSOperator(\n",
    "        task_id=\"bigquery_to_gcs_export\",\n",
    "        source_project_dataset_table=f\"{BIGQUERY_DATASET_ID}.{TABLE_ID}\",\n",
    "        destination_cloud_storage_uris=GCS_TRAIN_DATASET_PATH,\n",
    "        export_format=\"CSV\",\n",
    "        print_header=True,\n",
    "    )\n",
    "\n",
    "    # Triggering a pipeline from a GCS compiled yaml file\n",
    "    run_vertex_ai_pipeline = RunPipelineJobOperator(\n",
    "        task_id=\"start_vertex_ai_pipeline\",\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        template_path=VERTEX_AI_PIPELINE_YAML,\n",
    "        # example of passing params to kubeflow pipeline\n",
    "        parameter_values={\n",
    "            \"training_file_path\": \"{{ params.gcs_train_dataset_path }}\",\n",
    "        },\n",
    "        # Unique display name\n",
    "        display_name=\"triggered-demo-pipeline-{{ ts_nodash }}\",\n",
    "    )\n",
    "\n",
    "    # Fetching VertexAI pipeline job information\n",
    "    get_vertexai_ai_pipline_status = GetPipelineJobOperator(\n",
    "        task_id=\"vertex_ai_pipline_status\",\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        pipeline_job_id=\"{{ task_instance.xcom_pull(\"\n",
    "        \"task_ids='start_vertex_ai_pipeline', \"\n",
    "        \"key='pipeline_job_id') }}\",\n",
    "    )\n",
    "\n",
    "    # Deleting VertexAI pipeline job\n",
    "    delete_pipeline_job = DeletePipelineJobOperator(\n",
    "        task_id=\"delete_vertex_ai_pipeline_job\",\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        pipeline_job_id=\"{{ task_instance.xcom_pull(\"\n",
    "        \"task_ids='start_vertex_ai_pipeline', \"\n",
    "        \"key='pipeline_job_id') }}\",\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "    )\n",
    "\n",
    "    # Combine all steps into a DAG\n",
    "    (\n",
    "        load_gcs_to_bigquery\n",
    "        >> bigquery_to_gcs\n",
    "        >> run_vertex_ai_pipeline\n",
    "        >> get_vertexai_ai_pipline_status\n",
    "        >> delete_pipeline_job\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Also, this notebook assumes the dataset is already created and stored in Google Cloud Storage following the instructions covered in the [walkthrough notebook](https://github.com/GoogleCloudPlatform/asl-ml-immersion/blob/master/notebooks/kubeflow_pipelines/walkthrough/solutions/kfp_walkthrough_vertex.ipynb).\n",
    "\n",
    "If you haven't run it, please run the cell below and create the dataset before running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp gs://asl-public/data/covertype/training/dataset.csv $ARTIFACT_STORE/data/training/dataset.csv\n",
    "gsutil cp gs://asl-public/data/covertype/validation/dataset.csv $ARTIFACT_STORE/data/validation/dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a GCS bucket to save the build log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET = PROJECT_ID + \"-cicd-log\"\n",
    "os.environ[\"BUCKET\"] = BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the DAG to a Local File\n",
    "First, let's save the DAG code to a local Python file."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dag_filename = \"demo_vertex_ai_pipeline_integration.py\"\n",
    "\n",
    "with open(dag_filename, \"w\") as f:\n",
    "    f.write(airflow_dag_code)\n",
    "\n",
    "print(f\"DAG saved locally as {dag_filename}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Uploading the DAG to Cloud Composer Storage\n",
    "\n",
    "To deploy this DAG to your Cloud Composer environment, you need to upload it to the DAGs folder in your Composer's associated Cloud Storage bucket.\n",
    "\n",
    "**Before running this cell, make sure you have:**\n",
    "1.  **Installed Google Cloud Storage client library:** `pip install google-cloud-storage`\n",
    "2.  **Authenticated:** Your environment needs to be authenticated to GCP (e.g., `gcloud auth application-default login` or running in a GCP VM/Cloud Run/Vertex AI Workbench).\n",
    "3.  **Identified your Composer DAGs bucket:** This is typically named `gs://us-central1-YOUR_COMPOSER_ENV_NAME-HASH-bucket/dags/`. You can find this in the Cloud Composer console.\n",
    "#"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

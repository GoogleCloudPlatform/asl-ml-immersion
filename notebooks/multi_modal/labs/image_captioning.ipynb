{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning with Visual Attention \n",
    "\n",
    "## Learning Objectives\n",
    "1. Learn how to create an image captioning model\n",
    "2. Learn how to train and predict a text generation model.\n",
    "\n",
    "Image captioning models take an image as input, and output text. Ideally, we want the output of the model to accurately describe the events/things in the image, similar to a caption a human might provide. <br>\n",
    "For example, given an image like the example below, the model is expected to generate a caption such as *\"some people are playing baseball.\"*.\n",
    "\n",
    "<div><img src=\"../sample_images/baseball.jpeg\" width=\"500\"></div>\n",
    "\n",
    "In order to generate text, we will build an encoder-decoder model, where the encoder output embedding of an input image, and the decoder output text from the image embedding<br>\n",
    "\n",
    "I this notebook, we will use the model architecture similar to [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044), and build Attention-based image captioning model.\n",
    "\n",
    "This notebook is an end-to-end example. The training dataset is the COCO large-scale object detection, segmentation, and captioning dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G20XL3WTBXhj",
    "outputId": "a928c30a-f6d6-4120-9da1-f4f5753b9821"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from textwrap import wrap\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU,\n",
    "    Add,\n",
    "    AdditiveAttention,\n",
    "    Attention,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    LayerNormalization,\n",
    "    Reshape,\n",
    "    StringLookup,\n",
    "    TextVectorization,\n",
    ")\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr6sJGEIBe7D"
   },
   "source": [
    "## Read and prepare dataset\n",
    "\n",
    "We will use the TensorFlow datasets capability to read the [COCO captions](https://www.tensorflow.org/datasets/catalog/coco_captions) dataset.\n",
    "This version contains images, bounding boxes, labels, and captions from COCO 2014, split into the subsets defined by Karpathy and Li (2015) and takes\n",
    "care of some data quality issues with the original dataset (for example, some\n",
    "of the images in the original dataset did not have captions)\n",
    "\n",
    "First, let's define some constants.<br>\n",
    "In this lab, we will use a pretrained [InceptionResNetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_resnet_v2/InceptionResNetV2) model from `tf.keras.applications` as a feature extractor, so some constants are comming from the InceptionResNetV2 model definition.<br>\n",
    "So if you want to use other type of base model, please make sure to change these constants as well.\n",
    "\n",
    "`tf.keras.applications` is a pretrained model repository like [TensorFlow Hub](https://tfhub.dev), but while Tensorflow Hub hosts models for different modalities including image, text, audio, and so on, `tf.keras.application` only hosts popular and stable models for images.<br>\n",
    "However, `tf.keras.applications` is more flexible as it contains model metadata and it allow us to access and control the model behavior, while most of the TensorFlow Hub based models that only contains compiled SavedModels.<br>\n",
    "So, for example, we can get output not only from the final layer of the model (e.g. flattend 1D Tensor output of CNN models), but also from intermediate layers (e.g. intermediate 3D Tensor) by accessing layer metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2WQtNeGBbMD"
   },
   "outputs": [],
   "source": [
    "# Change these to control the accuracy/speed\n",
    "VOCAB_SIZE = 20000  # use fewer words to speed up convergence\n",
    "ATTENTION_DIM = 512  # size of dense layer in Attention\n",
    "WORD_EMBEDDING_DIM = 128\n",
    "\n",
    "# InceptionResNetV2 takes (299, 299, 3) image as inputs\n",
    "# and return features in (8, 8, 1536) shape\n",
    "FEATURE_EXTRACTOR = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
    "    include_top=False, weights=\"imagenet\"\n",
    ")\n",
    "IMG_HEIGHT = 299\n",
    "IMG_WIDTH = 299\n",
    "IMG_CHANNELS = 3\n",
    "FEATURES_SHAPE = (8, 8, 1536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Preprocess\n",
    "Here we preprocess the dataset. The function below:\n",
    "- resize image to (`IMG_HEIGHT`, `IMG_WIDTH`) shape\n",
    "- rescale pixel values from [0, 255] to [0, 1]\n",
    "- return image(`image_tensor`) and captions(`captions`) dictionary.\n",
    "\n",
    "**Note**: This dataset is too large to store in an local environment. Therefore, It is stored in a public GCS bucket located in us-central1. \n",
    "So if you access it from a Notebook outside the US, it will be (a) slow and (b) subject to a network charge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JxC6DhwAcw5"
   },
   "outputs": [],
   "source": [
    "GCS_DIR = \"gs://asl-public/data/tensorflow_datasets/\"\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "\n",
    "def get_image_label(example):\n",
    "    caption = example[\"captions\"][\"text\"][0]  # only the first caption per image\n",
    "    img = example[\"image\"]\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    img = img / 255\n",
    "    return {\"image_tensor\": img, \"caption\": caption}\n",
    "\n",
    "\n",
    "trainds = tfds.load(\"coco_captions\", split=\"train\", data_dir=GCS_DIR)\n",
    "\n",
    "trainds = trainds.map(\n",
    "    get_image_label, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").shuffle(BUFFER_SIZE)\n",
    "trainds = trainds.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize \n",
    "Let's take a look at images and sample captions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "KGz2bQaKV3iI",
    "outputId": "4bd05dee-e0f5-4934-ac0d-2afa0a20bad8"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for idx, data in enumerate(trainds.take(4)):\n",
    "    ax[idx].imshow(data[\"image_tensor\"].numpy())\n",
    "    caption = \"\\n\".join(wrap(data[\"caption\"].numpy().decode(\"utf-8\"), 30))\n",
    "    ax[idx].set_title(caption)\n",
    "    ax[idx].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4dyKHB2W4vZ"
   },
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "We add special tokens to represent the starts (`<start>`) and the ends (`<end>`) of sentences.<br>\n",
    "Start and end tokens are added here because we are using an encoder-decoder model and during prediction, to get the captioning started we use `<start>` and since captions are of variable length, we terminate the prediction when we see the `<end>` token.\n",
    "\n",
    "Then create a full list of the captions for further preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_token(data):\n",
    "    start = tf.convert_to_tensor(\"<start>\")\n",
    "    end = tf.convert_to_tensor(\"<end>\")\n",
    "    data[\"caption\"] = tf.strings.join(\n",
    "        [start, data[\"caption\"], end], separator=\" \"\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "trainds = trainds.map(add_start_end_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and tokenize the captions\n",
    "\n",
    "You will transform the text captions into integer sequences using the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer, with the following steps:\n",
    "\n",
    "* Use [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt) to iterate over all captions, split the captions into words, and compute a vocabulary of the top `VOCAB_SIZE` words.\n",
    "* Tokenize all captions by mapping each word to its index in the vocabulary. All output sequences will be padded to the length `MAX_CAPTION_LEN`. Here we directly specify `64` number which is sufficient for this dataset, but please note that this value should be computed by processing the entire dataset if you don't want to cut down very long sentense in a dataset.\n",
    "\n",
    "**Note**: This process takes around 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Complete the `TextVectorization` layer referring to [the document](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization?version=nightly).\n",
    "\n",
    "**Hint**: You can use `VOCAB_SIZE` and `MAX_CAPTION_LEN` variables, and `standardize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2FW4ob5NikW",
    "outputId": "60f31007-6f03-4136-886a-0f9447b0c12b"
   },
   "outputs": [],
   "source": [
    "MAX_CAPTION_LEN = 64\n",
    "\n",
    "\n",
    "# We will override the default standardization of TextVectorization to preserve\n",
    "# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\n",
    "def standardize(inputs):\n",
    "    inputs = tf.strings.lower(inputs)\n",
    "    return tf.strings.regex_replace(\n",
    "        inputs, r\"[!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~]?\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Choose the most frequent words from the vocabulary & remove punctuation etc.\n",
    "# TODO: Complete the TextVectorization layer.\n",
    "tokenizer = TextVectorization(...)\n",
    "\n",
    "tokenizer.adapt(trainds.map(lambda x: x[\"caption\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's try to tokenize a sample text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"<start> This is a sentence <end>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_captions = []\n",
    "for d in trainds.take(5):\n",
    "    sample_captions.append(d[\"caption\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(sample_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that all the sentenses starts and ends with the same token (e.g. '3' and '4'). These values represent start tokens and end tokens respectively.\n",
    "\n",
    "You can also convert ids to original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wordid in tokenizer([sample_captions[0]])[0]:\n",
    "    print(tokenizer.get_vocabulary()[wordid], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can create Word <-> Index converters using `StringLookup` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table: Word -> Index\n",
    "word_to_index = StringLookup(\n",
    "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary()\n",
    ")\n",
    "\n",
    "# Lookup table: Index -> Word\n",
    "index_to_word = StringLookup(\n",
    "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tf.data dataset for training\n",
    "Now Let's apply the adapted tokenization to all the examples and create tf.data Dataset for training.\n",
    "\n",
    "Here note that we are also creating labels by shifting texts from feature captions.<br>\n",
    "If we have an input caption `\"<start> I love cats <end>\"`, its label should be `\"I love cats <end> <padding>\"`.<br>\n",
    "With that, our model can try to learn to predict `I` from `<start>`.\n",
    "\n",
    "The dataset should return tuples, where the first elements are features (`image_tensor` and `caption`) and the second elements are labels (target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Define `create_ds_fn` to create the dataset. It should have the steps below:\n",
    "- Roll the caption by one step using `tf.roll`. The rolled caption will be used as a target.\n",
    "- Add zero value (padding) to the rolled caption to create the same length vector as the original caption.\n",
    "- Return the features (image and original caption) and the target (rolled caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0LHFYjhBo32",
    "outputId": "647be30d-0357-49a2-a49b-010b59696ed0"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def create_ds_fn(data):\n",
    "    img_tensor = data[\"image_tensor\"]\n",
    "    caption = tokenizer(data[\"caption\"])\n",
    "\n",
    "    # TODO: Define the steps.\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "batched_ds = (\n",
    "    trainds.map(create_ds_fn)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img, caption), label in batched_ds.take(2):\n",
    "    print(f\"Image shape: {img.shape}\")\n",
    "    print(f\"Caption shape: {caption.shape}\")\n",
    "    print(f\"Label shape: {label.shape}\")\n",
    "    print(caption[0])\n",
    "    print(label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ehA-gDDYh47"
   },
   "source": [
    "## Model\n",
    "Now let's design an image captioning model.<br>\n",
    "It consists of an image encoder, followed by a caption decoder.\n",
    "\n",
    "### Image Encoder\n",
    "The image encoder model is very simple. It extracts features through a pre-trained model and passes them to a fully connected layer.\n",
    "\n",
    "1. In this example, we extract the features from convolutional layers of InceptionResNetV2 which gives us a vector of (Batch Size, 8, 8, 1536).\n",
    "1. We reshape the vector to (Batch Size, 64, 1536)\n",
    "1. We squash it to a length of `ATTENTION_DIM` with a Dense Layer and return (Batch Size, 64, ATTENTION_DIM)\n",
    "1. Later, the Attention layer attends over the image to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_EXTRACTOR.trainable = False\n",
    "\n",
    "image_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "image_features = FEATURE_EXTRACTOR(image_input)\n",
    "\n",
    "x = Reshape((FEATURES_SHAPE[0] * FEATURES_SHAPE[1], FEATURES_SHAPE[2]))(\n",
    "    image_features\n",
    ")\n",
    "encoder_output = Dense(ATTENTION_DIM, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.Model(inputs=image_input, outputs=encoder_output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Decoder\n",
    "The caption decoder incorporates an attention mechanism that focuses on different parts of the input image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The attention head\n",
    "\n",
    "The decoder uses attention to selectively focus on parts of the input sequence.\n",
    "The attention takes a sequence of vectors as input for each example and returns an \"attention\" vector for each example. \n",
    "\n",
    "Let's look at how this works:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/6895245/173408554-d4b6387b-248b-421e-8911-550d0561d001.png\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/6895245/173408648-38c6b582-a68b-4697-982a-1d885b83dd0b.png\" alt=\"attention equation 2\" width=\"800\">\n",
    "\n",
    "Where:\n",
    "\n",
    "* $s$ is the encoder index.\n",
    "* $t$ is the decoder index.\n",
    "* $\\alpha_{ts}$ is the attention weights.\n",
    "* $h_s$ is the sequence of encoder outputs being attended to (the attention \"key\" and \"value\" in transformer terminology).\n",
    "* $h_t$ is the decoder state attending to the sequence (the attention \"query\" in transformer terminology).\n",
    "* $c_t$ is the resulting context vector.\n",
    "\n",
    "The equations:\n",
    "\n",
    "1. Calculates the attention weights, $\\alpha_{ts}$, as a softmax across the encoder's output sequence.\n",
    "2. Calculates the context vector as the weighted sum of the encoder outputs.\n",
    "\n",
    "Last is the $score$ function. Its job is to calculate a scalar logit-score for each key-query pair. There are two common approaches:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/6895245/173408773-3781cacc-de00-49c6-9909-f6cd65a0501b.png\" alt=\"attention equation 4\" width=\"800\">\n",
    "\n",
    "This notebook implement Luong-style attention using pre-defined `layers.Attention`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Steps\n",
    "\n",
    "The decoder's job is to generate predictions for the next output token.\n",
    "\n",
    "1. The decoder receives current word tokens as a batch.\n",
    "1. It embeds the word tokens to `ATTENTION_DIM` dimension.\n",
    "1. GRU layer keeps track of the word embeddings, and returns GRU outputs and states.\n",
    "1. Bahdanau-style attention attends over the encoder's output feature by using GRU outputs as a query.\n",
    "1. The attention outputs and GRU outputs are added (skip connection), and normalized in a layer normalization layer.\n",
    "1. It generates logit predictions for the next token based on the GRU output.\n",
    "\n",
    "We can define all the steps in Keras Functional API, but please note that here we instantiate layers that have trainable parameters so that we reuse the layers and the weights in inference phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Define the decoder steps following the instructions above.\n",
    "\n",
    "**Reference**:\n",
    "- [Embedding Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding?version=nightly)\n",
    "- [GPU Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n",
    "- [Attention Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_input = Input(shape=(MAX_CAPTION_LEN,), name=\"words\")\n",
    "\n",
    "# TODO: Define the Embedding layer\n",
    "embed_x = Embedding(...)(word_input)\n",
    "\n",
    "# TODO: Define the GRU layer.\n",
    "decoder_gru = GRU(...)\n",
    "gru_output, gru_state = decoder_gru(embed_x)\n",
    "\n",
    "decoder_attention = Attention()\n",
    "# TODO: Define the inputs to the Attention layer\n",
    "context_vector = decoder_attention([...])\n",
    "\n",
    "addition = Add()([gru_output, context_vector])\n",
    "\n",
    "layer_norm = LayerNormalization(axis=-1)\n",
    "layer_norm_out = layer_norm(addition)\n",
    "\n",
    "decoder_output_dense = Dense(VOCAB_SIZE)\n",
    "decoder_output = decoder_output_dense(layer_norm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = tf.keras.Model(\n",
    "    inputs=[word_input, encoder_output], outputs=decoder_output\n",
    ")\n",
    "tf.keras.utils.plot_model(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model\n",
    "\n",
    "Now we defined the encoder and the decoder. Let's combine them into an image model for training.<br>\n",
    "It has two inputs (`image_input` and `word_input`, and an output (`decoder_output`). This definition should correspond to the definition of the dataset pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_caption_train_model = tf.keras.Model(\n",
    "    inputs=[image_input, word_input], outputs=decoder_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "The loss function is a simple cross-entropy, but we need to remove padding (`0`) when calculating it.<br>\n",
    "So here we extract the length of the sentence (non-0 part), and compute the average of the loss only over the valid sentence part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # returns 1 to word index and 0 to padding (e.g. [1,1,1,1,1,0,0,0,0,...,0])\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int32)\n",
    "    sentence_len = tf.reduce_sum(mask)\n",
    "    loss_ = loss_[:sentence_len]\n",
    "\n",
    "    return tf.reduce_mean(loss_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_caption_train_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=loss_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGVl8cQpZ5Qu"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "Now we can train the model using the standard `model.fit` API.<br>\n",
    "It takes around 15-20 minutes with NVIDIA T4 GPU to train 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = image_caption_train_model.fit(batched_ds, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otiuFI4ZaK6w"
   },
   "source": [
    "## Caption!\n",
    "\n",
    "The predict step is different from the training, since we need to keep track of the GRU state during the caption generation, and pass a predicted word to the decoder as an input at the next time step.\n",
    "\n",
    "In order to do so, let's define another model for prediction while using the trained weights, so that it can keep and update the GRU state during the caption generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Complete the decoder for prediction.\n",
    "\n",
    "**Hint**: Most parts of the architecture are the same as training, but this prediction model has additional I/O to pass through the GRU state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the entire prediction model referring to the traiing model above.\n",
    "\n",
    "gru_state_input = Input(shape=(ATTENTION_DIM,), name=\"gru_state_input\")\n",
    "\n",
    "# Reuse trained GRU, but update it so that it can receive states.\n",
    "gru_output, gru_state = decoder_gru(...)\n",
    "\n",
    "# Reuse other layers as well\n",
    "context_vector = decoder_attention([...])\n",
    "addition_output = Add()([...])\n",
    "layer_norm_output = layer_norm(...)\n",
    "\n",
    "decoder_output = decoder_output_dense(...)\n",
    "\n",
    "# Define prediction Model with state input and output\n",
    "decoder_pred_model = tf.keras.Model(\n",
    "    inputs=[...],\n",
    "    outputs=[...],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Initialize the GRU states as zero vectors.\n",
    "1. Preprocess an input image, pass it to the encoder, and extract image features.\n",
    "1. Setup word tokens of `<start>` to start captioning.\n",
    "1. In the for loop, we\n",
    "    - pass word tokens (`dec_input`), GRU states (`gru_state`) and image features (`features`) to the prediction decoder and get predictions (`predictions`), and the updated GRU states.\n",
    "    - select Top-K words from logits, and choose a word probabilistically so that we avoid computing softmax over VOCAB_SIZE-sized vector.\n",
    "    - stop predicting when the model predicts the `<end>` token.\n",
    "    - replace the input word token with the predicted word token for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkmKr8nxMNyG"
   },
   "outputs": [],
   "source": [
    "MINIMUM_SENTENCE_LENGTH = 5\n",
    "\n",
    "\n",
    "## Probabilistic prediction using the trained model\n",
    "def predict_caption(filename):\n",
    "    gru_state = tf.zeros((1, ATTENTION_DIM))\n",
    "\n",
    "    img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    img = img / 255\n",
    "\n",
    "    features = encoder(tf.expand_dims(img, axis=0))\n",
    "    dec_input = tf.expand_dims([word_to_index(\"<start>\")], 1)\n",
    "    result = []\n",
    "    for i in range(MAX_CAPTION_LEN):\n",
    "        predictions, gru_state = decoder_pred_model(\n",
    "            [dec_input, gru_state, features]\n",
    "        )\n",
    "\n",
    "        # draws from log distribution given by predictions\n",
    "        top_probs, top_idxs = tf.math.top_k(\n",
    "            input=predictions[0][0], k=10, sorted=False\n",
    "        )\n",
    "        chosen_id = tf.random.categorical([top_probs], 1)[0].numpy()\n",
    "        predicted_id = top_idxs.numpy()[chosen_id][0]\n",
    "\n",
    "        result.append(tokenizer.get_vocabulary()[predicted_id])\n",
    "\n",
    "        if predicted_id == word_to_index(\"<end>\"):\n",
    "            return img, result\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 1)\n",
    "\n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's caption! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../sample_images/baseball.jpeg\"  # you can also try surf.jpeg\n",
    "\n",
    "for i in range(5):\n",
    "    image, caption = predict_caption(filename)\n",
    "    print(\" \".join(caption[:-1]) + \".\")\n",
    "\n",
    "img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look?<br>\n",
    "It seems the model captures the key aspects of the image: people, baseball, and a ball, although it could not be grammatically perfect.\n",
    "\n",
    "### Optional Task:\n",
    "Upload your own image and generate captions with it.<br>\n",
    "Also, you can try to train longer to achieve better captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We learned how to build an image captioning model by creating an image encoder and a text decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_fNzWuY2UoB"
   },
   "source": [
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fa3LRWv9BNk9",
    "y4dyKHB2W4vZ",
    "1ehA-gDDYh47"
   ],
   "name": "laks_work.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

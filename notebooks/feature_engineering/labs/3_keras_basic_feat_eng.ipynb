{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNdWfPXCjTjY"
   },
   "source": [
    "# Basic Feature Engineering in Keras \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "\n",
    "1. Create an input pipeline using tf.data\n",
    "2. Engineer features to create categorical, crossed, and numerical feature columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Overview \n",
    "In this lab, we utilize feature engineering to improve the prediction of housing prices using a Keras Sequential Model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dEreb4QKizj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 00:03:29.597137: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"TensorFlow version: \", tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the Google Machine Learning Courses Programming Exercises use the  [California Housing Dataset](https://developers.google.com/machine-learning/crash-course/california-housing-data-description\n",
    "), which contains data drawn from the 1990 U.S. Census.  Our lab dataset has been pre-processed so that there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2220\n",
      "-rw-r--r-- 1 jupyter jupyter 1435071 May 12 07:39 housing_pre-proc.csv\n",
      "-rw-r--r-- 1 jupyter jupyter  123590 May 12 07:39 taxi-test.csv\n",
      "-rw-r--r-- 1 jupyter jupyter  579055 May 12 07:39 taxi-train.csv\n",
      "-rw-r--r-- 1 jupyter jupyter  123114 May 12 07:39 taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM6-n6xntv3t"
   },
   "source": [
    "Let's read in the dataset and create a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "REZ57BXCLdfG",
    "outputId": "a6ef2eda-c7eb-4e2d-92e4-e7fcaa20b0af",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df = pd.read_csv(\"../data/housing_pre-proc.csv\", on_bad_lines=\"skip\")\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use .describe() to see some summary statistics for the numeric fields in our dataframe. Note, for example, the count row and corresponding columns. The count shows 20433.000000 for all feature columns. Thus, there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.570689</td>\n",
       "      <td>35.633221</td>\n",
       "      <td>28.633094</td>\n",
       "      <td>2636.504233</td>\n",
       "      <td>537.870553</td>\n",
       "      <td>1424.946949</td>\n",
       "      <td>499.433465</td>\n",
       "      <td>3.871162</td>\n",
       "      <td>206864.413155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.003578</td>\n",
       "      <td>2.136348</td>\n",
       "      <td>12.591805</td>\n",
       "      <td>2185.269567</td>\n",
       "      <td>421.385070</td>\n",
       "      <td>1133.208490</td>\n",
       "      <td>382.299226</td>\n",
       "      <td>1.899291</td>\n",
       "      <td>115435.667099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>14999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2.563700</td>\n",
       "      <td>119500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.490000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2127.000000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>3.536500</td>\n",
       "      <td>179700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.720000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3143.000000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>1722.000000</td>\n",
       "      <td>604.000000</td>\n",
       "      <td>4.744000</td>\n",
       "      <td>264700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6445.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>6082.000000</td>\n",
       "      <td>15.000100</td>\n",
       "      <td>500001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  20433.000000  20433.000000        20433.000000  20433.000000   \n",
       "mean    -119.570689     35.633221           28.633094   2636.504233   \n",
       "std        2.003578      2.136348           12.591805   2185.269567   \n",
       "min     -124.350000     32.540000            1.000000      2.000000   \n",
       "25%     -121.800000     33.930000           18.000000   1450.000000   \n",
       "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
       "75%     -118.010000     37.720000           37.000000   3143.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \\\n",
       "count    20433.000000  20433.000000  20433.000000   20433.000000   \n",
       "mean       537.870553   1424.946949    499.433465       3.871162   \n",
       "std        421.385070   1133.208490    382.299226       1.899291   \n",
       "min          1.000000      3.000000      1.000000       0.499900   \n",
       "25%        296.000000    787.000000    280.000000       2.563700   \n",
       "50%        435.000000   1166.000000    409.000000       3.536500   \n",
       "75%        647.000000   1722.000000    604.000000       4.744000   \n",
       "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
       "\n",
       "       median_house_value  \n",
       "count        20433.000000  \n",
       "mean        206864.413155  \n",
       "std         115435.667099  \n",
       "min          14999.000000  \n",
       "25%         119500.000000  \n",
       "50%         179700.000000  \n",
       "75%         264700.000000  \n",
       "max         500001.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0zhLtQqMPem"
   },
   "source": [
    "####  Split the dataset for ML\n",
    "\n",
    "The dataset we loaded was a single CSV file. We will split this into train, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "YEOpw7LhMYsI",
    "outputId": "6161a660-7133-465a-d754-d7acae2b68c8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13076 train examples\n",
      "3270 validation examples\n",
      "4087 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(housing_df, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "print(len(train), \"train examples\")\n",
    "print(len(val), \"validation examples\")\n",
    "print(len(test), \"test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dz9kfjOMBX9U"
   },
   "source": [
    "Now, we need to output the split files.  We will specifically need the test.csv later for testing.  You should see the files appear in the home directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "ADX23QUu_Wiu",
    "outputId": "e97fa59e-4ed4-48a3-8fba-c95f293944ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"../data/housing-train.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val.to_csv(\"../data/housing-val.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "CU1FgmKEAmWh",
    "outputId": "2cce91e1-2c4a-4fe8-a6c3-3da52cb9458f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.to_csv(\"../data/housing-test.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ../data/housing-test.csv <==\n",
      "longitude,latitude,housing_median_age,total_rooms,total_bedrooms,population,households,median_income,median_house_value,ocean_proximity\n",
      "-122.07,37.71,40.0,1808.0,302.0,746.0,270.0,5.3015,254900.0,NEAR BAY\n",
      "-117.97,33.81,30.0,2406.0,462.0,1753.0,456.0,4.485,180600.0,<1H OCEAN\n",
      "-118.1,33.93,33.0,1474.0,325.0,1205.0,335.0,3.1397,166800.0,<1H OCEAN\n",
      "-119.75,36.72,22.0,3247.0,859.0,4179.0,881.0,1.3343,60800.0,INLAND\n",
      "-119.72,34.44,39.0,1489.0,304.0,700.0,268.0,3.8819,289900.0,<1H OCEAN\n",
      "-121.78,39.71,8.0,140.0,28.0,84.0,29.0,2.125,179200.0,INLAND\n",
      "-117.4,34.0,31.0,1192.0,307.0,1013.0,283.0,2.0742,76200.0,INLAND\n",
      "-122.29,37.87,52.0,895.0,198.0,386.0,204.0,3.875,182600.0,NEAR BAY\n",
      "-121.65,39.76,31.0,1599.0,318.0,794.0,303.0,3.0,96700.0,INLAND\n",
      "\n",
      "==> ../data/housing-train.csv <==\n",
      "longitude,latitude,housing_median_age,total_rooms,total_bedrooms,population,households,median_income,median_house_value,ocean_proximity\n",
      "-117.24,34.12,29.0,2654.0,667.0,1822.0,593.0,2.1563,72300.0,INLAND\n",
      "-117.95,33.78,32.0,2296.0,560.0,1376.0,532.0,3.7303,188500.0,<1H OCEAN\n",
      "-117.3,34.1,44.0,589.0,130.0,504.0,137.0,1.775,63400.0,INLAND\n",
      "-116.98,32.74,17.0,3943.0,843.0,1995.0,766.0,2.6944,158300.0,<1H OCEAN\n",
      "-117.2,33.38,14.0,5392.0,821.0,2350.0,810.0,5.0507,291500.0,<1H OCEAN\n",
      "-122.18,37.45,43.0,2061.0,437.0,817.0,385.0,4.4688,460200.0,NEAR BAY\n",
      "-122.3,41.32,13.0,2300.0,513.0,1151.0,488.0,2.1571,81500.0,INLAND\n",
      "-121.29,38.1,14.0,1551.0,297.0,785.0,281.0,3.775,163300.0,INLAND\n",
      "-118.25,34.14,37.0,584.0,260.0,552.0,235.0,1.8235,275000.0,<1H OCEAN\n",
      "\n",
      "==> ../data/housing-val.csv <==\n",
      "longitude,latitude,housing_median_age,total_rooms,total_bedrooms,population,households,median_income,median_house_value,ocean_proximity\n",
      "-119.03,36.08,19.0,2471.0,431.0,1040.0,426.0,3.25,80600.0,INLAND\n",
      "-119.18,34.21,29.0,4039.0,680.0,1677.0,644.0,4.3897,257600.0,NEAR OCEAN\n",
      "-120.99,37.66,39.0,1748.0,329.0,831.0,302.0,2.5938,135600.0,INLAND\n",
      "-118.09,33.91,34.0,1582.0,343.0,1356.0,324.0,3.4211,141100.0,<1H OCEAN\n",
      "-120.38,37.99,36.0,2864.0,603.0,1155.0,565.0,2.3571,113400.0,INLAND\n",
      "-119.08,37.78,17.0,1631.0,335.0,285.0,128.0,2.7656,130000.0,INLAND\n",
      "-118.58,34.2,35.0,1323.0,228.0,756.0,216.0,4.2330000000000005,221300.0,<1H OCEAN\n",
      "-122.14,37.73,52.0,2024.0,320.0,823.0,334.0,5.0,264700.0,NEAR BAY\n",
      "-116.44,33.93,17.0,5293.0,1266.0,1201.0,599.0,1.6849,88400.0,INLAND\n",
      "\n",
      "==> ../data/housing_pre-proc.csv <==\n",
      "longitude,latitude,housing_median_age,total_rooms,total_bedrooms,population,households,median_income,median_house_value,ocean_proximity\n",
      "-122.23,37.88,41.0,880.0,129.0,322.0,126.0,8.3252,452600.0,NEAR BAY\n",
      "-122.22,37.86,21.0,7099.0,1106.0,2401.0,1138.0,8.3014,358500.0,NEAR BAY\n",
      "-122.24,37.85,52.0,1467.0,190.0,496.0,177.0,7.2574,352100.0,NEAR BAY\n",
      "-122.25,37.85,52.0,1274.0,235.0,558.0,219.0,5.6431,341300.0,NEAR BAY\n",
      "-122.25,37.85,52.0,1627.0,280.0,565.0,259.0,3.8462,342200.0,NEAR BAY\n",
      "-122.25,37.85,52.0,919.0,213.0,413.0,193.0,4.0368,269700.0,NEAR BAY\n",
      "-122.25,37.84,52.0,2535.0,489.0,1094.0,514.0,3.6591,299200.0,NEAR BAY\n",
      "-122.25,37.84,52.0,3104.0,687.0,1157.0,647.0,3.12,241400.0,NEAR BAY\n",
      "-122.26,37.84,42.0,2555.0,665.0,1206.0,595.0,2.0804,226700.0,NEAR BAY\n"
     ]
    }
   ],
   "source": [
    "!head ../data/housing*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aj35eYy_lutI"
   },
   "source": [
    "## Create an input pipeline using tf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84ef46LXMfvu"
   },
   "source": [
    "Next, we will wrap the dataframes with [tf.data](https://www.tensorflow.org/guide/datasets). This will enable us  to use feature columns as a bridge to map from the columns in the Pandas dataframe to features used to train the model. \n",
    "\n",
    "**Exercise.** Here, we create an input pipeline using tf.data.  This function is missing two lines.  Correct and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRLGSMDzM-dl"
   },
   "source": [
    "**Exercise.** Now that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "colab_type": "code",
    "id": "CSBo3dUVNFc9",
    "outputId": "d1be2646-b1e5-4110-dbba-5bc49d9b30f6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 00:10:59.814673: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_5' with dtype double and shape [13076]\n",
      "\t [[{{node Placeholder/_5}}]]\n",
      "2025-05-14 00:10:59.815431: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [13076]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_batch, label_batch \u001b[38;5;129;01min\u001b[39;00m train_ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvery feature:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(feature_batch\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA batch of households:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# TODO: Your code goes here\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "    print(\"Every feature:\", list(feature_batch.keys()))\n",
    "    print(\n",
    "        \"A batch of households:\",\n",
    "        # TODO: Your code goes here\n",
    "    )\n",
    "    print(\n",
    "        \"A batch of ocean_proximity:\",\n",
    "        # TODO: Your code goes here\n",
    "    )\n",
    "    print(\n",
    "        \"A batch of targets:\",\n",
    "        # TODO: Your code goes here\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OT5N6Se-NQsC"
   },
   "source": [
    "We can see that the dataset returns a dictionary of column names (from the dataframe) that map to column values from rows in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEGEAqaziwfC"
   },
   "source": [
    "#### Numeric columns\n",
    "The output of a feature column becomes the input to the model. A numeric is the simplest type of column. It is used to represent real valued features. When using this column, your model will receive the column value from the dataframe unchanged.\n",
    "\n",
    "**Exercise.** In the California housing prices dataset, most columns from the dataframe are numeric.  Let' create a variable called `numeric_cols` to hold only the numerical feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = # TODO: Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EwMEcH_52JT8"
   },
   "source": [
    "#### Max-min scaler function\n",
    "It is very important for numerical variables to get scaled before they are \"fed\" into the neural network. Here we use min-max scaling. Here we are creating a function named 'get_scal' which takes a list of numerical features and returns a 'minmax' function, which will be used in `tf.feature_column.numeric_column()` as normalizer_fn in parameters. 'Minmax' function itself takes a 'numerical' number from a particular feature and return scaled value of that number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ig1k5ovWBnN8"
   },
   "source": [
    "**Exercise.** Next, we scale the numerical feature columns that we assigned to the variable \"numeric cols\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar def get_scal(feature):\n",
    "def get_scal(feature):\n",
    "    def minmax(x):\n",
    "        mini = # TODO: Your code goes here\n",
    "        maxi = # TODO: Your code goes here\n",
    "        return # TODO: Your code goes here\n",
    "    return(minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8IUfcuVaS_g"
   },
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "for header in numeric_cols:\n",
    "    scal_input_fn = get_scal(\n",
    "        # TODO: Your code goes here\n",
    "    )\n",
    "    feature_columns.append(\n",
    "        fc.numeric_column(\n",
    "            # TODO: Your code goes here\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should validate the total number of feature columns.  Compare this number to the number of numeric features you input earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of feature columns: \", len(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ug3hB8Sl0jO"
   },
   "source": [
    "### Using the Keras Sequential Model\n",
    "\n",
    "Next, we will run this cell to compile and fit the Keras Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_YJPPb3xTPeZ",
    "outputId": "2d445722-1d43-4a27-a6c0-c6ce813ab450"
   },
   "outputs": [],
   "source": [
    "# Model create\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns, dtype=\"float64\")\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        feature_layer,\n",
    "        layers.Dense(12, activation=\"relu\"),\n",
    "        layers.Dense(8, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"linear\", name=\"median_house_value\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model compile\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "# Model Fit\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we show loss as  Mean Square Error (MSE).  Remember that MSE is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable (e.g. housing median age) and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "vo7hhkPqm6Jx",
    "outputId": "938907f6-b6c8-497c-a8f6-0f1cdbf336c9"
   },
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(train_ds)\n",
    "print(\"Mean Squared Error\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "252EPxGp7-FJ"
   },
   "source": [
    "#### Visualize the model loss curve\n",
    "\n",
    "Next, we will use matplotlib to draw the model's loss curves for training and validation.  A line plot is also created showing the mean squared error loss over the training epochs for both the train (blue) and test (orange) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history, metrics):\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for idx, key in enumerate(metrics):\n",
    "        ax = fig.add_subplot(nrows, ncols, idx + 1)\n",
    "        plt.plot(history.history[key])\n",
    "        plt.plot(history.history[f\"val_{key}\"])\n",
    "        plt.title(f\"model {key}\")\n",
    "        plt.ylabel(key)\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\", \"validation\"], loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history, [\"loss\", \"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqkozY268xi7"
   },
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uf4TyVJ_Dzxe"
   },
   "source": [
    "Next, we read in the test.csv file and validate that there are no null values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can use .describe() to see some summary statistics for the numeric fields in our dataframe.  The count shows 4087.000000 for all feature columns. Thus, there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "b4C4BmhV8ch9",
    "outputId": "82bcc9d3-4432-4068-ab82-6a6abbe4a024"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../data/housing-test.csv\")\n",
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nY2Yrt8fC7RW"
   },
   "source": [
    "**Exercise.** Now that we have created an input pipeline using tf.data and compiled a Keras Sequential Model, we now create the input function for the test data and to initialize the `test_predict` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rMdDeGDCwpT"
   },
   "outputs": [],
   "source": [
    "def test_input_fn(features, batch_size=256):\n",
    "    \"\"\"An input function for prediction.\"\"\"\n",
    "    # Convert the inputs to a Dataset without labels.\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        # TODO: Your code goes here\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = test_input_fn(dict(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5SkINtbDIdr"
   },
   "source": [
    "#### Prediction:  Linear Regression\n",
    "\n",
    "Before we begin to feature engineer our feature columns, we should predict the median house value.  By predicting the median house value now, we can then compare it with the median house value after feature engineeing.\n",
    "\n",
    "To predict with Keras, you simply call [model.predict()](https://keras.io/models/model/#predict) and pass in the housing features you want to predict the median_house_value for. Note:  We are predicting the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNc6TSoJDL7-"
   },
   "outputs": [],
   "source": [
    "predicted_median_house_value = model.predict(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFXK1SKPDYgD"
   },
   "source": [
    "Next, we run two predictions in separate cells - one where ocean_proximity=INLAND and one where ocean_proximity= NEAR OCEAN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xepss0vhoHge",
    "outputId": "46842a26-eacd-4801-857b-18c6a8f2005c"
   },
   "outputs": [],
   "source": [
    "# Ocean_proximity is INLAND\n",
    "model.predict(\n",
    "    {\n",
    "        \"longitude\": tf.convert_to_tensor([-121.86]),\n",
    "        \"latitude\": tf.convert_to_tensor([39.78]),\n",
    "        \"housing_median_age\": tf.convert_to_tensor([12.0]),\n",
    "        \"total_rooms\": tf.convert_to_tensor([7653.0]),\n",
    "        \"total_bedrooms\": tf.convert_to_tensor([1578.0]),\n",
    "        \"population\": tf.convert_to_tensor([3628.0]),\n",
    "        \"households\": tf.convert_to_tensor([1494.0]),\n",
    "        \"median_income\": tf.convert_to_tensor([3.0905]),\n",
    "        \"ocean_proximity\": tf.convert_to_tensor([\"INLAND\"]),\n",
    "    },\n",
    "    steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qPssm8p4EZHh",
    "outputId": "2a55d427-7857-401c-f60d-edbb36be19ec"
   },
   "outputs": [],
   "source": [
    "# Ocean_proximity is NEAR OCEAN\n",
    "model.predict(\n",
    "    {\n",
    "        \"longitude\": tf.convert_to_tensor([-122.43]),\n",
    "        \"latitude\": tf.convert_to_tensor([37.63]),\n",
    "        \"housing_median_age\": tf.convert_to_tensor([34.0]),\n",
    "        \"total_rooms\": tf.convert_to_tensor([4135.0]),\n",
    "        \"total_bedrooms\": tf.convert_to_tensor([687.0]),\n",
    "        \"population\": tf.convert_to_tensor([2154.0]),\n",
    "        \"households\": tf.convert_to_tensor([742.0]),\n",
    "        \"median_income\": tf.convert_to_tensor([4.9732]),\n",
    "        \"ocean_proximity\": tf.convert_to_tensor([\"NEAR OCEAN\"]),\n",
    "    },\n",
    "    steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Txl-MRuLFE_8"
   },
   "source": [
    "The arrays returns a predicted value.  What do these numbers mean?  Let's compare this value to the test set.  \n",
    "\n",
    "Go to the test.csv you read in a few cells up.  Locate the first line and find the median_house_value - which should be 249,000 dollars near the ocean. What value did your model predicted for the median_house_value? Was it a solid model performance? Let's see if we can improve this a bit with feature engineering!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer features to create categorical and numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78F1XH1Qwvbt"
   },
   "source": [
    "**Exercise.**  Create a cell that indicates which features will be used in the model.  \n",
    "Note:  Be sure to bucketize 'housing_median_age' and ensure that 'ocean_proximity' is one-hot encoded.  And, don't forget your numeric values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxSatLUxUmvI"
   },
   "outputs": [],
   "source": [
    "numeric_cols = # TODO: Your code goes here\n",
    "\n",
    "bucketized_cols = # TODO: Your code goes here\n",
    "\n",
    "# indicator columns,Categorical features\n",
    "categorical_cols = # TODO: Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HbypkYHxxwt"
   },
   "source": [
    "Next, we scale the numerical, bucktized, and categorical feature columns that we assigned to the variables in the preceding cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExX5Akz0UnE-"
   },
   "outputs": [],
   "source": [
    "# Scalar def get_scal(feature):\n",
    "def get_scal(feature):\n",
    "    def minmax(x):\n",
    "        mini = train[feature].min()\n",
    "        maxi = train[feature].max()\n",
    "        return (x - mini) / (maxi - mini)\n",
    "\n",
    "    return minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzqcddUQUnKn"
   },
   "outputs": [],
   "source": [
    "# All numerical features - scaling\n",
    "feature_columns = []\n",
    "for header in numeric_cols:\n",
    "    scal_input_fn = get_scal(header)\n",
    "    feature_columns.append(\n",
    "        fc.numeric_column(header, normalizer_fn=scal_input_fn)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYUpUZvgwrPe"
   },
   "source": [
    "### Categorical Feature\n",
    "In this dataset, 'ocean_proximity' is represented as a string.  We cannot feed strings directly to a model. Instead, we must first map them to numeric values. The categorical vocabulary columns provide a way to represent strings as a one-hot vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZnlnFZkyEbe"
   },
   "source": [
    "**Exercise.** Next, we create a categorical feature using `ocean_proximity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Cf6SoFTUnc6"
   },
   "outputs": [],
   "source": [
    "for feature_name in categorical_cols:\n",
    "    vocabulary = # TODO: Your code goes here\n",
    "    categorical_c = # TODO: Your code goes here\n",
    "    one_hot = # TODO: Your code goes here\n",
    "    \n",
    "    feature_columns.append(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qnGyWaijzShj"
   },
   "source": [
    "### Bucketized Feature\n",
    "\n",
    "Often, you don't want to feed a number directly into the model, but instead split its value into different categories based on numerical ranges. Consider our raw data that represents a homes' age. Instead of representing the house age as a numeric column, we could split the home age into several buckets using a [bucketized column](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column). Notice the one-hot values below describe which age range each row matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZRlFyP7fOw-"
   },
   "source": [
    "Next we create a bucketized column using 'housing_median_age'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xB-yiVLmUnXp"
   },
   "outputs": [],
   "source": [
    "age = fc.numeric_column(\"housing_median_age\")\n",
    "\n",
    "# Bucketized cols\n",
    "age_buckets = # TODO: Your code goes here\n",
    "\n",
    "feature_columns.append(age_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri4_wssOg943"
   },
   "source": [
    "### Feature Cross\n",
    "\n",
    "Combining features into a single feature, better known as [feature crosses](https://developers.google.com/machine-learning/glossary/#feature_cross), enables a model to learn separate weights for each combination of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6HHJl3J0j0T"
   },
   "source": [
    "**Exercise.** Next, we create a feature cross of 'housing_median_age' and 'ocean_proximity'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JVLnG0WbUnkl"
   },
   "outputs": [],
   "source": [
    "vocabulary = housing_df['ocean_proximity'].unique()\n",
    "ocean_proximity = fc.categorical_column_with_vocabulary_list('ocean_proximity',\n",
    "                                                             vocabulary)\n",
    "\n",
    "crossed_feature = # TODO: Your code goes here\n",
    "crossed_feature = fc.indicator_column(crossed_feature)\n",
    "\n",
    "feature_columns.append(crossed_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should validate the total number of feature columns.  Compare this number to the number of numeric features you input earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of feature columns: \", len(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lNr00mP41sJp"
   },
   "source": [
    "Next, we will run this cell to compile and fit the Keras Sequential model.  This is the same model we ran earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4Dwal3oxUoCe",
    "outputId": "1ae08747-7dbe-47a5-b3e7-87581e460b1b"
   },
   "outputs": [],
   "source": [
    "# Model create\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns, dtype=\"float64\")\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        feature_layer,\n",
    "        layers.Dense(12, activation=\"relu\"),\n",
    "        layers.Dense(8, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"linear\", name=\"median_house_value\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model compile\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "# Model Fit\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LdUQszM16Oj"
   },
   "source": [
    "Next, we show loss and mean squared error then plot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ZtFSpkd9UoAW",
    "outputId": "bac4836e-c4f1-4b29-876d-91fe1b51a5a7"
   },
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(train_ds)\n",
    "print(\"Mean Squared Error\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "O8kWMa6xUn-M",
    "outputId": "05ed9323-1102-4245-a40b-88543f11b0f3"
   },
   "outputs": [],
   "source": [
    "plot_curves(history, [\"loss\", \"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4tWwOQt2e-P"
   },
   "source": [
    "**Exercise.** Get a prediction from the model.  Note:  You may use the same values from the previous prediciton.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict({\n",
    "    'longitude': # TODO: Your code goes here,\n",
    "    'latitude': # TODO: Your code goes here,\n",
    "    'housing_median_age': # TODO: Your code goes here,\n",
    "    'total_rooms': # TODO: Your code goes here,\n",
    "    'total_bedrooms': # TODO: Your code goes here,\n",
    "    'population': # TODO: Your code goes here,\n",
    "    'households': # TODO: Your code goes here,\n",
    "    'median_income': # TODO: Your code goes here,\n",
    "    'ocean_proximity': # TODO: Your code goes here)\n",
    "}, steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcbdA3arXkej"
   },
   "source": [
    "### Analysis \n",
    "\n",
    "The array returns a predicted value.  Compare this value to the test set you ran earlier. Your predicted value may be a bit better.\n",
    "\n",
    "Now that you have your \"feature engineering template\" setup, you can experiment by creating additional features.  For example, you can create derived features, such as households per population, and see how they impact the model.  You can also experiment with replacing the features you used to create the feature cross.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Basic Feature Engineering in Keras.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

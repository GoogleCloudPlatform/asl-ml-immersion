{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fdb77ea71896541",
   "metadata": {},
   "source": [
    "# Run LLM inference on Cloud Run GPUs with Gemma 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5d860-0945-4799-b2ba-74298c87ee3b",
   "metadata": {},
   "source": [
    "## Deploy a Gemma model with a prebuilt container\n",
    "This guide shows how to run LLM inference on Cloud Run GPUs with Gemma 3 and Ollama, and has the following objectives:\n",
    " - Deploy Ollama with the Gemma 3 model on a GPU-enabled Cloud Run service using a prebuilt container.\n",
    " - Using the deployed Cloud Run service with the Google Gen AI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d07082-c407-46c5-9fbe-9a420ba2f989",
   "metadata": {},
   "source": [
    "#### You can deploy the container image and make Ollama with Gemma 3 available as a Cloud Run service.\n",
    "Use the following gcloud run deploy command to deploy your Cloud Run service:\n",
    "gcloud run deploy {SERVICE_NAME} \\\n",
    " --image {IMAGE} \\\n",
    " --concurrency 4 \\\n",
    " --cpu 8 \\\n",
    " --set-env-vars OLLAMA_NUM_PARALLEL=4 \\\n",
    " --set-env-vars=API_KEY={YOUR_API_KEY} \\\n",
    " --gpu 1 \\\n",
    " --gpu-type nvidia-l4 \\\n",
    " --max-instances 1 \\\n",
    " --memory 32Gi \\\n",
    " --allow-unauthenticated \\\n",
    " --no-cpu-throttling \\\n",
    " --timeout=600 \\\n",
    " --region {REGION}\n",
    " \n",
    "The Cloud Run service is configured with:\n",
    "One Nvidia L4 GPU per instance.\n",
    "A maximum of four concurrent requests to the instance that matches the number of request slots (OLLAMA_NUM_PARALLEL) available per model to serve concurrent inference requests.\n",
    "A maximum of seven Cloud Run service instances which should match your GPU quota per project and per region.\n",
    "The no-cpu-throttling setting required to use GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c47ddc-622b-438b-aa26-9e0b7db828c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Supported Models and Pre-Built Docker Images:\n",
    "\n",
    " - gemma-3-1b-it\n",
    " - gemma-3-4b-it\n",
    " - gemma-3-12b-it\n",
    " - gemma-3-27b-it\n",
    " - gemma-3n-e2b-it\n",
    " - gemma-3n-e4b-it\n",
    " \n",
    "These images have the respective Gemma models bundled:\n",
    "\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3-1b\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3-4b\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3-12b\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3-27b\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3n-e2b\n",
    " - us-docker.pkg.dev/cloudrun/container/gemma/gemma3n-e4b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7672bbe-364c-421a-88a6-dd9e2ad94dca",
   "metadata": {},
   "source": [
    "### Copy the next commant to console:"
   ]
  },
  {
   "cell_type": "code",
   "id": "802b2d53-594d-4330-99ce-75cdd2a264e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(\"gcloud run deploy llm-gemma3-4b \\\n",
    " --image us-docker.pkg.dev/cloudrun/container/gemma/gemma3-4b \\\n",
    " --concurrency 4 \\\n",
    " --cpu 8 \\\n",
    " --set-env-vars OLLAMA_NUM_PARALLEL=4 \\\n",
    " --set-env-vars=API_KEY=TESTKEY12345 \\\n",
    " --gpu 1 \\\n",
    " --gpu-type nvidia-l4 \\\n",
    " --max-instances 1 \\\n",
    " --memory 32Gi \\\n",
    " --allow-unauthenticated \\\n",
    " --no-cpu-throttling \\\n",
    " --timeout=600 \\\n",
    " --region us-central1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d82b0083-5f5e-4e4c-9665-f32009616c23",
   "metadata": {},
   "source": [
    "## Send prompts to the LLM CloudRun service by using Google Gen AI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e7f3c3-93d6-410d-9c83-005204d7115d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Initializing Google Gen AI SDK Client"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ade28bc-9ef3-40e3-a52b-7bbe9e95fd53",
   "metadata": {
    "tags": []
   },
   "source": [
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "GEMMA_CLOUD_RUN_ENDPOINT=\"https://llm-gemma3-4b- ... .us-central1.run.app\" #TODO: Add your cloud run enpoint here\n",
    "GEMMA_CLOUD_RUN_API_KEY=\"TESTKEY12345\"\n",
    "# Configure the client to use your Cloud Run endpoint and API key\n",
    "client = genai.Client(api_key=GEMMA_CLOUD_RUN_API_KEY, http_options=HttpOptions(base_url=GEMMA_CLOUD_RUN_ENDPOINT))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b6a87fc-4fa0-4aac-bc39-2fd09c70da58",
   "metadata": {},
   "source": [
    "#### Generate content (non-streaming): "
   ]
  },
  {
   "cell_type": "code",
   "id": "e217b4e4-cbf3-4825-9a25-65ddd367e5e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "response = client.models.generate_content(\n",
    "   model=\"gemma-3-4b-it\", # Replace model with the Gemma 3 model you selected, such as \"gemma-3-4b-it\".\n",
    "   contents=[\"How does AI work?\"]\n",
    ")\n",
    "print(response.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce2078e7-044b-4465-8cc8-41bcf49df595",
   "metadata": {},
   "source": [
    "#### Stream generate content example"
   ]
  },
  {
   "cell_type": "code",
   "id": "2bd05495-f797-480e-8247-4abe5a6db33a",
   "metadata": {
    "tags": []
   },
   "source": [
    "response = client.models.generate_content_stream(\n",
    "   model=\"gemma-3-4b-it\", # Replace model with the Gemma 3 model you selected, such as \"gemma-3-4b-it\".\n",
    "   contents=[\"Write a story about a magic backpack. You are the narrator of an interactive text adventure game.\"]\n",
    ")\n",
    "for chunk in response:\n",
    "   print(chunk.text, end=\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "31650488-1e42-465e-8eab-05d3f092b5fb",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "After you have finished, it is a good practice to clean up your cloud resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b6138-4d26-40ca-a306-3470b1136367",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-adk_kernel-adk_kernel",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "ADK kernel (Local)",
   "language": "python",
   "name": "conda-env-adk_kernel-adk_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

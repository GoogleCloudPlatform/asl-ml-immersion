{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image Classification with Keras\n",
    "\n",
    "This notebook demonstrates how to implement a Convolutional Neural Network (CNN) on MNIST using Keras.\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Understand how a convolutional operation works.\n",
    "2. Understand how to build a CNN for image classification.\n",
    "3. Visualize learned features from a CNN to understand the CNN structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import Markdown, display\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Convolutional Operation\n",
    "\n",
    "A **convolution** is a fundamental mathematical operation in image processing. It involves applying a small matrix of numbers, called a **kernel** or **filter**, to an input image. The kernel slides over the image, performing an element-wise multiplication with the underlying pixels and then summing the results to produce a single output pixel. This process is repeated across the entire image to create a **feature map**, which highlights specific features like edges or textures.\n",
    "\n",
    "In this section, we'll see this in action using a simple example. We'll use a standard MNIST image and apply two well-known filters—the Sobel X and Sobel Y kernels—to detect horizontal and vertical edges.\n",
    "\n",
    "\n",
    "The Sobel kernels are designed to highlight directional changes in pixel intensity, which correspond to edges. The Sobel X kernel detects horizontal edges, while the Sobel Y kernel detects vertical edges.\n",
    "\n",
    "\n",
    "After applying these kernels to the image, the output feature maps clearly show the detected edges, with the horizontal edges highlighted by the Sobel X kernel and the vertical edges by the Sobel Y kernel. This demonstrates how a simple convolutional operation can extract meaningful features from an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a single image\n",
    "image = x_train[0].astype(\"float32\")\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define two Sobel kernels: one for detecting horizontal edges and one for detecting vertical edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "sobel_x = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "sobel_y = np.array([\n",
    "    [-1, -2, -1],\n",
    "    [0, 0, 0],\n",
    "    [1, 2, 1]\n",
    "], dtype=np.float32)\n",
    "# fmt: on\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot(1, 2, 1, title=\"Sobel X Kernel\")\n",
    "plt.imshow(sobel_x, cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2, title=\"Sobel Y Kernel\")\n",
    "plt.imshow(\n",
    "    sobel_y,\n",
    "    cmap=\"gray\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will apply these kernels to our image using a `Conv2D` layer. We will create two separate layers, one for each kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_reshaped = image.reshape(1, 28, 28, 1)\n",
    "sobel_x_reshaped = sobel_x.reshape(3, 3, 1, 1)\n",
    "sobel_y_reshaped = sobel_y.reshape(3, 3, 1, 1)\n",
    "\n",
    "# Create Conv2D layers\n",
    "conv_x = layers.Conv2D(1, (3, 3), use_bias=False, padding=\"valid\")\n",
    "conv_y = layers.Conv2D(1, (3, 3), use_bias=False, padding=\"valid\")\n",
    "\n",
    "conv_x.build(input_shape=(1, 28, 28, 1))\n",
    "conv_y.build(input_shape=(1, 28, 28, 1))\n",
    "\n",
    "conv_x.set_weights([sobel_x_reshaped])\n",
    "conv_y.set_weights([sobel_y_reshaped])\n",
    "\n",
    "# Apply the convolutions\n",
    "output_x = conv_x(image_reshaped)\n",
    "output_y = conv_y(image_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the results. The feature maps show the detected horizontal and vertical edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(output_x.numpy().reshape(26, 26))\n",
    "plt.title(\"Horizontal Edges\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output_y.numpy().reshape(26, 26))\n",
    "plt.title(\"Vertical Edges\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training a Convolutional Neural Network (CNN)\n",
    "\n",
    "A **Convolutional Neural Network (CNN)** is a type of deep learning model that excels at processing and classifying images. It's built with layers that automatically learn to extract relevant features from the input data. The key idea is that the network learns a hierarchy of features: simple features like edges in early layers, and more complex features like shapes and objects in deeper layers.\n",
    "\n",
    "### Preprocessing the Data\n",
    "\n",
    "Before feeding our MNIST images into the CNN, we need to prepare the data. The two main steps are:\n",
    "\n",
    "1.  **Scaling**: The pixel values of the MNIST images range from 0 to 255. We normalize these values by dividing by 255.0 to scale them into the range of 0 to 1. This helps the model to train more efficiently.\n",
    "\n",
    "2.  **One-Hot Encoding**: The image labels are single integers (0-9). We convert them into a **one-hot encoded vector**. For example, the label `5` becomes a vector `[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]`, where only the index corresponding to the correct digit is marked with a 1. This is a common practice for multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Our CNN is a sequential model composed of several layers. The core idea is to progressively extract and refine features from the images before making a final classification.\n",
    "\n",
    "The architecture consists of:\n",
    "* **Convolutional Layers (`Conv2D`)**: These layers apply filters to the input, learning to detect features. We use four of these layers with increasing numbers of filters (16, 32, and 64) to learn a richer hierarchy of features. The `relu` activation function is used to introduce non-linearity, allowing the model to learn more complex relationships.\n",
    "* **Max Pooling Layers (`MaxPooling2D`)**: These layers follow the convolutional layers. They downsample the feature maps by taking the maximum value in a small window, which helps to reduce the spatial dimensions, computational cost, and prevent overfitting.\n",
    "* **Flatten Layer**: This layer reshapes the 2D feature maps into a single 1D vector. This is a necessary step to connect the convolutional part of the network to the dense classification layers.\n",
    "* **Dense Layers**: These are standard fully connected neural network layers. The first dense layer with 64 neurons processes the features from the flatten layer. The final dense layer with 10 neurons (one for each digit) uses a `softmax` activation function to output a probability distribution, indicating the model's confidence for each possible digit.\n",
    "\n",
    "The model summary below shows the output shape and parameter count for each layer in the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(\n",
    "            16,\n",
    "            (3, 3),\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(28, 28, 1),\n",
    "            name=\"Conv2D_1\",\n",
    "        ),\n",
    "        layers.MaxPooling2D((2, 2), name=\"MaxPooling2D_1\"),\n",
    "        layers.Conv2D(\n",
    "            32, (3, 3), activation=\"relu\", padding=\"same\", name=\"Conv2D_2\"\n",
    "        ),\n",
    "        layers.MaxPooling2D((2, 2), name=\"MaxPooling2D_2\"),\n",
    "        layers.Conv2D(\n",
    "            64, (3, 3), activation=\"relu\", padding=\"same\", name=\"Conv2D_3\"\n",
    "        ),\n",
    "        layers.MaxPooling2D((2, 2), name=\"MaxPooling2D_3\"),\n",
    "        layers.Conv2D(\n",
    "            64, (3, 3), activation=\"relu\", padding=\"same\", name=\"Conv2D_4\"\n",
    "        ),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Now we will train our model on the MNIST dataset. We will use a batch size of 128 and train for 10 epochs. An epoch is one complete pass through the entire training dataset. The batch size is the number of samples that will be propagated through the network at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, epochs=10, batch_size=128, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "Now that we have trained our model, we need to evaluate its performance on the test data. We will use the accuracy metric to evaluate the model. Accuracy is the percentage of correctly classified images.\n",
    "\n",
    "### Evaluate the Model\n",
    "\n",
    "After training, we evaluate the model's performance on a separate test dataset that it has never seen before. The accuracy metric tells us the percentage of test images that the model correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Failed Cases\n",
    "\n",
    "Despite its high accuracy, the model isn't perfect. Analyzing the misclassified images can offer insights into the model's limitations. Some of these errors might be due to ambiguous or poorly written digits that are difficult for both humans and AI to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, index in enumerate(misclassified_indices[:9]):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(x_test[index].reshape(28, 28))\n",
    "    plt.title(\n",
    "        f\"Predicted: {predicted_labels[index]}, True: {true_labels[index]}\"\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing CNN\n",
    "\n",
    "### Visualize Learned Kernels\n",
    "\n",
    "The filters inside a convolutional layer are what the model learns to identify specific features. By visualizing the weights of these filters, we can get a sense of what the model is looking for at each stage.\n",
    "\n",
    "Here, we display the 16 filters from the first convolutional layer. Each square represents a 3x3 filter. You can see that the filters have learned to detect basic features such as various oriented edges and simple patterns. These basic features are the building blocks for the more complex features learned in subsequent layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_conv_layer = model.layers[0]\n",
    "weights = first_conv_layer.get_weights()[0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(weights.shape[-1]):\n",
    "    plt.subplot(4, 8, i + 1)\n",
    "    plt.title(f\"Kernel {i + 1}\", fontsize=8)\n",
    "    plt.imshow(weights[:, :, 0, i])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Feature Maps\n",
    "\n",
    "As data passes through a CNN, each layer transforms the input into feature maps. These maps highlight the presence of features that the filters in that layer have learned to detect. Visualizing these maps for different layers helps us understand how the network progressively builds up a more abstract representation of the input image.\n",
    "\n",
    "- **First Convolutional Layer**: The feature maps from this layer show the presence of simple features like edges and corners, as detected by the learned kernels.\n",
    "- **Second Convolutional Layer**: These maps are the result of applying filters to the outputs of the first layer. They show more complex features, which are combinations of the simpler features detected in the previous layer. This demonstrates the hierarchical nature of feature learning in CNNs.\n",
    "- **Third and Fourth Convolutional Layers**: As we go deeper, the feature maps become more abstract and less directly interpretable to the human eye. They represent higher-level features and patterns that are essential for the final classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_feature_map(activation):\n",
    "    num_channel = activation.shape[-1]\n",
    "    plt.figure(figsize=(10, num_channel // 8 * 1.5))\n",
    "    for i in range(num_channel):\n",
    "        plt.subplot(num_channel // 8, 8, i + 1)\n",
    "        plt.title(f\"Channel {i + 1}\", fontsize=8)\n",
    "        plt.imshow(activation[0, :, :, i], vmax=np.max(activation))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_IMG_INDEX = 0  # Change this index to check differnet images\n",
    "CONV_ONLY = True  # False to visualize MaxPooling too\n",
    "\n",
    "layers = model.layers[0:7:2] if CONV_ONLY else model.layers[:7]\n",
    "outputs = [layer.output for layer in layers]\n",
    "activation_model = models.Model(inputs=model.inputs, outputs=outputs)\n",
    "\n",
    "test_image = x_test[TEST_IMG_INDEX].reshape(1, 28, 28, 1)\n",
    "activations = activation_model.predict(test_image)\n",
    "\n",
    "display(Markdown(f\"## Input Image, shape: {x_test[TEST_IMG_INDEX].shape}\"))\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(x_test[TEST_IMG_INDEX])\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "for i, activation in enumerate(activations):\n",
    "    display(\n",
    "        Markdown(f\"## {layers[i].name}, output shape: {activation.shape[1:]}\")\n",
    "    )\n",
    "    visualize_feature_map(activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualizations illustrate how the CNN transforms the raw image pixels into a set of features, which contains highly abstruct signals about class information, enabling a correct classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# FraudFinder - Streaming Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This series of labs are updated upon [FraudFinder](https://github.com/googlecloudplatform/fraudfinder) repository which builds a end-to-end real-time fraud detection system on Google Cloud. Throughout the FraudFinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3f6a603e433"
   },
   "source": [
    "### Objective\n",
    "\n",
    "As you engineer features for model training, it's important to consider how the features are computed when making predictions with new data. For online predictions, you may have features that can be pre-computed via _batch feature engineering_. You may also have features that need to be computed on-the-fly via _streaming-based feature engineering_. For these Fraudfinder labs, for computing features based on the last n _days_, you will use _batch_ feature engineering in BigQuery; for computing features based on the last n _minutes_, you will use _streaming-based_ feature engineering using Dataflow.\n",
    "\n",
    "In order to calculate very recent customer and terminal activity (i.e. within the last hour), computation has to be done on real-time streaming data, rather than via batch-based feature engineering. This notebook shows a step-by-step guide to create a real-time inference pipeline. You will learn to:\n",
    "\n",
    "- Create a real-time inference pipeline using Apache Beam.\n",
    "- Enrich streaming data with features from the Vertex AI Feature Store.\n",
    "- Perform real-time inference using a deployed Vertex AI model.\n",
    "- Deploy the Apache Beam pipeline to Dataflow.\n",
    "- Write the inference results to Pub/Sub and BigQuery.\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Pub/Sub](https://cloud.google.com/pubsub/)\n",
    "- [Dataflow](https://cloud.google.com/dataflow/)\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "\n",
    "The steps performed in this notebook are:\n",
    "\n",
    "1. Read streaming data from a Pub/Sub topic.\n",
    "2. Enrich the data by looking up customer and terminal features from the Vertex AI Feature Store.\n",
    "3. Invoke a deployed Vertex AI model for real-time predictions.\n",
    "4. Write the prediction results to another Pub/Sub topic for downstream consumption.\n",
    "5. Write the prediction results to BigQuery for storage and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "535c9218e18d"
   },
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94fe7f01ef5a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)\n",
    "OUTPUT_TOPIC_NAME = \"fraud_finder_inference\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PubSub topic for inference pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud pubsub topics create fraud_finder_inference --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud pubsub subscriptions create \"fraud_finder_inference_sub\" --topic=\"fraud_finder_inference\" --topic-project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folder\n",
    "\n",
    "In favour of clean folder structure, we will create a separate folder and place all the files that we will produce there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER = \"./beam_pipeline\"\n",
    "PYTHON_SCRIPT = f\"{FOLDER}/main.py\"\n",
    "REQUIREMENTS_FILE = f\"{FOLDER}/requirements.txt\"\n",
    "\n",
    "# Create new folder for pipeline files\n",
    "!rm -rf {FOLDER} || True\n",
    "!mkdir {FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we begin\n",
    "\n",
    "For deploying Apache Beam pipelines to Dataflow, it is a best practice to submit the job from a Python script rather than directly from a notebook. This approach helps in managing dependencies cleanly and is more suitable for production environments. When you run a Dataflow job, the entire session is serialized and sent to the workers. Notebook environments can have a lot of state that can cause issues with this serialization process.\n",
    "\n",
    "Therefore, in the following cells, we will be writing our pipeline code to a Python script called `main.py`. We will then execute this script to deploy the Dataflow job. This method is used for clarity and to demonstrate a more robust deployment strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf",
    "tags": []
   },
   "source": [
    "### Write import statements\n",
    "\n",
    "Here we write the code to import all the required libraries to the external python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a725b800a52b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {PYTHON_SCRIPT}\n",
    "import apache_beam as beam\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.transforms.combiners import CountCombineFn, MeanCombineFn\n",
    "\n",
    "from apache_beam.transforms.enrichment import Enrichment\n",
    "from apache_beam.transforms.enrichment_handlers.vertex_ai_feature_store import VertexAIFeatureStoreEnrichmentHandler\n",
    "\n",
    "import google.auth\n",
    "\n",
    "import json\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.io import PubsubMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bd54c684051"
   },
   "source": [
    "### Defining an auxiliary magic function\n",
    "\n",
    "The magic function `writefile` from Jupyter Notebook can only write the cell as is and could not unpack Python variables. Hence, we need to create an auxiliary magic function that can unpack Python variables and write them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad8dba046d05",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, \"a\") as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve deployed Vertex AI endpoint id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "endpoints = vertex_ai.Endpoint.list(\n",
    "    filter=f\"display_name={ENDPOINT_NAME}\",  # optional: filter by specific endpoint name\n",
    "    order_by=\"update_time\",\n",
    ")\n",
    "\n",
    "ENDPOINT_ID = endpoints[-1].name\n",
    "print(f\"Vertex AI Endpoint ID={ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a74807b98c34"
   },
   "source": [
    "### Write the variable values\n",
    "\n",
    "Here we write the variable values to the external python script using the new magic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a4f9df411aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding additional variables to project_variables\n",
    "project_variables = \"\\n\".join(config[1:-1])\n",
    "project_variables += f'\\nPROJECT_ID = \"{PROJECT}\"'\n",
    "project_variables += f'\\nBUCKET_NAME = \"{BUCKET_NAME}\"'\n",
    "project_variables += f'\\nREQUIREMENTS_FILE = \"{REQUIREMENTS_FILE}\"'\n",
    "project_variables += f'\\nENDPOINT_ID = \"{ENDPOINT_ID}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5055cc7ce79e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writetemplate {PYTHON_SCRIPT}\n",
    "\n",
    "# Project variables\n",
    "{project_variables}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c82e89dd7f2c"
   },
   "source": [
    "### Write constant variables\n",
    "\n",
    "Here we write constant variables to the external python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a437ab07d805",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile -a {PYTHON_SCRIPT}\n",
    "\n",
    "# Pub/Sub variables\n",
    "SUBSCRIPTION_NAME = \"ff-tx-for-feat-eng-sub\"\n",
    "SUBSCRIPTION_PATH = f\"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\"\n",
    "\n",
    "# Dataflow variables\n",
    "FIFTEEN_MIN_IN_SECS = 15 * 60\n",
    "THIRTY_MIN_IN_SECS = 30 * 60\n",
    "WINDOW_SIZE = 60 * 60 # 1 hour in secs\n",
    "WINDOW_PERIOD = 1 * 60  # 1 min in secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ee7d34ce9b1"
   },
   "source": [
    "### Building the pipeline\n",
    "\n",
    "Now we are ready to build the pipeline. The pipeline is designed to process streaming data, enrich it with features from a feature store, run inference using a trained model, and then output the results to two different destinations.\n",
    "\n",
    "Here's a breakdown of the pipeline's components:\n",
    "\n",
    "*   **Data Source (Pub/Sub):** The pipeline starts by reading messages from a Pub/Sub subscription. These messages represent real-time transactions that need to be evaluated for fraud.\n",
    "\n",
    "*   **Enrichment (Vertex AI Feature Store):** Each incoming transaction is then enriched with pre-computed features from the Vertex AI Feature Store. We use the `Enrichment` transform from the Apache Beam SDK, along with the `VertexAIFeatureStoreEnrichmentHandler`. This allows us to look up features for both the customer and the terminal involved in the transaction.\n",
    "\n",
    "*   **Inference (Vertex AI Prediction):** Once the transaction data is enriched with the necessary features, it is passed to a deployed Vertex AI model for inference. We use the `RunInference` transform, which is a generic transform for running machine learning models in an Apache Beam pipeline. We configure it with a `VertexAIModelHandlerJSON` to handle the communication with the Vertex AI Prediction service.\n",
    "\n",
    "*   **Data Sinks (Pub/Sub and BigQuery):** The pipeline has two output branches:\n",
    "    *   One branch writes the inference results to a Pub/Sub topic. This is useful for downstream applications that need to react to the predictions in real-time.\n",
    "    *   The other branch writes the results to a BigQuery table. This allows for the storage and long-term analysis of the predictions.\n",
    "\n",
    "The entire pipeline is defined within the `main` function, which will be written to our Python script and then deployed to Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c77896262f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile -a {PYTHON_SCRIPT}\n",
    "\n",
    "def main():\n",
    "    # # Initialize Vertex AI client\n",
    "    # aiplatform.init(\n",
    "    #     project=PROJECT_ID,\n",
    "    #     location=REGION\n",
    "    # )\n",
    "    \n",
    "    API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "    \n",
    "    # Setup pipeline options for deploying to dataflow\n",
    "    pipeline_options = PipelineOptions(streaming=True,\n",
    "                                       save_main_session=True,\n",
    "                                       runner=\"DataflowRunner\",\n",
    "                                       project=PROJECT_ID,\n",
    "                                       region=REGION,\n",
    "                                       temp_location=f\"gs://{BUCKET_NAME}/dataflow/tmp\",\n",
    "                                       requirements_file=REQUIREMENTS_FILE,\n",
    "                                       max_num_workers=1)\n",
    "    \n",
    "    # Build pipeline and transformation steps\n",
    "    pipeline = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "    output_table = f'{PROJECT_ID}.tx.streaming_pipeline'\n",
    "    pub_sub_toipc_output = f\"projects/{PROJECT_ID}/topics/fraud_finder_inference\"\n",
    "\n",
    "    def convert_row_to_payload(element: beam.Row):\n",
    "        element_dict = element._asdict()\n",
    "        # Default features in case if its not exist in a feature store:\n",
    "        default_features = {\n",
    "            'tx_amount': element_dict['TX_AMOUNT'],\n",
    "            'customer_id_avg_amount_14day_window': 0,\n",
    "            'customer_id_avg_amount_15min_window': 0,\n",
    "            'customer_id_avg_amount_1day_window': 0,\n",
    "            'customer_id_avg_amount_30min_window': 0,\n",
    "            'customer_id_avg_amount_60min_window': 0,\n",
    "            'customer_id_avg_amount_7day_window': 0,\n",
    "            'customer_id_nb_tx_14day_window': 0,\n",
    "            'customer_id_nb_tx_7day_window': 0,\n",
    "            'customer_id_nb_tx_15min_window': 0,\n",
    "            'customer_id_nb_tx_1day_window': 0,\n",
    "            'customer_id_nb_tx_30min_window': 0,\n",
    "            'customer_id_nb_tx_60min_window': 0,\n",
    "            'terminal_id_avg_amount_15min_window': 0,\n",
    "            'terminal_id_avg_amount_30min_window': 0,\n",
    "            'terminal_id_avg_amount_60min_window':0,\n",
    "            'terminal_id_nb_tx_14day_window': 0,\n",
    "            'terminal_id_nb_tx_15min_window': 0,\n",
    "            'terminal_id_nb_tx_1day_window': 0,\n",
    "            'terminal_id_nb_tx_30min_window': 0,\n",
    "            'terminal_id_nb_tx_60min_window': 0,\n",
    "            'terminal_id_nb_tx_7day_window': 0,\n",
    "            'terminal_id_risk_14day_window': 0,\n",
    "            'terminal_id_risk_1day_window': 0,\n",
    "            'terminal_id_risk_7day_window': 0\n",
    "        }\n",
    "        default_features.update(element_dict)\n",
    "        del default_features['TX_AMOUNT']\n",
    "        return default_features\n",
    "    \n",
    "    def item_to_message(item: Dict[str, Any]) -> PubsubMessage:\n",
    "        # Re-import needed types. When using the Dataflow runner, this\n",
    "        # function executes on a worker, where the global namespace is not\n",
    "        # available. For more information, see:\n",
    "        # https://cloud.google.com/dataflow/docs/guides/common-errors#name-error\n",
    "        from apache_beam.io import PubsubMessage\n",
    "\n",
    "        attributes = {\"type\": \"inference\"}\n",
    "        data = bytes(json.dumps(item), \"utf-8\")\n",
    "\n",
    "        return PubsubMessage(data=data, attributes=attributes)\n",
    "\n",
    "    model_handler = VertexAIModelHandlerJSON(endpoint_id=ENDPOINT_ID,\n",
    "                                             project=PROJECT_ID,\n",
    "                                             location=REGION,\n",
    "                                            ).with_preprocess_fn(convert_row_to_payload)\n",
    "\n",
    "    vertex_ai_handler_customers = VertexAIFeatureStoreEnrichmentHandler(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        api_endpoint=API_ENDPOINT,\n",
    "        feature_store_name=FEATURESTORE_ID,\n",
    "        feature_view_name=\"fv_fraudfinder_customers\",\n",
    "        row_key=\"CUSTOMER_ID\",\n",
    "    )\n",
    "\n",
    "    vertex_ai_handler_terminals = VertexAIFeatureStoreEnrichmentHandler(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        api_endpoint=API_ENDPOINT,\n",
    "        feature_store_name=FEATURESTORE_ID,\n",
    "        feature_view_name=\"fv_fraudfinder_terminals\",\n",
    "        row_key=\"TERMINAL_ID\",\n",
    "    )\n",
    "\n",
    "    SUBSCRIPTION_NAME = \"ff-tx-sub\"\n",
    "    SUBSCRIPTION_PATH = f\"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\"\n",
    "\n",
    "    source = (\n",
    "        pipeline\n",
    "        | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(subscription=SUBSCRIPTION_PATH)\n",
    "        | 'Decode message' >> beam.Map(lambda row: beam.Row(**json.loads(row.decode('utf-8'))))\n",
    "    )\n",
    "\n",
    "    inference = (\n",
    "    source\n",
    "    | \"Enrich Customer Online FS\" >> Enrichment(vertex_ai_handler_customers)\n",
    "    | \"Enrich Terminal Online FS\" >> Enrichment(vertex_ai_handler_terminals)\n",
    "    | \"RunInference\" >> RunInference(model_handler)\n",
    "    | \"Prep BQ Row\" >> beam.Map(lambda x: {**x.example, \"model_id\": x.model_id, **x.inference}))\n",
    "    \n",
    "    \n",
    "    _ = (\n",
    "    inference\n",
    "    | \"Convert to Pub/Sub messages\" >> beam.Map(item_to_message)\n",
    "    | \"Write to Pub/Sub\" >> beam.io.WriteToPubSub(topic=pub_sub_toipc_output, with_attributes=True))\n",
    "\n",
    "    _ = (\n",
    "    inference\n",
    "    | \"Write BigQuery\" >> beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "        table=output_table,\n",
    "        method=beam.io.gcp.bigquery.WriteToBigQuery.Method.STREAMING_INSERTS,\n",
    "        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED))\n",
    "    \n",
    "    # Run the pipeline (async)\n",
    "    pipeline.run()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9133e7c54aa"
   },
   "source": [
    "### Creating `requirement.txt` for Dataflow Workers\n",
    "\n",
    "As we are using `google-cloud-aiplatform` and `google-apitools` package, we need to pass the `requirement.txt` to the Dataflow Workers so that the workers will install the packages in their respective environment before running the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b58205547e1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {REQUIREMENTS_FILE}\n",
    "google-cloud-aiplatform==1.115.0\n",
    "google-apitools==0.5.32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9ede8f5aff0"
   },
   "source": [
    "### Deploying the pipeline\n",
    "\n",
    "Now we are ready to deploy this pipeline to Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0601ef9a1b20",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 {PYTHON_SCRIPT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully deployed a real-time inference pipeline to Dataflow. You can monitor the job and see the results in the [Dataflow console](https://console.cloud.google.com/dataflow/jobs). This concludes the notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_feature_engineering_streaming.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
